# -*- coding: utf-8 -*-
"""Copie de DLII_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c4gMlzfMedMPnITOScXl3ihKCval8Rtk
"""

colab = True
if colab:
    from google_drive_downloader import GoogleDriveDownloader as gdd
    gdd.download_file_from_google_drive(file_id='1nnsLbtFQ944iU-bw06I34kuQS7n-VBdL', dest_path='./data.zip')

    from zipfile import ZipFile
    with ZipFile('data.zip', 'r') as zipObj:
        # Extract all the contents of zip file in current directory
        zipObj.extractall()

# Unzip mnist data
!gzip -d ./data/t10k-images-idx3-ubyte.gz ./data/t10k-labels-idx1-ubyte.gz ./data/train-labels-idx1-ubyte.gz ./data/train-images-idx3-ubyte.gz

# Install idx2numpy for reading MNIST
!pip install idx2numpy

"""<h1 align='center'>Projet Deep Learning II</h1>

<h4 align='center'>Authors : Kevin XU & Qin WANG</h4>

## Table of Contents
1. [Introduction](#intro)
2. [Données](#donnees)
3. [Fonctions élémentaires](#fctelementaire)
    + [Construction d'un RBM et test sur Binary AlphaDigits](#constructionRBM)
    + [Construction d’un DBN et test sur Binary AlphaDigits](#constructionDBN)
    + [Construction d’un DNN et test sur MNIST](#constructionDNN)
4. [Travail préliminaire (Binary AlphaDigit)](#travailprelim)
5. [Etude à réaliser (MNIST)](#mnist)
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import random
import copy

import scipy.io
import scipy
from scipy.special import softmax
import idx2numpy

"""# <a id='intro'>1. Introduction</a>

L’objectif du projet est de réaliser un réseau de neurones profond pré-entraîné ou non pour la classification de chiffres manuscrits. On va comparer les performances, en terme de taux de bonnes classifications, d’un réseau pré-entrainé et d’un réseau initialisé aléatoirement, en fonction du nombre de données d’apprentissage, du nombre de couches du réseau et enfin du nombre de neurones par couches.

# <a id='donnees'>2. Données</a>

**To download :**
+ http://yann.lecun.com/exdb/mnist/ (les 4 fichiers)
+ https://cs.nyu.edu/~roweis/data.html (Binary Alphadigits)
"""

path_to_data = './data/'

# Function for Binary Alphadigits dataset
alphadigs = scipy.io.loadmat(path_to_data + 'binaryalphadigs.mat')

def lire_alpha_digit(index_carac):
    """ 
    Get alpha digit data for given character.
    
    Parameters
    ----------
    index_carac : list 
        The characters that we want to learn.

    Returns
    -------
    data : ndarray
        Matrix containing the data with rows corresponding to samples and columns to pixels.
    """
    nb_pixel = alphadigs['dat'][0,0].shape[0] * alphadigs['dat'][0,0].shape[1]
    data = np.empty(shape=(1, nb_pixel))

    for i in index_carac:
        data = np.append(data, np.array([img.flatten() for img in alphadigs['dat'][i,:]]), axis=0)
    return data[1:,:]

alphadigs_data = lire_alpha_digit([10])
alphadigs_data.shape

# Function for MNIST dataset
def to_black_white(images):
    images[images < 128] = 0
    images[images >= 128] = 1
    return images

def lire_MNIST():
    """ 
    Get alpha digit data for given character.
    
    Parameters
    ----------

    Returns
    -------
    data : ndarray
        Matrix containing the data with rows corresponding to samples and columns to pixels.
    """
    train_image_file = 'data/train-images-idx3-ubyte'
    train_label_file = 'data/train-labels-idx1-ubyte'
    test_image_file = 'data/t10k-images-idx3-ubyte'
    test_label_file = 'data/t10k-labels-idx1-ubyte'

    train_image = idx2numpy.convert_from_file(train_image_file)
    train_image = to_black_white(np.array([img.flatten() for img in train_image]))

    test_image = idx2numpy.convert_from_file(test_image_file)
    test_image = to_black_white(np.array([img.flatten() for img in test_image]))

    train_label = idx2numpy.convert_from_file(train_label_file)
    test_label = idx2numpy.convert_from_file(test_label_file)

    return train_image, train_label, test_image, test_label

train_image, train_label, test_image, test_label = lire_MNIST()
nb_labels = np.unique(train_label).shape[0]

print(train_image.shape)
print(train_label.shape)
print(test_image.shape)
print(test_label.shape)
plt.imshow(np.reshape(train_image[1], newshape=(28,28)), cmap=plt.cm.binary)

"""# <a id='fctelementaire'>3. Fonctions élémentaires</a>

## <a id='constructionRBM'>3.1 Construction d'un RBM et test sur Binary AlphaDigits</a>
"""

def sigmoid(x):
    """ 
    Sigmoid function.
    
    Parameters
    ----------
    x : float 
    
    Returns
    -------
    result : float
    """
    return 1 / (1 + np.exp(-x))
    
def init_RBM(p, q):
    """ 
    Create a RBM structure with weights and biases initialized.
    
    Parameters
    ----------
    p : int
        Number of visible units.
    
    q : int
        Number of hidden units.
        
    Returns
    -------
    RBM : dict
    """
    RBM = {
        "W" : np.random.randn(p, q) * 0.1,
        "a" : np.zeros(shape=(1, p)),
        "b" : np.zeros(shape=(1, q)),
        "p" : p,
        "q" : q
    }
    return RBM

def entree_sortie_RBM(RBM, input_data):
    """ 
    Compute the values of the hidden layer for given input data by the RBM.
    
    Parameters
    ----------
    RBM : dict
        A RBM structure.
    
    input_data : ndarray
        
    Returns
    -------
    result : ndarray
    """
    return sigmoid(RBM['b'] + input_data.dot(RBM['W']))

def sortie_entree_RBM(RBM, output_data):
    """ 
    Compute the values of the visible layer for given output data by the RBM.
    
    Parameters
    ----------
    RBM : dict
        A RBM structure.
    
    output_data : ndarray
        
    Returns
    -------
    result : ndarray
    """
    return sigmoid(RBM['a'] + output_data.dot(RBM['W'].T))

def train_RBM(RBM, input_data, nb_iter=100, lr=0.1, batch_size=32, verbose=True):
    """ 
    Train a RBM by Contrastive-Divergence (CD-1) algorithm.
    
    Parameters
    ----------
    RBM : dict
        A RBM structure.
    
    input_data : ndarray
        Input data.
        
    nb_iter : int
        Number of iterations.
        
    lr : float
        Learning rate.
        
    batch_size : int
        The batch size.
        
    verbose : bool
        Enable verbose output.
        
    Returns
    -------
    RBM : dict
        A RBM structure.
    """
    n = input_data.shape[0]
    p = RBM['p']
    q = RBM['q']
    for i in range(nb_iter):
        np.random.shuffle(input_data) 
        for batch in range(batch_size):
            x = input_data[np.minimum(np.arange(batch * batch_size, (batch + 1) * batch_size), n-1),:]
            v_0 = x
            h_0 = (np.random.uniform(0, 1, size=(x.shape[0], q)) < entree_sortie_RBM(RBM, x)).astype(int)
            v_1 = (np.random.uniform(0, 1, size=(x.shape[0], p)) < sortie_entree_RBM(RBM, h_0)).astype(int)
            
            da = np.sum(v_0 - v_1, axis=0)
            db = np.sum(entree_sortie_RBM(RBM, x) - entree_sortie_RBM(RBM, v_1), axis=0)
            dW = v_0.T.dot(entree_sortie_RBM(RBM, x)) - v_1.T.dot(entree_sortie_RBM(RBM, v_1))

            RBM['W'] += lr * dW / batch_size      
            RBM['a'] += lr * da / batch_size
            RBM['b'] += lr * db / batch_size
            
        if verbose:
            # erreur quadratique
            sortie = entree_sortie_RBM(RBM, input_data)
            new_entree = sortie_entree_RBM(RBM, sortie)
            erreur_reconstruction = np.sum((input_data - new_entree)**2/n)
            print("iteration %d \t : \t erreur reconstruction %.2f" % (i, erreur_reconstruction))
    return RBM

def generer_image_RBM(RBM, image_shape, nb_images=3, nb_iter_gibbs=100, plot=True):
    """ 
    Generate samples following a RBM by Gibbs sampling algorithm.
    
    Parameters
    ----------
    RBM : dict
        A RBM structure.

    image_shape : tuple
      Shape of the images
    
    nb_images : int
        Number of images to generate.
        
    nb_iter_gibbs : int
        Number of iterations in Gibbs sampling.
    
    plot : bool
        Plot the images.
        
    Returns
    -------
    images : list of arrays
        Generated samples.
    """
    p = RBM['p']
    q = RBM['q']
    images = []
    
    if plot:
        plt.figure(figsize=(20, 20))
    for i in range(nb_images):
        x = (np.random.uniform(0, 1, size=(1, p)) < 0.5).astype(int)
        for j in range(nb_iter_gibbs):
            h = (np.random.uniform(0, 1, size=(1, q)) < entree_sortie_RBM(RBM, x)).astype(int)
            x = (np.random.uniform(0, 1, size=(1, p)) < sortie_entree_RBM(RBM, h)).astype(int)
        images.append(x)

        # Plot image 
        if plot:
            x = np.reshape(x, newshape=image_shape)
            plt.subplot(1, nb_images, i+1)
            plt.axis('off')
            plt.imshow(x, cmap='gray')
    
    return images

def principal_RBM_alpha(q, index_carac, nb_images, nb_iter=100, lr=0.1, batch_size=32, nb_iter_gibbs=100, verbose=1, plot=True):
    """ 
    Learn characters of Binary AlphaDigits with a RBM.
    
    Parameters
    ----------
    q : int
        Number of hidden units.
    
    index_carac : list
        Index of characters to learn.
      
    nb_images : int
        Number of images to generate.
    
    nb_iter : int
        Number of iterations in the Gradient Descent.
        
    lr : float
        Learning rate.
        
    batch_size : int
        The batch size.

    nb_iter_gibbs : int
        Number of iterations in Gibbs sampling.
        
    verbose : bool
        Enable verbose output.
    
    plot : bool
        Plot the images.
        
    Returns
    -------
    RBM : dict
        A RBM structure.
    """
    data = lire_alpha_digit(index_carac)
    p = data.shape[1]
    image_shape = (20,16)
    RBM = init_RBM(p, q)
    RBM_trained = train_RBM(RBM, data, nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)
    generer_image_RBM(RBM_trained, image_shape, nb_images, nb_iter_gibbs, plot=plot)
    return RBM_trained

"""## <a id='constructionDBN'>3.2 Construction d'un DBN et test sur Binary AlphaDigits</a>"""

def init_DBN(p, q=32, n_layers=2):
    """ 
    Create a DBN structure with weights and biases initialized.
    
    Parameters
    ----------
    p : int
        Number of units of visible layer.
    
    q : int
        Number of units of hidden layers.
    
    n_layers : int
        Number of layers.
        
    Returns
    -------
    DBN : list of dicts
    """
    DBN = [init_RBM(p, q)]
    
    for i in range(n_layers):
        DBN.append(init_RBM(q, q))
    return DBN
    
def train_DBN(DBN, input_data, nb_iter=100, lr=0.1, batch_size=32, verbose=True):
    """ 
    Train a DBN in a greedy layer-wise fashion.
    
    Parameters
    ----------
    DBN : list of dicts
        A DBN structure.
    
    input_data : ndarray
        Input data.
        
    nb_iter : int
        Number of iterations in the Gradient Descent.
        
    lr : float
        Learning rate.
        
    batch_size : int
        The batch size.
        
    verbose : bool
        Enable verbose output.
        
    Returns
    -------
    DBN : list of dicts
        A DBN structure.
    """
    DBN[0] = train_RBM(DBN[0], input_data, nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)
    proba = entree_sortie_RBM(DBN[0], input_data)
    new_input_data = (np.random.uniform(0, 1, size=(input_data.shape[0], proba.shape[1])) < proba).astype(int)
    
    for k in range(1, len(DBN)):
        DBN[k] = train_RBM(DBN[k], new_input_data, nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)
        proba = entree_sortie_RBM(DBN[k], new_input_data)
        new_input_data = (np.random.uniform(0, 1, size=(new_input_data.shape[0], proba.shape[1])) < proba).astype(int)

    return DBN

def generer_image_DBN(DBN, image_shape, nb_images=3, nb_iter_gibbs=100, plot=True):
    """ 
    Generate samples following a DBN.
    
    Parameters
    ----------
    DBN : list of dicts
        A DBN structure.
    
    image_shape : tuple
      Shape of the images
    
    nb_images : int
        Number of images to generate.
        
    nb_iter_gibbs : int
        Number of iterations in Gibbs sampling.
    
    plot : bool
        Plot the images.
        
    Returns
    -------

    """
    l = len(DBN)
    p = DBN[0]['p']
    q = DBN[0]['q']
    images = []

    if plot:
        plt.figure(figsize=(20, 20))
    for i in range(nb_images):
        x = (np.random.uniform(0, 1, size=(1, p)) < 0.5).astype(int)
        for j in range(nb_iter_gibbs):
            for k in range(l):
                h = (np.random.uniform(0, 1, size=(1, DBN[k]["q"])) < entree_sortie_RBM(DBN[k], x)).astype(int)
                x = h
            for k in range(l):
                x = (np.random.uniform(0, 1, size=(1, DBN[l-1-k]["p"])) < sortie_entree_RBM(DBN[l-1-k], h)).astype(int)
                h = x
        images.append(x)
    
        # Plot image 
        if plot:
            x = np.reshape(x, newshape=image_shape)
            plt.subplot(1, nb_images, i+1)
            plt.imshow(x, cmap='gray')

    return images

def principal_DBN_alpha(q, n_layers, index_carac, nb_images, nb_iter=100, lr=0.1, batch_size=32, nb_iter_gibbs=100, verbose=True, plot=True):
    """ 
    Learn characters of Binary AlphaDigits Binary  with a DBN.
    
    Parameters
    ----------
    q : int
        Number of hidden units.
    
    n_layers : int
        Number of layers.
        
    index_carac : list
        Index of characters to learn.
      
    nb_images : int
        Number of images to generate.
    
    nb_iter : int
        Number of iterations in the Gradient Descent.
        
    lr : float
        Learning rate.
        
    batch_size : int
        The batch size.

    nb_iter_gibbs : int
        Number of iterations in Gibbs sampling.
        
    verbose : bool
        Enable verbose output.
    
    plot : bool
        Plot the images.
        
    Returns
    -------
    DBN : dict
        A DBN structure.
    """
    data = lire_alpha_digit(index_carac)
    p = data.shape[1]
    image_shape = (20,16)
    
    DBN = init_DBN(p, q, n_layers=n_layers)
    DBN_trained = train_DBN(DBN, data, nb_iter=nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)
    images = generer_image_DBN(DBN_trained, image_shape=image_shape, nb_images=nb_images, nb_iter_gibbs=nb_iter_gibbs, plot=plot)
    return DBN_trained

"""## <a id='constructionDNN'>3.3 Construction d'un DNN et test sur MNIST</a>"""

def calcul_softmax(RBM, input_data):
    """ 
    Compute the probabilities from the output units with the softmax function.
    
    Parameters
    ----------
    RBM : dict
        A RBM structure.
    
    input_data : ndarray
        
    Returns
    -------
    probs : ndarray
        Output probability vector.
    """
    return softmax(RBM['b'] + input_data.dot(RBM['W']), axis=1)

def entree_sortie_reseau(DNN, input_data):
    """ 
    Compute the forward pass for given input data.

    Parameters
    ----------
    DNN : dict
        A DNN structure.
    
    input_data : ndarray
        
    Returns
    -------
    result : dict
    """
    result = dict()
    result[0] = input_data
    for i in range(0, len(DNN)-1):
        result[i+1] = entree_sortie_RBM(DNN[i], result[i])
    result[i+2] = calcul_softmax(DNN[i+1], result[i+1])
    return result
    
def accuracy(output, labels):
    """
    Computes classification accuracy.

    Parameters
    ----------
    y_pred : ndarray
        Predicted probabilities.

    y_true : ndarray
        Target probabilities.
    
    Returns
    -------
    result : float
    """
    correct = (output.argmax(axis=1) == labels.argmax(axis=1)).sum()
    return correct / len(labels)
    
def to_one_hot(y):
    """ 
    One-hot encode a vector.

    Parameters
    ----------
    y : ndarray
        Labels vector

    Returns
    -------
    ohe : ndarray
        One-hot encoding of y.
    """
    ohe = np.zeros((y.size, nb_labels))
    ohe[np.arange(y.size),y] = 1
    return ohe

def cross_entropy(y_pred, y_true):
    """ 
    One-hot encode a vector.

    Parameters
    ----------
    y_pred : ndarray
        Predicted probabilities.

    y_true : ndarray
        Target probabilities.
    
    Returns
    -------
    loss : float
    """
    loss = - np.mean(np.log(y_pred) * y_true)
    return loss
    
def retropropagation(DNN, input_data, labels, nb_iter=100, lr=0.1, batch_size=32, verbose=True):
    """ 
    Perform retropropagation to estimate the weights/biaises of the network from labeled data.
    
    Parameters
    ----------
    DNN : dict
        A DNN structure.

    nb_iter : int
        Number of iterations in the Gradient Descent.
        
    lr : float
        Learning rate.
        
    batch_size : int
        The batch size.

    input_data : ndarray

    labels : ndarray
        The labels of the input data.

    verbose : bool
        Enable verbose output.
        
    Returns
    -------
    DNN : dict
        A DNN structure.
    """
    # Add a softmax layer
    DNN.append(init_RBM(DNN[0]['q'], nb_labels))

    n_layers = len(DNN)
    n = input_data.shape[0]
    for i in range(nb_iter):
        total_loss = 0
        total_accuracy = 0

        for batch in range(0, batch_size):
            x_batch = input_data[np.minimum(np.arange(batch * batch_size, (batch+1) * batch_size), n-1)]
            y_batch = labels[np.minimum(np.arange(batch * batch_size, (batch+1) * batch_size), n-1)]
            y_batch_ohe = to_one_hot(y_batch)

            # Forward
            pred = entree_sortie_reseau(DNN, x_batch)
            total_loss += cross_entropy(pred[n_layers], y_batch_ohe)
            total_accuracy += accuracy(pred[n_layers], y_batch_ohe)

            for l in range(n_layers):
                # Backward
                if l == 0:
                    delta = pred[n_layers] - y_batch_ohe
                else:
                    delta = dA * (pred[n_layers-l] * (1 - pred[n_layers-l]))
                dW = 1/batch_size * pred[n_layers-l-1].T.dot(delta)
                db = 1/batch_size * np.sum(delta, axis=0)
                dA = delta.dot(DNN[n_layers-l-1]['W'].T)
                
                # Update weights
                DNN[n_layers-l-1]['W'] -= lr * dW
                DNN[n_layers-l-1]['b'] -= lr * db

        if verbose:
            print("iteration %d \t : \t loss %.5f - accuracy %.5f" % (i, total_loss/batch_size, total_accuracy/batch_size))

    return DNN

def test_DNN(DNN_trained, test_images, test_labels):
    """ 
    Test the performances of a trained network.

    Parameters
    ----------
    DNN_trained : dict
        A DNN structure.

    test_images : ndarray
      
    test_labels : ndarray

    Returns
    -------
    error_rate : float
    """
    y_ohe = to_one_hot(test_labels)
    pred = entree_sortie_reseau(DNN_trained, test_images)
    return 1 - accuracy(pred[len(pred)-1], y_ohe)

"""# <a id='travailprelim'>4. Travail préliminaire (Binary AlphaDigit)</a>"""

# Parameters
q = 256
index_carac = [2, 17, 25]
nb_images = 10
nb_iter = 100
lr = 0.1
batch_size = 64
nb_iter_gibbs = 50
verbose = True
plot = True

RBM_trained = principal_RBM_alpha(q, index_carac, nb_images, nb_iter, 
                                  lr, batch_size, nb_iter_gibbs, 
                                  verbose, plot)
# images = generer_image_RBM(RBM_trained, image_shape, nb_images, nb_iter_gibbs)

# Parameters
q = 64
n_layers = 3
index_carac = [2, 17, 25]
nb_images = 10
nb_iter = 100
lr = 0.1
batch_size = 64
nb_iter_gibbs = 100
verbose = True
plot = True

DBN_trained = principal_DBN_alpha(q, n_layers, index_carac, nb_images, nb_iter, lr, batch_size, nb_iter_gibbs, verbose, plot=plot)
# images = generer_image_DBN(DBN_trained, image_shape, nb_images, nb_iter_gibbs, plot=plot)

"""# <a id='mnist'>5. Etude à réaliser (MNIST)</a>

## 5.1 Programme
"""

def principal_mnist(n_samples, DNN_trained, pre_train, nb_iter=200, nb_iter_RBM=100, lr=0.1, batch_size=32, verbose=True):
    """ 
    Learn characters of MNIST.
    
    Parameters
    ----------
    n_samples : int
        Number of training data.

    DNN_trained : dict
        A DBN structure.

    pre_train : bool
        Pre-training of the network.
    
    nb_iter : int
        Number of iterations in the Gradient Descent of backpropagation.

    nb_iter_RBM : int
        Number of iterations in the Gradient Descent of RBM.
        
    lr : float
        Learning rate.
        
    batch_size : int
        The batch size.
        
    verbose : bool
        Enable verbose output.
    
    Returns
    -------
    DNN : dict
        A DBN structure.
    """
    mask = random.sample(list(np.arange(train_image.shape[0])), n_samples)
    input_data = train_image[mask]
    labels = train_label[mask]
    
    if pre_train:
        DNN_trained = train_DBN(DNN_trained, input_data.copy(), nb_iter=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=False)
        
    # Training
    DNN_trained = retropropagation(DNN_trained, input_data, labels, nb_iter=nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)

    return DNN_trained

n_samples = 60000
p = train_image.shape[1]
q = 256
n_layers = 1
pre_train = True
nb_iter = 50
nb_iter_RBM = 50
lr = 0.1
batch_size = 64
verbose = True

DNN_trained = init_DBN(p, q, n_layers)
DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)

print("Error rate on test set : %.5f" % (test_DNN(DNN_trained, test_image, test_label)))

"""## 5.2 Analyse"""

# Parameters
p = train_image.shape[1]
nb_iter = 200
nb_iter_RBM = 100
lr = 0.1
batch_size = 64
verbose = True

"""### 2 Réseaux en fonction du nombre de couches"""

# Parameters
n_samples = 60000
q = 200
n_layers_l = [2, 3, 5, 7]

error_rates_1, error_rates_2 = [], []

for n_layers in n_layers_l:
    # Initialization
    DNN_trained = init_DBN(p, q, n_layers)
    DNN_pre_trained = copy.deepcopy(DNN_trained)
    
    # Training
    DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train=False, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)
    DNN_pre_trained = principal_mnist(n_samples, DNN_pre_trained, pre_train=True, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)

    error_rates_1.append(test_DNN(DNN_trained, test_image, test_label))
    error_rates_2.append(test_DNN(DNN_pre_trained, test_image, test_label))

print(error_rates_1)
print(error_rates_2)

plt.plot(n_layers_l, error_rates_1, label='No pre-training', color='blue')
plt.plot(n_layers_l, error_rates_2, label='With pre-training', color='red')
plt.title('Error rate according to number of layers', size=20)
plt.xlabel('Number of layers', size=15)
plt.ylabel('Error rate', size=15)
plt.legend(fontsize=15)

fig = plt.gcf()
fig.set_size_inches(10,6)
fig.savefig('n_layers.jpg', dpi = 300, bbox_inches='tight', orientation = 'landscape')

"""### 2 Réseaux en fonction du nombre de neuronnes par couches"""

# Parameters
n_samples = 60000
q_l = [100, 300, 500, 700, 1000]
n_layers = 2

error_rates_1, error_rates_2 = [], []

for q in q_l:
    # Initialization
    DNN_trained = init_DBN(p, q, n_layers)
    DNN_pre_trained = copy.deepcopy(DNN_trained)
    
    # Training
    DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train=False, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)
    DNN_pre_trained = principal_mnist(n_samples, DNN_pre_trained, pre_train=True, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)

    error_rates_1.append(test_DNN(DNN_trained, test_image, test_label))
    error_rates_2.append(test_DNN(DNN_pre_trained, test_image, test_label))

print(error_rates_1)
print(error_rates_2)

plt.plot(q_l, error_rates_1, label='No pre-training', color='blue')
plt.plot(q_l, error_rates_2, label='With pre-training', color='red')
plt.title('Error rate according to number of neurons per layer', size=20)
plt.xlabel('Number of neurons', size=15)
plt.ylabel('Error rate', size=15)
plt.legend(fontsize=15)

fig = plt.gcf()
fig.set_size_inches(10,6)
fig.savefig('n_neurons.jpg', dpi = 300, bbox_inches='tight', orientation = 'landscape')

"""### 3 Réseaux en fonction du nombre de données train"""

# Parameters
n_samples_l = [1000, 3000, 7000, 10000, 30000, 60000]
q = 200
n_layers = 2

error_rates_1, error_rates_2 = [], []

for n_samples in n_samples_l:
    # Initialization
    DNN_trained = init_DBN(p, q, n_layers)
    DNN_pre_trained = copy.deepcopy(DNN_trained)
    
    # Training
    DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train=False, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)
    DNN_pre_trained = principal_mnist(n_samples, DNN_pre_trained, pre_train=True, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)

    error_rates_1.append(test_DNN(DNN_trained, test_image, test_label))
    error_rates_2.append(test_DNN(DNN_pre_trained, test_image, test_label))

print(error_rates_1)
print(error_rates_2)

plt.plot(n_samples_l, error_rates_1, label='No pre-training', color='blue')
plt.plot(n_samples_l, error_rates_2, label='With pre-training', color='red')
plt.title('Error rate according to number of training images', size=20)
plt.xlabel('Number of training images', size=15)
plt.ylabel('Error rate', size=15)
plt.legend(fontsize=15)

fig = plt.gcf()
fig.set_size_inches(10,6)
fig.savefig('n_samples.jpg', dpi = 300, bbox_inches='tight', orientation = 'landscape')

