{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rwm29eYSYkMN",
    "outputId": "52e485b8-2c49-4417-82ca-61d92ec19bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1nnsLbtFQ944iU-bw06I34kuQS7n-VBdL into ./data.zip... Done.\n"
     ]
    }
   ],
   "source": [
    "colab = True\n",
    "if colab:\n",
    "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "    gdd.download_file_from_google_drive(file_id='1nnsLbtFQ944iU-bw06I34kuQS7n-VBdL', dest_path='./data.zip')\n",
    "\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile('data.zip', 'r') as zipObj:\n",
    "        # Extract all the contents of zip file in current directory\n",
    "        zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZINhhANCk1WW"
   },
   "outputs": [],
   "source": [
    "# Unzip mnist data\n",
    "!gzip -d ./data/t10k-images-idx3-ubyte.gz ./data/t10k-labels-idx1-ubyte.gz ./data/train-labels-idx1-ubyte.gz ./data/train-images-idx3-ubyte.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mADF0BDHlsgh",
    "outputId": "07d53e77-68fe-4506-e8ff-bc96185f7a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting idx2numpy\n",
      "  Downloading https://files.pythonhosted.org/packages/23/6b/abab4652eb249f432c62431907c8de32bdcedb5abdf869ff86653efff981/idx2numpy-1.2.2.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.18.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.12.0)\n",
      "Building wheels for collected packages: idx2numpy\n",
      "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for idx2numpy: filename=idx2numpy-1.2.2-cp36-none-any.whl size=8032 sha256=da2b80611b3d74494f3ac677498848a688493262221488ae6b40d90b1cd01f5b\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/b5/69/3e0757b3086607e95db70661798fdf98a77a0bb79c54e1f320\n",
      "Successfully built idx2numpy\n",
      "Installing collected packages: idx2numpy\n",
      "Successfully installed idx2numpy-1.2.2\n"
     ]
    }
   ],
   "source": [
    "# Install idx2numpy for reading MNIST\n",
    "!pip install idx2numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tg2UfRqfX3yo"
   },
   "source": [
    "<h1 align='center'>Projet Deep Learning II</h1>\n",
    "\n",
    "<h4 align='center'>Authors : Kevin XU & Qin WANG</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCe0CohUX3yt"
   },
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#intro)\n",
    "2. [Données](#donnees)\n",
    "3. [Fonctions élémentaires](#fctelementaire)\n",
    "    + [Construction d'un RBM et test sur Binary AlphaDigits](#constructionRBM)\n",
    "    + [Construction d’un DBN et test sur Binary AlphaDigits](#constructionDBN)\n",
    "    + [Construction d’un DNN et test sur MNIST](#constructionDNN)\n",
    "4. [Travail préliminaire (Binary AlphaDigit)](#travailprelim)\n",
    "5. [Etude à réaliser (MNIST)](#mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHVTKgrkX3yv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import scipy.io\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXkssCCHX3zC"
   },
   "source": [
    "\n",
    "# <a id='intro'>1. Introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3KnIepUX3zE"
   },
   "source": [
    "L’objectif du projet est de réaliser un réseau de neurones profond pré-entraîné ou non pour la classification de chiffres manuscrits. On va comparer les performances, en terme de taux de bonnes classifications, d’un réseau pré-entrainé et d’un réseau initialisé aléatoirement, en fonction du nombre de données d’apprentissage, du nombre de couches du réseau et enfin du nombre de neurones par couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5n1Xs2shX3zG"
   },
   "source": [
    "\n",
    "# <a id='donnees'>2. Données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OQIzn-rX3zH"
   },
   "source": [
    "**To download :**\n",
    "+ http://yann.lecun.com/exdb/mnist/ (les 4 fichiers)\n",
    "+ https://cs.nyu.edu/~roweis/data.html (Binary Alphadigits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPwm7h6IX3zI"
   },
   "outputs": [],
   "source": [
    "path_to_data = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d6PYzX1mX3zS",
    "outputId": "4d605c82-9501-408a-b05d-46a10a847dec",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 320)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for Binary Alphadigits dataset\n",
    "alphadigs = scipy.io.loadmat(path_to_data + 'binaryalphadigs.mat')\n",
    "\n",
    "def lire_alpha_digit(index_carac):\n",
    "    \"\"\" \n",
    "    Get alpha digit data for given character.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    index_carac : list \n",
    "        The characters that we want to learn.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : ndarray\n",
    "        Matrix containing the data with rows corresponding to samples and columns to pixels.\n",
    "    \"\"\"\n",
    "    nb_pixel = alphadigs['dat'][0,0].shape[0] * alphadigs['dat'][0,0].shape[1]\n",
    "    data = np.empty(shape=(1, nb_pixel))\n",
    "\n",
    "    for i in index_carac:\n",
    "        data = np.append(data, np.array([img.flatten() for img in alphadigs['dat'][i,:]]), axis=0)\n",
    "    return data[1:,:]\n",
    "\n",
    "alphadigs_data = lire_alpha_digit([10])\n",
    "alphadigs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cftyMv36X3zd"
   },
   "outputs": [],
   "source": [
    "# Function for MNIST dataset\n",
    "def to_black_white(images):\n",
    "    images[images < 128] = 0\n",
    "    images[images >= 128] = 1\n",
    "    return images\n",
    "\n",
    "def lire_MNIST():\n",
    "    \"\"\" \n",
    "    Get alpha digit data for given character.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : ndarray\n",
    "        Matrix containing the data with rows corresponding to samples and columns to pixels.\n",
    "    \"\"\"\n",
    "    train_image_file = 'data/train-images-idx3-ubyte'\n",
    "    train_label_file = 'data/train-labels-idx1-ubyte'\n",
    "    test_image_file = 'data/t10k-images-idx3-ubyte'\n",
    "    test_label_file = 'data/t10k-labels-idx1-ubyte'\n",
    "\n",
    "    train_image = idx2numpy.convert_from_file(train_image_file)\n",
    "    train_image = to_black_white(np.array([img.flatten() for img in train_image]))\n",
    "\n",
    "    test_image = idx2numpy.convert_from_file(test_image_file)\n",
    "    test_image = to_black_white(np.array([img.flatten() for img in test_image]))\n",
    "\n",
    "    train_label = idx2numpy.convert_from_file(train_label_file)\n",
    "    test_label = idx2numpy.convert_from_file(test_label_file)\n",
    "\n",
    "    return train_image, train_label, test_image, test_label\n",
    "\n",
    "train_image, train_label, test_image, test_label = lire_MNIST()\n",
    "nb_labels = np.unique(train_label).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "xiYQNjydtDZl",
    "outputId": "18cf427d-3a02-4ad4-b23e-166c48c7d152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0071165dd8>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_image.shape)\n",
    "print(train_label.shape)\n",
    "print(test_image.shape)\n",
    "print(test_label.shape)\n",
    "plt.imshow(np.reshape(train_image[1], newshape=(28,28)), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIhnpmslX3zk"
   },
   "source": [
    "\n",
    "# <a id='fctelementaire'>3. Fonctions élémentaires</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJD1SWlNX3zm"
   },
   "source": [
    "\n",
    "## <a id='constructionRBM'>3.1 Construction d'un RBM et test sur Binary AlphaDigits</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXutXEURX3zt"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" \n",
    "    Sigmoid function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : float\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def init_RBM(p, q):\n",
    "    \"\"\" \n",
    "    Create a RBM structure with weights and biases initialized.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : int\n",
    "        Number of visible units.\n",
    "    \n",
    "    q : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    RBM : dict\n",
    "    \"\"\"\n",
    "    RBM = {\n",
    "        \"W\" : np.random.randn(p, q) * 0.1,\n",
    "        \"a\" : np.zeros(shape=(1, p)),\n",
    "        \"b\" : np.zeros(shape=(1, q)),\n",
    "        \"p\" : p,\n",
    "        \"q\" : q\n",
    "    }\n",
    "    return RBM\n",
    "\n",
    "def entree_sortie_RBM(RBM, input_data):\n",
    "    \"\"\" \n",
    "    Compute the values of the hidden layer for given input data by the RBM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RBM : dict\n",
    "        A RBM structure.\n",
    "    \n",
    "    input_data : ndarray\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : ndarray\n",
    "    \"\"\"\n",
    "    return sigmoid(RBM['b'] + input_data.dot(RBM['W']))\n",
    "\n",
    "def sortie_entree_RBM(RBM, output_data):\n",
    "    \"\"\" \n",
    "    Compute the values of the visible layer for given output data by the RBM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RBM : dict\n",
    "        A RBM structure.\n",
    "    \n",
    "    output_data : ndarray\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : ndarray\n",
    "    \"\"\"\n",
    "    return sigmoid(RBM['a'] + output_data.dot(RBM['W'].T))\n",
    "\n",
    "def train_RBM(RBM, input_data, nb_iter=100, lr=0.1, batch_size=32, verbose=True):\n",
    "    \"\"\" \n",
    "    Train a RBM by Contrastive-Divergence (CD-1) algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RBM : dict\n",
    "        A RBM structure.\n",
    "    \n",
    "    input_data : ndarray\n",
    "        Input data.\n",
    "        \n",
    "    nb_iter : int\n",
    "        Number of iterations.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "        \n",
    "    verbose : bool\n",
    "        Enable verbose output.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    RBM : dict\n",
    "        A RBM structure.\n",
    "    \"\"\"\n",
    "    n = input_data.shape[0]\n",
    "    p = RBM['p']\n",
    "    q = RBM['q']\n",
    "    for i in range(nb_iter):\n",
    "        np.random.shuffle(input_data) \n",
    "        for batch in range(batch_size):\n",
    "            x = input_data[np.minimum(np.arange(batch * batch_size, (batch + 1) * batch_size), n-1),:]\n",
    "            v_0 = x\n",
    "            h_0 = (np.random.uniform(0, 1, size=(x.shape[0], q)) < entree_sortie_RBM(RBM, x)).astype(int)\n",
    "            v_1 = (np.random.uniform(0, 1, size=(x.shape[0], p)) < sortie_entree_RBM(RBM, h_0)).astype(int)\n",
    "            \n",
    "            da = np.sum(v_0 - v_1, axis=0)\n",
    "            db = np.sum(entree_sortie_RBM(RBM, x) - entree_sortie_RBM(RBM, v_1), axis=0)\n",
    "            dW = v_0.T.dot(entree_sortie_RBM(RBM, x)) - v_1.T.dot(entree_sortie_RBM(RBM, v_1))\n",
    "\n",
    "            RBM['W'] += lr * dW / batch_size      \n",
    "            RBM['a'] += lr * da / batch_size\n",
    "            RBM['b'] += lr * db / batch_size\n",
    "            \n",
    "        if verbose:\n",
    "            # erreur quadratique\n",
    "            sortie = entree_sortie_RBM(RBM, input_data)\n",
    "            new_entree = sortie_entree_RBM(RBM, sortie)\n",
    "            erreur_reconstruction = np.sum((input_data - new_entree)**2/n)\n",
    "            print(\"iteration %d \\t : \\t erreur reconstruction %.2f\" % (i, erreur_reconstruction))\n",
    "    return RBM\n",
    "\n",
    "def generer_image_RBM(RBM, image_shape, nb_images=3, nb_iter_gibbs=100, plot=True):\n",
    "    \"\"\" \n",
    "    Generate samples following a RBM by Gibbs sampling algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RBM : dict\n",
    "        A RBM structure.\n",
    "\n",
    "    image_shape : tuple\n",
    "      Shape of the images\n",
    "    \n",
    "    nb_images : int\n",
    "        Number of images to generate.\n",
    "        \n",
    "    nb_iter_gibbs : int\n",
    "        Number of iterations in Gibbs sampling.\n",
    "    \n",
    "    plot : bool\n",
    "        Plot the images.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    images : list of arrays\n",
    "        Generated samples.\n",
    "    \"\"\"\n",
    "    p = RBM['p']\n",
    "    q = RBM['q']\n",
    "    images = []\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(20, 20))\n",
    "    for i in range(nb_images):\n",
    "        x = (np.random.uniform(0, 1, size=(1, p)) < 0.5).astype(int)\n",
    "        for j in range(nb_iter_gibbs):\n",
    "            h = (np.random.uniform(0, 1, size=(1, q)) < entree_sortie_RBM(RBM, x)).astype(int)\n",
    "            x = (np.random.uniform(0, 1, size=(1, p)) < sortie_entree_RBM(RBM, h)).astype(int)\n",
    "        images.append(x)\n",
    "\n",
    "        # Plot image \n",
    "        if plot:\n",
    "            x = np.reshape(x, newshape=image_shape)\n",
    "            plt.subplot(1, nb_images, i+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(x, cmap='gray')\n",
    "    \n",
    "    return images\n",
    "\n",
    "def principal_RBM_alpha(q, index_carac, nb_images, nb_iter=100, lr=0.1, batch_size=32, nb_iter_gibbs=100, verbose=1, plot=True):\n",
    "    \"\"\" \n",
    "    Learn characters of Binary AlphaDigits with a RBM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    q : int\n",
    "        Number of hidden units.\n",
    "    \n",
    "    index_carac : list\n",
    "        Index of characters to learn.\n",
    "      \n",
    "    nb_images : int\n",
    "        Number of images to generate.\n",
    "    \n",
    "    nb_iter : int\n",
    "        Number of iterations in the Gradient Descent.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "\n",
    "    nb_iter_gibbs : int\n",
    "        Number of iterations in Gibbs sampling.\n",
    "        \n",
    "    verbose : bool\n",
    "        Enable verbose output.\n",
    "    \n",
    "    plot : bool\n",
    "        Plot the images.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    RBM : dict\n",
    "        A RBM structure.\n",
    "    \"\"\"\n",
    "    data = lire_alpha_digit(index_carac)\n",
    "    p = data.shape[1]\n",
    "    image_shape = (20,16)\n",
    "    RBM = init_RBM(p, q)\n",
    "    RBM_trained = train_RBM(RBM, data, nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "    generer_image_RBM(RBM_trained, image_shape, nb_images, nb_iter_gibbs, plot=plot)\n",
    "    return RBM_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqa_AlpSX3z4"
   },
   "source": [
    "\n",
    "## <a id='constructionDBN'>3.2 Construction d'un DBN et test sur Binary AlphaDigits</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zt-ZVGm6X30M"
   },
   "outputs": [],
   "source": [
    "def init_DBN(p, q=32, n_layers=2):\n",
    "    \"\"\" \n",
    "    Create a DBN structure with weights and biases initialized.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : int\n",
    "        Number of units of visible layer.\n",
    "    \n",
    "    q : int\n",
    "        Number of units of hidden layers.\n",
    "    \n",
    "    n_layers : int\n",
    "        Number of layers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DBN : list of dicts\n",
    "    \"\"\"\n",
    "    DBN = [init_RBM(p, q)]\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        DBN.append(init_RBM(q, q))\n",
    "    return DBN\n",
    "    \n",
    "def train_DBN(DBN, input_data, nb_iter=100, lr=0.1, batch_size=32, verbose=True):\n",
    "    \"\"\" \n",
    "    Train a DBN in a greedy layer-wise fashion.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    DBN : list of dicts\n",
    "        A DBN structure.\n",
    "    \n",
    "    input_data : ndarray\n",
    "        Input data.\n",
    "        \n",
    "    nb_iter : int\n",
    "        Number of iterations in the Gradient Descent.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "        \n",
    "    verbose : bool\n",
    "        Enable verbose output.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DBN : list of dicts\n",
    "        A DBN structure.\n",
    "    \"\"\"\n",
    "    DBN[0] = train_RBM(DBN[0], input_data, nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "    proba = entree_sortie_RBM(DBN[0], input_data)\n",
    "    new_input_data = (np.random.uniform(0, 1, size=(input_data.shape[0], proba.shape[1])) < proba).astype(int)\n",
    "    \n",
    "    for k in range(1, len(DBN)):\n",
    "        DBN[k] = train_RBM(DBN[k], new_input_data, nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "        proba = entree_sortie_RBM(DBN[k], new_input_data)\n",
    "        new_input_data = (np.random.uniform(0, 1, size=(new_input_data.shape[0], proba.shape[1])) < proba).astype(int)\n",
    "\n",
    "    return DBN\n",
    "\n",
    "def generer_image_DBN(DBN, image_shape, nb_images=3, nb_iter_gibbs=100, plot=True):\n",
    "    \"\"\" \n",
    "    Generate samples following a DBN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    DBN : list of dicts\n",
    "        A DBN structure.\n",
    "    \n",
    "    image_shape : tuple\n",
    "      Shape of the images\n",
    "    \n",
    "    nb_images : int\n",
    "        Number of images to generate.\n",
    "        \n",
    "    nb_iter_gibbs : int\n",
    "        Number of iterations in Gibbs sampling.\n",
    "    \n",
    "    plot : bool\n",
    "        Plot the images.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    l = len(DBN)\n",
    "    p = DBN[0]['p']\n",
    "    q = DBN[0]['q']\n",
    "    images = []\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(20, 20))\n",
    "    for i in range(nb_images):\n",
    "        x = (np.random.uniform(0, 1, size=(1, p)) < 0.5).astype(int)\n",
    "        for j in range(nb_iter_gibbs):\n",
    "            for k in range(l):\n",
    "                h = (np.random.uniform(0, 1, size=(1, DBN[k][\"q\"])) < entree_sortie_RBM(DBN[k], x)).astype(int)\n",
    "                x = h\n",
    "            for k in range(l):\n",
    "                x = (np.random.uniform(0, 1, size=(1, DBN[l-1-k][\"p\"])) < sortie_entree_RBM(DBN[l-1-k], h)).astype(int)\n",
    "                h = x\n",
    "        images.append(x)\n",
    "    \n",
    "        # Plot image \n",
    "        if plot:\n",
    "            x = np.reshape(x, newshape=image_shape)\n",
    "            plt.subplot(1, nb_images, i+1)\n",
    "            plt.imshow(x, cmap='gray')\n",
    "\n",
    "    return images\n",
    "\n",
    "def principal_DBN_alpha(q, n_layers, index_carac, nb_images, nb_iter=100, lr=0.1, batch_size=32, nb_iter_gibbs=100, verbose=True, plot=True):\n",
    "    \"\"\" \n",
    "    Learn characters of Binary AlphaDigits Binary  with a DBN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    q : int\n",
    "        Number of hidden units.\n",
    "    \n",
    "    n_layers : int\n",
    "        Number of layers.\n",
    "        \n",
    "    index_carac : list\n",
    "        Index of characters to learn.\n",
    "      \n",
    "    nb_images : int\n",
    "        Number of images to generate.\n",
    "    \n",
    "    nb_iter : int\n",
    "        Number of iterations in the Gradient Descent.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "\n",
    "    nb_iter_gibbs : int\n",
    "        Number of iterations in Gibbs sampling.\n",
    "        \n",
    "    verbose : bool\n",
    "        Enable verbose output.\n",
    "    \n",
    "    plot : bool\n",
    "        Plot the images.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DBN : dict\n",
    "        A DBN structure.\n",
    "    \"\"\"\n",
    "    data = lire_alpha_digit(index_carac)\n",
    "    p = data.shape[1]\n",
    "    image_shape = (20,16)\n",
    "    \n",
    "    DBN = init_DBN(p, q, n_layers=n_layers)\n",
    "    DBN_trained = train_DBN(DBN, data, nb_iter=nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "    images = generer_image_DBN(DBN_trained, image_shape=image_shape, nb_images=nb_images, nb_iter_gibbs=nb_iter_gibbs, plot=plot)\n",
    "    return DBN_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4td7rf78X30R"
   },
   "source": [
    "\n",
    "## <a id='constructionDNN'>3.3 Construction d'un DNN et test sur MNIST</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NQ9GSxWkK-2"
   },
   "outputs": [],
   "source": [
    "def calcul_softmax(RBM, input_data):\n",
    "    \"\"\" \n",
    "    Compute the probabilities from the output units with the softmax function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RBM : dict\n",
    "        A RBM structure.\n",
    "    \n",
    "    input_data : ndarray\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    probs : ndarray\n",
    "        Output probability vector.\n",
    "    \"\"\"\n",
    "    return softmax(RBM['b'] + input_data.dot(RBM['W']), axis=1)\n",
    "\n",
    "def entree_sortie_reseau(DNN, input_data):\n",
    "    \"\"\" \n",
    "    Compute the forward pass for given input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    DNN : dict\n",
    "        A DNN structure.\n",
    "    \n",
    "    input_data : ndarray\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : dict\n",
    "    \"\"\"\n",
    "    result = dict()\n",
    "    result[0] = input_data\n",
    "    for i in range(0, len(DNN)-1):\n",
    "        result[i+1] = entree_sortie_RBM(DNN[i], result[i])\n",
    "    result[i+2] = calcul_softmax(DNN[i+1], result[i+1])\n",
    "    return result\n",
    "    \n",
    "def accuracy(output, labels):\n",
    "    \"\"\"\n",
    "    Computes classification accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : ndarray\n",
    "        Predicted probabilities.\n",
    "\n",
    "    y_true : ndarray\n",
    "        Target probabilities.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : float\n",
    "    \"\"\"\n",
    "    correct = (output.argmax(axis=1) == labels.argmax(axis=1)).sum()\n",
    "    return correct / len(labels)\n",
    "    \n",
    "def to_one_hot(y):\n",
    "    \"\"\" \n",
    "    One-hot encode a vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Labels vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ohe : ndarray\n",
    "        One-hot encoding of y.\n",
    "    \"\"\"\n",
    "    ohe = np.zeros((y.size, nb_labels))\n",
    "    ohe[np.arange(y.size),y] = 1\n",
    "    return ohe\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    \"\"\" \n",
    "    One-hot encode a vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : ndarray\n",
    "        Predicted probabilities.\n",
    "\n",
    "    y_true : ndarray\n",
    "        Target probabilities.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    loss = - np.mean(np.log(y_pred) * y_true)\n",
    "    return loss\n",
    "    \n",
    "def retropropagation(DNN, input_data, labels, nb_iter=100, lr=0.1, batch_size=32, verbose=True):\n",
    "    \"\"\" \n",
    "    Perform retropropagation to estimate the weights/biaises of the network from labeled data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    DNN : dict\n",
    "        A DNN structure.\n",
    "\n",
    "    nb_iter : int\n",
    "        Number of iterations in the Gradient Descent.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "\n",
    "    input_data : ndarray\n",
    "\n",
    "    labels : ndarray\n",
    "        The labels of the input data.\n",
    "\n",
    "    verbose : bool\n",
    "        Enable verbose output.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DNN : dict\n",
    "        A DNN structure.\n",
    "    \"\"\"\n",
    "    # Add a softmax layer\n",
    "    DNN.append(init_RBM(DNN[0]['q'], nb_labels))\n",
    "\n",
    "    n_layers = len(DNN)\n",
    "    n = input_data.shape[0]\n",
    "    for i in range(nb_iter):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        for batch in range(0, batch_size):\n",
    "            x_batch = input_data[np.minimum(np.arange(batch * batch_size, (batch+1) * batch_size), n-1)]\n",
    "            y_batch = labels[np.minimum(np.arange(batch * batch_size, (batch+1) * batch_size), n-1)]\n",
    "            y_batch_ohe = to_one_hot(y_batch)\n",
    "\n",
    "            # Forward\n",
    "            pred = entree_sortie_reseau(DNN, x_batch)\n",
    "            total_loss += cross_entropy(pred[n_layers], y_batch_ohe)\n",
    "            total_accuracy += accuracy(pred[n_layers], y_batch_ohe)\n",
    "\n",
    "            for l in range(n_layers):\n",
    "                # Backward\n",
    "                if l == 0:\n",
    "                    delta = pred[n_layers] - y_batch_ohe\n",
    "                else:\n",
    "                    delta = dA * (pred[n_layers-l] * (1 - pred[n_layers-l]))\n",
    "                dW = 1/batch_size * pred[n_layers-l-1].T.dot(delta)\n",
    "                db = 1/batch_size * np.sum(delta, axis=0)\n",
    "                dA = delta.dot(DNN[n_layers-l-1]['W'].T)\n",
    "                \n",
    "                # Update weights\n",
    "                DNN[n_layers-l-1]['W'] -= lr * dW\n",
    "                DNN[n_layers-l-1]['b'] -= lr * db\n",
    "\n",
    "        if verbose:\n",
    "            print(\"iteration %d \\t : \\t loss %.5f - accuracy %.5f\" % (i, total_loss/batch_size, total_accuracy/batch_size))\n",
    "\n",
    "    return DNN\n",
    "\n",
    "def test_DNN(DNN_trained, test_images, test_labels):\n",
    "    \"\"\" \n",
    "    Test the performances of a trained network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    DNN_trained : dict\n",
    "        A DNN structure.\n",
    "\n",
    "    test_images : ndarray\n",
    "      \n",
    "    test_labels : ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    error_rate : float\n",
    "    \"\"\"\n",
    "    y_ohe = to_one_hot(test_labels)\n",
    "    pred = entree_sortie_reseau(DNN_trained, test_images)\n",
    "    return 1 - accuracy(pred[len(pred)-1], y_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bt0bvkeVX30T"
   },
   "source": [
    "\n",
    "# <a id='travailprelim'>4. Travail préliminaire (Binary AlphaDigit)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QMxnOw_HX30U",
    "outputId": "56f8a9d0-e75b-454d-9012-d0ac50cb65fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 \t : \t erreur reconstruction 92.93\n",
      "iteration 1 \t : \t erreur reconstruction 121.85\n",
      "iteration 2 \t : \t erreur reconstruction 101.82\n",
      "iteration 3 \t : \t erreur reconstruction 90.60\n",
      "iteration 4 \t : \t erreur reconstruction 112.39\n",
      "iteration 5 \t : \t erreur reconstruction 79.80\n",
      "iteration 6 \t : \t erreur reconstruction 109.72\n",
      "iteration 7 \t : \t erreur reconstruction 83.89\n",
      "iteration 8 \t : \t erreur reconstruction 79.87\n",
      "iteration 9 \t : \t erreur reconstruction 81.66\n",
      "iteration 10 \t : \t erreur reconstruction 72.41\n",
      "iteration 11 \t : \t erreur reconstruction 73.62\n",
      "iteration 12 \t : \t erreur reconstruction 95.78\n",
      "iteration 13 \t : \t erreur reconstruction 87.13\n",
      "iteration 14 \t : \t erreur reconstruction 81.01\n",
      "iteration 15 \t : \t erreur reconstruction 80.87\n",
      "iteration 16 \t : \t erreur reconstruction 69.40\n",
      "iteration 17 \t : \t erreur reconstruction 69.02\n",
      "iteration 18 \t : \t erreur reconstruction 70.87\n",
      "iteration 19 \t : \t erreur reconstruction 71.81\n",
      "iteration 20 \t : \t erreur reconstruction 73.98\n",
      "iteration 21 \t : \t erreur reconstruction 64.44\n",
      "iteration 22 \t : \t erreur reconstruction 71.56\n",
      "iteration 23 \t : \t erreur reconstruction 91.80\n",
      "iteration 24 \t : \t erreur reconstruction 74.65\n",
      "iteration 25 \t : \t erreur reconstruction 59.41\n",
      "iteration 26 \t : \t erreur reconstruction 68.47\n",
      "iteration 27 \t : \t erreur reconstruction 59.28\n",
      "iteration 28 \t : \t erreur reconstruction 51.38\n",
      "iteration 29 \t : \t erreur reconstruction 64.16\n",
      "iteration 30 \t : \t erreur reconstruction 53.95\n",
      "iteration 31 \t : \t erreur reconstruction 48.15\n",
      "iteration 32 \t : \t erreur reconstruction 44.96\n",
      "iteration 33 \t : \t erreur reconstruction 47.38\n",
      "iteration 34 \t : \t erreur reconstruction 49.49\n",
      "iteration 35 \t : \t erreur reconstruction 57.05\n",
      "iteration 36 \t : \t erreur reconstruction 52.76\n",
      "iteration 37 \t : \t erreur reconstruction 49.68\n",
      "iteration 38 \t : \t erreur reconstruction 43.82\n",
      "iteration 39 \t : \t erreur reconstruction 49.77\n",
      "iteration 40 \t : \t erreur reconstruction 47.53\n",
      "iteration 41 \t : \t erreur reconstruction 57.54\n",
      "iteration 42 \t : \t erreur reconstruction 46.93\n",
      "iteration 43 \t : \t erreur reconstruction 41.38\n",
      "iteration 44 \t : \t erreur reconstruction 44.42\n",
      "iteration 45 \t : \t erreur reconstruction 43.77\n",
      "iteration 46 \t : \t erreur reconstruction 48.34\n",
      "iteration 47 \t : \t erreur reconstruction 45.24\n",
      "iteration 48 \t : \t erreur reconstruction 41.66\n",
      "iteration 49 \t : \t erreur reconstruction 38.57\n",
      "iteration 50 \t : \t erreur reconstruction 39.21\n",
      "iteration 51 \t : \t erreur reconstruction 44.12\n",
      "iteration 52 \t : \t erreur reconstruction 44.95\n",
      "iteration 53 \t : \t erreur reconstruction 39.86\n",
      "iteration 54 \t : \t erreur reconstruction 38.64\n",
      "iteration 55 \t : \t erreur reconstruction 38.23\n",
      "iteration 56 \t : \t erreur reconstruction 37.16\n",
      "iteration 57 \t : \t erreur reconstruction 35.46\n",
      "iteration 58 \t : \t erreur reconstruction 37.87\n",
      "iteration 59 \t : \t erreur reconstruction 36.12\n",
      "iteration 60 \t : \t erreur reconstruction 34.80\n",
      "iteration 61 \t : \t erreur reconstruction 35.79\n",
      "iteration 62 \t : \t erreur reconstruction 44.68\n",
      "iteration 63 \t : \t erreur reconstruction 35.99\n",
      "iteration 64 \t : \t erreur reconstruction 38.96\n",
      "iteration 65 \t : \t erreur reconstruction 34.74\n",
      "iteration 66 \t : \t erreur reconstruction 31.99\n",
      "iteration 67 \t : \t erreur reconstruction 33.38\n",
      "iteration 68 \t : \t erreur reconstruction 36.79\n",
      "iteration 69 \t : \t erreur reconstruction 36.36\n",
      "iteration 70 \t : \t erreur reconstruction 35.76\n",
      "iteration 71 \t : \t erreur reconstruction 34.62\n",
      "iteration 72 \t : \t erreur reconstruction 34.69\n",
      "iteration 73 \t : \t erreur reconstruction 33.02\n",
      "iteration 74 \t : \t erreur reconstruction 31.10\n",
      "iteration 75 \t : \t erreur reconstruction 32.22\n",
      "iteration 76 \t : \t erreur reconstruction 30.63\n",
      "iteration 77 \t : \t erreur reconstruction 29.32\n",
      "iteration 78 \t : \t erreur reconstruction 29.34\n",
      "iteration 79 \t : \t erreur reconstruction 28.18\n",
      "iteration 80 \t : \t erreur reconstruction 27.22\n",
      "iteration 81 \t : \t erreur reconstruction 28.02\n",
      "iteration 82 \t : \t erreur reconstruction 31.36\n",
      "iteration 83 \t : \t erreur reconstruction 31.66\n",
      "iteration 84 \t : \t erreur reconstruction 27.97\n",
      "iteration 85 \t : \t erreur reconstruction 26.74\n",
      "iteration 86 \t : \t erreur reconstruction 27.10\n",
      "iteration 87 \t : \t erreur reconstruction 29.15\n",
      "iteration 88 \t : \t erreur reconstruction 26.61\n",
      "iteration 89 \t : \t erreur reconstruction 28.01\n",
      "iteration 90 \t : \t erreur reconstruction 29.11\n",
      "iteration 91 \t : \t erreur reconstruction 26.81\n",
      "iteration 92 \t : \t erreur reconstruction 25.27\n",
      "iteration 93 \t : \t erreur reconstruction 24.63\n",
      "iteration 94 \t : \t erreur reconstruction 25.64\n",
      "iteration 95 \t : \t erreur reconstruction 26.06\n",
      "iteration 96 \t : \t erreur reconstruction 26.64\n",
      "iteration 97 \t : \t erreur reconstruction 25.35\n",
      "iteration 98 \t : \t erreur reconstruction 25.56\n",
      "iteration 99 \t : \t erreur reconstruction 23.73\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "q = 256\n",
    "index_carac = [2, 17, 25]\n",
    "nb_images = 10\n",
    "nb_iter = 100\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "nb_iter_gibbs = 50\n",
    "verbose = True\n",
    "plot = True\n",
    "\n",
    "RBM_trained = principal_RBM_alpha(q, index_carac, nb_images, nb_iter, \n",
    "                                  lr, batch_size, nb_iter_gibbs, \n",
    "                                  verbose, plot)\n",
    "# images = generer_image_RBM(RBM_trained, image_shape, nb_images, nb_iter_gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "F-uZgDiltwIy",
    "outputId": "c5c39718-9dd4-4c88-9c9d-0b449f9cc0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 \t : \t erreur reconstruction 100.75\n",
      "iteration 1 \t : \t erreur reconstruction 89.12\n",
      "iteration 2 \t : \t erreur reconstruction 122.64\n",
      "iteration 3 \t : \t erreur reconstruction 120.80\n",
      "iteration 4 \t : \t erreur reconstruction 87.18\n",
      "iteration 5 \t : \t erreur reconstruction 94.44\n",
      "iteration 6 \t : \t erreur reconstruction 95.19\n",
      "iteration 7 \t : \t erreur reconstruction 85.61\n",
      "iteration 8 \t : \t erreur reconstruction 71.81\n",
      "iteration 9 \t : \t erreur reconstruction 84.97\n",
      "iteration 10 \t : \t erreur reconstruction 110.90\n",
      "iteration 11 \t : \t erreur reconstruction 81.10\n",
      "iteration 12 \t : \t erreur reconstruction 69.19\n",
      "iteration 13 \t : \t erreur reconstruction 67.72\n",
      "iteration 14 \t : \t erreur reconstruction 68.81\n",
      "iteration 15 \t : \t erreur reconstruction 66.02\n",
      "iteration 16 \t : \t erreur reconstruction 76.51\n",
      "iteration 17 \t : \t erreur reconstruction 72.80\n",
      "iteration 18 \t : \t erreur reconstruction 77.32\n",
      "iteration 19 \t : \t erreur reconstruction 66.29\n",
      "iteration 20 \t : \t erreur reconstruction 80.06\n",
      "iteration 21 \t : \t erreur reconstruction 72.33\n",
      "iteration 22 \t : \t erreur reconstruction 83.85\n",
      "iteration 23 \t : \t erreur reconstruction 63.52\n",
      "iteration 24 \t : \t erreur reconstruction 66.02\n",
      "iteration 25 \t : \t erreur reconstruction 70.31\n",
      "iteration 26 \t : \t erreur reconstruction 63.91\n",
      "iteration 27 \t : \t erreur reconstruction 60.43\n",
      "iteration 28 \t : \t erreur reconstruction 62.18\n",
      "iteration 29 \t : \t erreur reconstruction 61.17\n",
      "iteration 30 \t : \t erreur reconstruction 60.44\n",
      "iteration 31 \t : \t erreur reconstruction 59.27\n",
      "iteration 32 \t : \t erreur reconstruction 65.53\n",
      "iteration 33 \t : \t erreur reconstruction 73.97\n",
      "iteration 34 \t : \t erreur reconstruction 57.67\n",
      "iteration 35 \t : \t erreur reconstruction 59.54\n",
      "iteration 36 \t : \t erreur reconstruction 56.21\n",
      "iteration 37 \t : \t erreur reconstruction 55.34\n",
      "iteration 38 \t : \t erreur reconstruction 54.59\n",
      "iteration 39 \t : \t erreur reconstruction 55.94\n",
      "iteration 40 \t : \t erreur reconstruction 56.19\n",
      "iteration 41 \t : \t erreur reconstruction 60.49\n",
      "iteration 42 \t : \t erreur reconstruction 55.48\n",
      "iteration 43 \t : \t erreur reconstruction 56.76\n",
      "iteration 44 \t : \t erreur reconstruction 61.97\n",
      "iteration 45 \t : \t erreur reconstruction 62.05\n",
      "iteration 46 \t : \t erreur reconstruction 57.87\n",
      "iteration 47 \t : \t erreur reconstruction 60.84\n",
      "iteration 48 \t : \t erreur reconstruction 57.36\n",
      "iteration 49 \t : \t erreur reconstruction 53.06\n",
      "iteration 50 \t : \t erreur reconstruction 50.30\n",
      "iteration 51 \t : \t erreur reconstruction 52.00\n",
      "iteration 52 \t : \t erreur reconstruction 59.53\n",
      "iteration 53 \t : \t erreur reconstruction 57.75\n",
      "iteration 54 \t : \t erreur reconstruction 54.09\n",
      "iteration 55 \t : \t erreur reconstruction 50.61\n",
      "iteration 56 \t : \t erreur reconstruction 51.08\n",
      "iteration 57 \t : \t erreur reconstruction 51.15\n",
      "iteration 58 \t : \t erreur reconstruction 49.56\n",
      "iteration 59 \t : \t erreur reconstruction 49.08\n",
      "iteration 60 \t : \t erreur reconstruction 50.14\n",
      "iteration 61 \t : \t erreur reconstruction 51.13\n",
      "iteration 62 \t : \t erreur reconstruction 48.84\n",
      "iteration 63 \t : \t erreur reconstruction 50.87\n",
      "iteration 64 \t : \t erreur reconstruction 50.04\n",
      "iteration 65 \t : \t erreur reconstruction 52.57\n",
      "iteration 66 \t : \t erreur reconstruction 50.45\n",
      "iteration 67 \t : \t erreur reconstruction 52.91\n",
      "iteration 68 \t : \t erreur reconstruction 51.78\n",
      "iteration 69 \t : \t erreur reconstruction 51.24\n",
      "iteration 70 \t : \t erreur reconstruction 49.41\n",
      "iteration 71 \t : \t erreur reconstruction 50.79\n",
      "iteration 72 \t : \t erreur reconstruction 50.01\n",
      "iteration 73 \t : \t erreur reconstruction 52.16\n",
      "iteration 74 \t : \t erreur reconstruction 48.35\n",
      "iteration 75 \t : \t erreur reconstruction 48.44\n",
      "iteration 76 \t : \t erreur reconstruction 46.04\n",
      "iteration 77 \t : \t erreur reconstruction 43.98\n",
      "iteration 78 \t : \t erreur reconstruction 46.09\n",
      "iteration 79 \t : \t erreur reconstruction 44.61\n",
      "iteration 80 \t : \t erreur reconstruction 45.68\n",
      "iteration 81 \t : \t erreur reconstruction 44.93\n",
      "iteration 82 \t : \t erreur reconstruction 47.52\n",
      "iteration 83 \t : \t erreur reconstruction 45.70\n",
      "iteration 84 \t : \t erreur reconstruction 47.03\n",
      "iteration 85 \t : \t erreur reconstruction 47.20\n",
      "iteration 86 \t : \t erreur reconstruction 50.14\n",
      "iteration 87 \t : \t erreur reconstruction 48.09\n",
      "iteration 88 \t : \t erreur reconstruction 45.20\n",
      "iteration 89 \t : \t erreur reconstruction 45.01\n",
      "iteration 90 \t : \t erreur reconstruction 43.66\n",
      "iteration 91 \t : \t erreur reconstruction 42.51\n",
      "iteration 92 \t : \t erreur reconstruction 42.28\n",
      "iteration 93 \t : \t erreur reconstruction 53.92\n",
      "iteration 94 \t : \t erreur reconstruction 49.59\n",
      "iteration 95 \t : \t erreur reconstruction 42.93\n",
      "iteration 96 \t : \t erreur reconstruction 43.56\n",
      "iteration 97 \t : \t erreur reconstruction 42.24\n",
      "iteration 98 \t : \t erreur reconstruction 42.03\n",
      "iteration 99 \t : \t erreur reconstruction 42.69\n",
      "iteration 0 \t : \t erreur reconstruction 7.57\n",
      "iteration 1 \t : \t erreur reconstruction 11.16\n",
      "iteration 2 \t : \t erreur reconstruction 16.26\n",
      "iteration 3 \t : \t erreur reconstruction 11.59\n",
      "iteration 4 \t : \t erreur reconstruction 12.49\n",
      "iteration 5 \t : \t erreur reconstruction 10.99\n",
      "iteration 6 \t : \t erreur reconstruction 10.33\n",
      "iteration 7 \t : \t erreur reconstruction 9.95\n",
      "iteration 8 \t : \t erreur reconstruction 9.05\n",
      "iteration 9 \t : \t erreur reconstruction 8.26\n",
      "iteration 10 \t : \t erreur reconstruction 9.17\n",
      "iteration 11 \t : \t erreur reconstruction 7.83\n",
      "iteration 12 \t : \t erreur reconstruction 6.71\n",
      "iteration 13 \t : \t erreur reconstruction 8.84\n",
      "iteration 14 \t : \t erreur reconstruction 6.38\n",
      "iteration 15 \t : \t erreur reconstruction 6.47\n",
      "iteration 16 \t : \t erreur reconstruction 6.57\n",
      "iteration 17 \t : \t erreur reconstruction 6.25\n",
      "iteration 18 \t : \t erreur reconstruction 5.16\n",
      "iteration 19 \t : \t erreur reconstruction 4.51\n",
      "iteration 20 \t : \t erreur reconstruction 5.37\n",
      "iteration 21 \t : \t erreur reconstruction 5.47\n",
      "iteration 22 \t : \t erreur reconstruction 6.26\n",
      "iteration 23 \t : \t erreur reconstruction 6.20\n",
      "iteration 24 \t : \t erreur reconstruction 5.30\n",
      "iteration 25 \t : \t erreur reconstruction 5.64\n",
      "iteration 26 \t : \t erreur reconstruction 4.85\n",
      "iteration 27 \t : \t erreur reconstruction 4.84\n",
      "iteration 28 \t : \t erreur reconstruction 4.41\n",
      "iteration 29 \t : \t erreur reconstruction 4.93\n",
      "iteration 30 \t : \t erreur reconstruction 4.92\n",
      "iteration 31 \t : \t erreur reconstruction 4.77\n",
      "iteration 32 \t : \t erreur reconstruction 5.24\n",
      "iteration 33 \t : \t erreur reconstruction 4.77\n",
      "iteration 34 \t : \t erreur reconstruction 4.67\n",
      "iteration 35 \t : \t erreur reconstruction 4.58\n",
      "iteration 36 \t : \t erreur reconstruction 4.78\n",
      "iteration 37 \t : \t erreur reconstruction 4.86\n",
      "iteration 38 \t : \t erreur reconstruction 4.89\n",
      "iteration 39 \t : \t erreur reconstruction 6.30\n",
      "iteration 40 \t : \t erreur reconstruction 5.70\n",
      "iteration 41 \t : \t erreur reconstruction 4.68\n",
      "iteration 42 \t : \t erreur reconstruction 5.12\n",
      "iteration 43 \t : \t erreur reconstruction 4.87\n",
      "iteration 44 \t : \t erreur reconstruction 4.40\n",
      "iteration 45 \t : \t erreur reconstruction 4.22\n",
      "iteration 46 \t : \t erreur reconstruction 4.71\n",
      "iteration 47 \t : \t erreur reconstruction 4.57\n",
      "iteration 48 \t : \t erreur reconstruction 3.88\n",
      "iteration 49 \t : \t erreur reconstruction 3.62\n",
      "iteration 50 \t : \t erreur reconstruction 4.05\n",
      "iteration 51 \t : \t erreur reconstruction 4.26\n",
      "iteration 52 \t : \t erreur reconstruction 4.56\n",
      "iteration 53 \t : \t erreur reconstruction 4.32\n",
      "iteration 54 \t : \t erreur reconstruction 4.34\n",
      "iteration 55 \t : \t erreur reconstruction 4.46\n",
      "iteration 56 \t : \t erreur reconstruction 3.91\n",
      "iteration 57 \t : \t erreur reconstruction 3.57\n",
      "iteration 58 \t : \t erreur reconstruction 3.29\n",
      "iteration 59 \t : \t erreur reconstruction 3.41\n",
      "iteration 60 \t : \t erreur reconstruction 3.28\n",
      "iteration 61 \t : \t erreur reconstruction 3.60\n",
      "iteration 62 \t : \t erreur reconstruction 3.04\n",
      "iteration 63 \t : \t erreur reconstruction 3.20\n",
      "iteration 64 \t : \t erreur reconstruction 3.00\n",
      "iteration 65 \t : \t erreur reconstruction 3.48\n",
      "iteration 66 \t : \t erreur reconstruction 3.61\n",
      "iteration 67 \t : \t erreur reconstruction 3.48\n",
      "iteration 68 \t : \t erreur reconstruction 3.49\n",
      "iteration 69 \t : \t erreur reconstruction 3.21\n",
      "iteration 70 \t : \t erreur reconstruction 3.14\n",
      "iteration 71 \t : \t erreur reconstruction 3.00\n",
      "iteration 72 \t : \t erreur reconstruction 2.93\n",
      "iteration 73 \t : \t erreur reconstruction 3.42\n",
      "iteration 74 \t : \t erreur reconstruction 3.56\n",
      "iteration 75 \t : \t erreur reconstruction 3.77\n",
      "iteration 76 \t : \t erreur reconstruction 3.39\n",
      "iteration 77 \t : \t erreur reconstruction 3.53\n",
      "iteration 78 \t : \t erreur reconstruction 3.27\n",
      "iteration 79 \t : \t erreur reconstruction 3.21\n",
      "iteration 80 \t : \t erreur reconstruction 3.36\n",
      "iteration 81 \t : \t erreur reconstruction 3.67\n",
      "iteration 82 \t : \t erreur reconstruction 4.01\n",
      "iteration 83 \t : \t erreur reconstruction 3.50\n",
      "iteration 84 \t : \t erreur reconstruction 3.62\n",
      "iteration 85 \t : \t erreur reconstruction 3.27\n",
      "iteration 86 \t : \t erreur reconstruction 3.39\n",
      "iteration 87 \t : \t erreur reconstruction 3.19\n",
      "iteration 88 \t : \t erreur reconstruction 3.01\n",
      "iteration 89 \t : \t erreur reconstruction 3.09\n",
      "iteration 90 \t : \t erreur reconstruction 3.06\n",
      "iteration 91 \t : \t erreur reconstruction 3.05\n",
      "iteration 92 \t : \t erreur reconstruction 3.18\n",
      "iteration 93 \t : \t erreur reconstruction 3.14\n",
      "iteration 94 \t : \t erreur reconstruction 2.82\n",
      "iteration 95 \t : \t erreur reconstruction 3.11\n",
      "iteration 96 \t : \t erreur reconstruction 2.80\n",
      "iteration 97 \t : \t erreur reconstruction 2.87\n",
      "iteration 98 \t : \t erreur reconstruction 2.90\n",
      "iteration 99 \t : \t erreur reconstruction 2.67\n",
      "iteration 0 \t : \t erreur reconstruction 15.47\n",
      "iteration 1 \t : \t erreur reconstruction 17.61\n",
      "iteration 2 \t : \t erreur reconstruction 16.52\n",
      "iteration 3 \t : \t erreur reconstruction 13.12\n",
      "iteration 4 \t : \t erreur reconstruction 15.75\n",
      "iteration 5 \t : \t erreur reconstruction 12.66\n",
      "iteration 6 \t : \t erreur reconstruction 13.37\n",
      "iteration 7 \t : \t erreur reconstruction 12.64\n",
      "iteration 8 \t : \t erreur reconstruction 13.34\n",
      "iteration 9 \t : \t erreur reconstruction 12.13\n",
      "iteration 10 \t : \t erreur reconstruction 9.79\n",
      "iteration 11 \t : \t erreur reconstruction 10.63\n",
      "iteration 12 \t : \t erreur reconstruction 11.26\n",
      "iteration 13 \t : \t erreur reconstruction 11.72\n",
      "iteration 14 \t : \t erreur reconstruction 11.27\n",
      "iteration 15 \t : \t erreur reconstruction 10.97\n",
      "iteration 16 \t : \t erreur reconstruction 10.16\n",
      "iteration 17 \t : \t erreur reconstruction 10.58\n",
      "iteration 18 \t : \t erreur reconstruction 10.61\n",
      "iteration 19 \t : \t erreur reconstruction 10.57\n",
      "iteration 20 \t : \t erreur reconstruction 8.71\n",
      "iteration 21 \t : \t erreur reconstruction 9.29\n",
      "iteration 22 \t : \t erreur reconstruction 8.56\n",
      "iteration 23 \t : \t erreur reconstruction 9.22\n",
      "iteration 24 \t : \t erreur reconstruction 9.03\n",
      "iteration 25 \t : \t erreur reconstruction 8.52\n",
      "iteration 26 \t : \t erreur reconstruction 9.01\n",
      "iteration 27 \t : \t erreur reconstruction 9.24\n",
      "iteration 28 \t : \t erreur reconstruction 9.00\n",
      "iteration 29 \t : \t erreur reconstruction 9.00\n",
      "iteration 30 \t : \t erreur reconstruction 8.31\n",
      "iteration 31 \t : \t erreur reconstruction 8.63\n",
      "iteration 32 \t : \t erreur reconstruction 8.48\n",
      "iteration 33 \t : \t erreur reconstruction 8.47\n",
      "iteration 34 \t : \t erreur reconstruction 8.35\n",
      "iteration 35 \t : \t erreur reconstruction 8.71\n",
      "iteration 36 \t : \t erreur reconstruction 8.45\n",
      "iteration 37 \t : \t erreur reconstruction 9.12\n",
      "iteration 38 \t : \t erreur reconstruction 8.93\n",
      "iteration 39 \t : \t erreur reconstruction 8.65\n",
      "iteration 40 \t : \t erreur reconstruction 8.63\n",
      "iteration 41 \t : \t erreur reconstruction 8.30\n",
      "iteration 42 \t : \t erreur reconstruction 9.11\n",
      "iteration 43 \t : \t erreur reconstruction 8.11\n",
      "iteration 44 \t : \t erreur reconstruction 7.98\n",
      "iteration 45 \t : \t erreur reconstruction 7.53\n",
      "iteration 46 \t : \t erreur reconstruction 8.20\n",
      "iteration 47 \t : \t erreur reconstruction 8.05\n",
      "iteration 48 \t : \t erreur reconstruction 8.47\n",
      "iteration 49 \t : \t erreur reconstruction 8.27\n",
      "iteration 50 \t : \t erreur reconstruction 8.90\n",
      "iteration 51 \t : \t erreur reconstruction 7.69\n",
      "iteration 52 \t : \t erreur reconstruction 7.58\n",
      "iteration 53 \t : \t erreur reconstruction 7.24\n",
      "iteration 54 \t : \t erreur reconstruction 7.69\n",
      "iteration 55 \t : \t erreur reconstruction 7.54\n",
      "iteration 56 \t : \t erreur reconstruction 7.96\n",
      "iteration 57 \t : \t erreur reconstruction 8.61\n",
      "iteration 58 \t : \t erreur reconstruction 8.24\n",
      "iteration 59 \t : \t erreur reconstruction 7.86\n",
      "iteration 60 \t : \t erreur reconstruction 7.69\n",
      "iteration 61 \t : \t erreur reconstruction 7.99\n",
      "iteration 62 \t : \t erreur reconstruction 7.67\n",
      "iteration 63 \t : \t erreur reconstruction 7.81\n",
      "iteration 64 \t : \t erreur reconstruction 7.52\n",
      "iteration 65 \t : \t erreur reconstruction 7.43\n",
      "iteration 66 \t : \t erreur reconstruction 7.36\n",
      "iteration 67 \t : \t erreur reconstruction 7.29\n",
      "iteration 68 \t : \t erreur reconstruction 6.92\n",
      "iteration 69 \t : \t erreur reconstruction 7.47\n",
      "iteration 70 \t : \t erreur reconstruction 6.54\n",
      "iteration 71 \t : \t erreur reconstruction 6.70\n",
      "iteration 72 \t : \t erreur reconstruction 6.72\n",
      "iteration 73 \t : \t erreur reconstruction 6.39\n",
      "iteration 74 \t : \t erreur reconstruction 6.27\n",
      "iteration 75 \t : \t erreur reconstruction 6.31\n",
      "iteration 76 \t : \t erreur reconstruction 6.28\n",
      "iteration 77 \t : \t erreur reconstruction 6.15\n",
      "iteration 78 \t : \t erreur reconstruction 5.89\n",
      "iteration 79 \t : \t erreur reconstruction 6.15\n",
      "iteration 80 \t : \t erreur reconstruction 5.99\n",
      "iteration 81 \t : \t erreur reconstruction 6.28\n",
      "iteration 82 \t : \t erreur reconstruction 6.55\n",
      "iteration 83 \t : \t erreur reconstruction 6.35\n",
      "iteration 84 \t : \t erreur reconstruction 6.20\n",
      "iteration 85 \t : \t erreur reconstruction 6.33\n",
      "iteration 86 \t : \t erreur reconstruction 6.46\n",
      "iteration 87 \t : \t erreur reconstruction 6.78\n",
      "iteration 88 \t : \t erreur reconstruction 6.50\n",
      "iteration 89 \t : \t erreur reconstruction 5.95\n",
      "iteration 90 \t : \t erreur reconstruction 5.61\n",
      "iteration 91 \t : \t erreur reconstruction 5.66\n",
      "iteration 92 \t : \t erreur reconstruction 5.57\n",
      "iteration 93 \t : \t erreur reconstruction 5.33\n",
      "iteration 94 \t : \t erreur reconstruction 5.72\n",
      "iteration 95 \t : \t erreur reconstruction 6.32\n",
      "iteration 96 \t : \t erreur reconstruction 6.11\n",
      "iteration 97 \t : \t erreur reconstruction 6.17\n",
      "iteration 98 \t : \t erreur reconstruction 5.63\n",
      "iteration 99 \t : \t erreur reconstruction 5.87\n",
      "iteration 0 \t : \t erreur reconstruction 11.39\n",
      "iteration 1 \t : \t erreur reconstruction 12.48\n",
      "iteration 2 \t : \t erreur reconstruction 8.06\n",
      "iteration 3 \t : \t erreur reconstruction 13.24\n",
      "iteration 4 \t : \t erreur reconstruction 12.60\n",
      "iteration 5 \t : \t erreur reconstruction 11.94\n",
      "iteration 6 \t : \t erreur reconstruction 8.73\n",
      "iteration 7 \t : \t erreur reconstruction 8.05\n",
      "iteration 8 \t : \t erreur reconstruction 7.81\n",
      "iteration 9 \t : \t erreur reconstruction 8.05\n",
      "iteration 10 \t : \t erreur reconstruction 8.62\n",
      "iteration 11 \t : \t erreur reconstruction 9.20\n",
      "iteration 12 \t : \t erreur reconstruction 8.93\n",
      "iteration 13 \t : \t erreur reconstruction 7.67\n",
      "iteration 14 \t : \t erreur reconstruction 7.77\n",
      "iteration 15 \t : \t erreur reconstruction 7.57\n",
      "iteration 16 \t : \t erreur reconstruction 7.04\n",
      "iteration 17 \t : \t erreur reconstruction 7.66\n",
      "iteration 18 \t : \t erreur reconstruction 7.13\n",
      "iteration 19 \t : \t erreur reconstruction 5.75\n",
      "iteration 20 \t : \t erreur reconstruction 7.40\n",
      "iteration 21 \t : \t erreur reconstruction 7.38\n",
      "iteration 22 \t : \t erreur reconstruction 6.81\n",
      "iteration 23 \t : \t erreur reconstruction 6.40\n",
      "iteration 24 \t : \t erreur reconstruction 7.41\n",
      "iteration 25 \t : \t erreur reconstruction 5.38\n",
      "iteration 26 \t : \t erreur reconstruction 5.57\n",
      "iteration 27 \t : \t erreur reconstruction 5.40\n",
      "iteration 28 \t : \t erreur reconstruction 6.22\n",
      "iteration 29 \t : \t erreur reconstruction 6.99\n",
      "iteration 30 \t : \t erreur reconstruction 6.41\n",
      "iteration 31 \t : \t erreur reconstruction 5.37\n",
      "iteration 32 \t : \t erreur reconstruction 5.59\n",
      "iteration 33 \t : \t erreur reconstruction 5.32\n",
      "iteration 34 \t : \t erreur reconstruction 5.44\n",
      "iteration 35 \t : \t erreur reconstruction 5.41\n",
      "iteration 36 \t : \t erreur reconstruction 5.12\n",
      "iteration 37 \t : \t erreur reconstruction 5.26\n",
      "iteration 38 \t : \t erreur reconstruction 5.38\n",
      "iteration 39 \t : \t erreur reconstruction 4.76\n",
      "iteration 40 \t : \t erreur reconstruction 5.01\n",
      "iteration 41 \t : \t erreur reconstruction 5.11\n",
      "iteration 42 \t : \t erreur reconstruction 5.05\n",
      "iteration 43 \t : \t erreur reconstruction 5.23\n",
      "iteration 44 \t : \t erreur reconstruction 5.08\n",
      "iteration 45 \t : \t erreur reconstruction 4.56\n",
      "iteration 46 \t : \t erreur reconstruction 4.09\n",
      "iteration 47 \t : \t erreur reconstruction 4.12\n",
      "iteration 48 \t : \t erreur reconstruction 3.88\n",
      "iteration 49 \t : \t erreur reconstruction 3.45\n",
      "iteration 50 \t : \t erreur reconstruction 3.54\n",
      "iteration 51 \t : \t erreur reconstruction 3.91\n",
      "iteration 52 \t : \t erreur reconstruction 4.24\n",
      "iteration 53 \t : \t erreur reconstruction 4.48\n",
      "iteration 54 \t : \t erreur reconstruction 4.45\n",
      "iteration 55 \t : \t erreur reconstruction 4.51\n",
      "iteration 56 \t : \t erreur reconstruction 4.15\n",
      "iteration 57 \t : \t erreur reconstruction 4.18\n",
      "iteration 58 \t : \t erreur reconstruction 4.21\n",
      "iteration 59 \t : \t erreur reconstruction 4.33\n",
      "iteration 60 \t : \t erreur reconstruction 4.08\n",
      "iteration 61 \t : \t erreur reconstruction 3.70\n",
      "iteration 62 \t : \t erreur reconstruction 3.62\n",
      "iteration 63 \t : \t erreur reconstruction 3.53\n",
      "iteration 64 \t : \t erreur reconstruction 3.59\n",
      "iteration 65 \t : \t erreur reconstruction 3.59\n",
      "iteration 66 \t : \t erreur reconstruction 3.57\n",
      "iteration 67 \t : \t erreur reconstruction 3.48\n",
      "iteration 68 \t : \t erreur reconstruction 3.24\n",
      "iteration 69 \t : \t erreur reconstruction 3.55\n",
      "iteration 70 \t : \t erreur reconstruction 3.98\n",
      "iteration 71 \t : \t erreur reconstruction 3.53\n",
      "iteration 72 \t : \t erreur reconstruction 3.46\n",
      "iteration 73 \t : \t erreur reconstruction 3.48\n",
      "iteration 74 \t : \t erreur reconstruction 3.36\n",
      "iteration 75 \t : \t erreur reconstruction 3.35\n",
      "iteration 76 \t : \t erreur reconstruction 3.54\n",
      "iteration 77 \t : \t erreur reconstruction 3.48\n",
      "iteration 78 \t : \t erreur reconstruction 3.78\n",
      "iteration 79 \t : \t erreur reconstruction 3.70\n",
      "iteration 80 \t : \t erreur reconstruction 3.34\n",
      "iteration 81 \t : \t erreur reconstruction 3.25\n",
      "iteration 82 \t : \t erreur reconstruction 3.57\n",
      "iteration 83 \t : \t erreur reconstruction 3.61\n",
      "iteration 84 \t : \t erreur reconstruction 3.41\n",
      "iteration 85 \t : \t erreur reconstruction 3.36\n",
      "iteration 86 \t : \t erreur reconstruction 3.27\n",
      "iteration 87 \t : \t erreur reconstruction 3.30\n",
      "iteration 88 \t : \t erreur reconstruction 3.33\n",
      "iteration 89 \t : \t erreur reconstruction 3.44\n",
      "iteration 90 \t : \t erreur reconstruction 3.29\n",
      "iteration 91 \t : \t erreur reconstruction 3.30\n",
      "iteration 92 \t : \t erreur reconstruction 3.20\n",
      "iteration 93 \t : \t erreur reconstruction 3.24\n",
      "iteration 94 \t : \t erreur reconstruction 3.24\n",
      "iteration 95 \t : \t erreur reconstruction 3.14\n",
      "iteration 96 \t : \t erreur reconstruction 3.15\n",
      "iteration 97 \t : \t erreur reconstruction 3.30\n",
      "iteration 98 \t : \t erreur reconstruction 3.54\n",
      "iteration 99 \t : \t erreur reconstruction 3.36\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "q = 64\n",
    "n_layers = 3\n",
    "index_carac = [2, 17, 25]\n",
    "nb_images = 10\n",
    "nb_iter = 100\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "nb_iter_gibbs = 100\n",
    "verbose = True\n",
    "plot = True\n",
    "\n",
    "DBN_trained = principal_DBN_alpha(q, n_layers, index_carac, nb_images, nb_iter, lr, batch_size, nb_iter_gibbs, verbose, plot=plot)\n",
    "# images = generer_image_DBN(DBN_trained, image_shape, nb_images, nb_iter_gibbs, plot=plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYVjpz7lX30W"
   },
   "source": [
    "\n",
    "# <a id='mnist'>5. Etude à réaliser (MNIST)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOjmawrzBdxt"
   },
   "source": [
    "## 5.1 Programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9Griz8GX30W"
   },
   "outputs": [],
   "source": [
    "def principal_mnist(n_samples, DNN_trained, pre_train, nb_iter=200, nb_iter_RBM=100, lr=0.1, batch_size=32, verbose=True):\n",
    "    \"\"\" \n",
    "    Learn characters of MNIST.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of training data.\n",
    "\n",
    "    DNN_trained : dict\n",
    "        A DBN structure.\n",
    "\n",
    "    pre_train : bool\n",
    "        Pre-training of the network.\n",
    "    \n",
    "    nb_iter : int\n",
    "        Number of iterations in the Gradient Descent of backpropagation.\n",
    "\n",
    "    nb_iter_RBM : int\n",
    "        Number of iterations in the Gradient Descent of RBM.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "        \n",
    "    verbose : bool\n",
    "        Enable verbose output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DNN : dict\n",
    "        A DBN structure.\n",
    "    \"\"\"\n",
    "    mask = random.sample(list(np.arange(train_image.shape[0])), n_samples)\n",
    "    input_data = train_image[mask]\n",
    "    labels = train_label[mask]\n",
    "    \n",
    "    if pre_train:\n",
    "        DNN_trained = train_DBN(DNN_trained, input_data.copy(), nb_iter=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=False)\n",
    "        \n",
    "    # Training\n",
    "    DNN_trained = retropropagation(DNN_trained, input_data, labels, nb_iter=nb_iter, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    return DNN_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "eLpk4vSzBfS5",
    "outputId": "2cc95ce9-3bf6-4944-d06a-a9ed4be17265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 \t : \t loss 0.11637 - accuracy 0.68726\n",
      "iteration 1 \t : \t loss 0.05809 - accuracy 0.86548\n",
      "iteration 2 \t : \t loss 0.04628 - accuracy 0.88672\n",
      "iteration 3 \t : \t loss 0.04065 - accuracy 0.89404\n",
      "iteration 4 \t : \t loss 0.03714 - accuracy 0.90112\n",
      "iteration 5 \t : \t loss 0.03464 - accuracy 0.90625\n",
      "iteration 6 \t : \t loss 0.03270 - accuracy 0.91040\n",
      "iteration 7 \t : \t loss 0.03113 - accuracy 0.91357\n",
      "iteration 8 \t : \t loss 0.02979 - accuracy 0.91553\n",
      "iteration 9 \t : \t loss 0.02864 - accuracy 0.91968\n",
      "iteration 10 \t : \t loss 0.02761 - accuracy 0.92188\n",
      "iteration 11 \t : \t loss 0.02669 - accuracy 0.92529\n",
      "iteration 12 \t : \t loss 0.02586 - accuracy 0.92847\n",
      "iteration 13 \t : \t loss 0.02509 - accuracy 0.93091\n",
      "iteration 14 \t : \t loss 0.02438 - accuracy 0.93311\n",
      "iteration 15 \t : \t loss 0.02371 - accuracy 0.93506\n",
      "iteration 16 \t : \t loss 0.02309 - accuracy 0.93726\n",
      "iteration 17 \t : \t loss 0.02250 - accuracy 0.93945\n",
      "iteration 18 \t : \t loss 0.02195 - accuracy 0.94043\n",
      "iteration 19 \t : \t loss 0.02142 - accuracy 0.94287\n",
      "iteration 20 \t : \t loss 0.02092 - accuracy 0.94434\n",
      "iteration 21 \t : \t loss 0.02044 - accuracy 0.94604\n",
      "iteration 22 \t : \t loss 0.01998 - accuracy 0.94653\n",
      "iteration 23 \t : \t loss 0.01954 - accuracy 0.94824\n",
      "iteration 24 \t : \t loss 0.01912 - accuracy 0.94849\n",
      "iteration 25 \t : \t loss 0.01871 - accuracy 0.94971\n",
      "iteration 26 \t : \t loss 0.01832 - accuracy 0.95142\n",
      "iteration 27 \t : \t loss 0.01795 - accuracy 0.95264\n",
      "iteration 28 \t : \t loss 0.01758 - accuracy 0.95435\n",
      "iteration 29 \t : \t loss 0.01723 - accuracy 0.95483\n",
      "iteration 30 \t : \t loss 0.01689 - accuracy 0.95581\n",
      "iteration 31 \t : \t loss 0.01655 - accuracy 0.95679\n",
      "iteration 32 \t : \t loss 0.01623 - accuracy 0.95776\n",
      "iteration 33 \t : \t loss 0.01592 - accuracy 0.95874\n",
      "iteration 34 \t : \t loss 0.01562 - accuracy 0.95947\n",
      "iteration 35 \t : \t loss 0.01532 - accuracy 0.95996\n",
      "iteration 36 \t : \t loss 0.01504 - accuracy 0.96167\n",
      "iteration 37 \t : \t loss 0.01476 - accuracy 0.96240\n",
      "iteration 38 \t : \t loss 0.01449 - accuracy 0.96362\n",
      "iteration 39 \t : \t loss 0.01422 - accuracy 0.96411\n",
      "iteration 40 \t : \t loss 0.01396 - accuracy 0.96606\n",
      "iteration 41 \t : \t loss 0.01371 - accuracy 0.96655\n",
      "iteration 42 \t : \t loss 0.01346 - accuracy 0.96777\n",
      "iteration 43 \t : \t loss 0.01322 - accuracy 0.96802\n",
      "iteration 44 \t : \t loss 0.01299 - accuracy 0.96899\n",
      "iteration 45 \t : \t loss 0.01276 - accuracy 0.96948\n",
      "iteration 46 \t : \t loss 0.01254 - accuracy 0.97046\n",
      "iteration 47 \t : \t loss 0.01232 - accuracy 0.97119\n",
      "iteration 48 \t : \t loss 0.01211 - accuracy 0.97144\n",
      "iteration 49 \t : \t loss 0.01190 - accuracy 0.97144\n",
      "Error rate on test set : 0.07130\n"
     ]
    }
   ],
   "source": [
    "n_samples = 60000\n",
    "p = train_image.shape[1]\n",
    "q = 256\n",
    "n_layers = 1\n",
    "pre_train = True\n",
    "nb_iter = 50\n",
    "nb_iter_RBM = 50\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "verbose = True\n",
    "\n",
    "DNN_trained = init_DBN(p, q, n_layers)\n",
    "DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "print(\"Error rate on test set : %.5f\" % (test_DNN(DNN_trained, test_image, test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Nhp0Hx9J2CS"
   },
   "source": [
    "## 5.2 Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2yWgFSP8y8g"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "p = train_image.shape[1]\n",
    "nb_iter = 200\n",
    "nb_iter_RBM = 100\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hF6YI1caPnn"
   },
   "source": [
    "### 2 Réseaux en fonction du nombre de couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RPZU1xv2aOE-",
    "outputId": "43ffcceb-16a1-4f39-929d-9bb573f05650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 \t : \t loss 0.23286 - accuracy 0.10962\n",
      "iteration 1 \t : \t loss 0.22884 - accuracy 0.14136\n",
      "iteration 2 \t : \t loss 0.22491 - accuracy 0.18042\n",
      "iteration 3 \t : \t loss 0.21908 - accuracy 0.25171\n",
      "iteration 4 \t : \t loss 0.20951 - accuracy 0.33862\n",
      "iteration 5 \t : \t loss 0.19381 - accuracy 0.41455\n",
      "iteration 6 \t : \t loss 0.17227 - accuracy 0.49268\n",
      "iteration 7 \t : \t loss 0.14992 - accuracy 0.55444\n",
      "iteration 8 \t : \t loss 0.13094 - accuracy 0.60547\n",
      "iteration 9 \t : \t loss 0.11566 - accuracy 0.65137\n",
      "iteration 10 \t : \t loss 0.10332 - accuracy 0.69458\n",
      "iteration 11 \t : \t loss 0.09333 - accuracy 0.72803\n",
      "iteration 12 \t : \t loss 0.08513 - accuracy 0.75073\n",
      "iteration 13 \t : \t loss 0.07824 - accuracy 0.77441\n",
      "iteration 14 \t : \t loss 0.07234 - accuracy 0.79053\n",
      "iteration 15 \t : \t loss 0.06725 - accuracy 0.80591\n",
      "iteration 16 \t : \t loss 0.06284 - accuracy 0.82056\n",
      "iteration 17 \t : \t loss 0.05905 - accuracy 0.83032\n",
      "iteration 18 \t : \t loss 0.05577 - accuracy 0.84253\n",
      "iteration 19 \t : \t loss 0.05293 - accuracy 0.85107\n",
      "iteration 20 \t : \t loss 0.05046 - accuracy 0.85693\n",
      "iteration 21 \t : \t loss 0.04829 - accuracy 0.86206\n",
      "iteration 22 \t : \t loss 0.04637 - accuracy 0.86743\n",
      "iteration 23 \t : \t loss 0.04466 - accuracy 0.87109\n",
      "iteration 24 \t : \t loss 0.04312 - accuracy 0.87524\n",
      "iteration 25 \t : \t loss 0.04173 - accuracy 0.87915\n",
      "iteration 26 \t : \t loss 0.04046 - accuracy 0.88477\n",
      "iteration 27 \t : \t loss 0.03930 - accuracy 0.88867\n",
      "iteration 28 \t : \t loss 0.03824 - accuracy 0.89258\n",
      "iteration 29 \t : \t loss 0.03725 - accuracy 0.89575\n",
      "iteration 30 \t : \t loss 0.03633 - accuracy 0.89941\n",
      "iteration 31 \t : \t loss 0.03547 - accuracy 0.90259\n",
      "iteration 32 \t : \t loss 0.03467 - accuracy 0.90454\n",
      "iteration 33 \t : \t loss 0.03391 - accuracy 0.90845\n",
      "iteration 34 \t : \t loss 0.03319 - accuracy 0.90991\n",
      "iteration 35 \t : \t loss 0.03251 - accuracy 0.91260\n",
      "iteration 36 \t : \t loss 0.03186 - accuracy 0.91260\n",
      "iteration 37 \t : \t loss 0.03123 - accuracy 0.91479\n",
      "iteration 38 \t : \t loss 0.03064 - accuracy 0.91650\n",
      "iteration 39 \t : \t loss 0.03006 - accuracy 0.91724\n",
      "iteration 40 \t : \t loss 0.02951 - accuracy 0.91846\n",
      "iteration 41 \t : \t loss 0.02898 - accuracy 0.92114\n",
      "iteration 42 \t : \t loss 0.02846 - accuracy 0.92285\n",
      "iteration 43 \t : \t loss 0.02796 - accuracy 0.92358\n",
      "iteration 44 \t : \t loss 0.02747 - accuracy 0.92480\n",
      "iteration 45 \t : \t loss 0.02700 - accuracy 0.92700\n",
      "iteration 46 \t : \t loss 0.02654 - accuracy 0.92847\n",
      "iteration 47 \t : \t loss 0.02609 - accuracy 0.92896\n",
      "iteration 48 \t : \t loss 0.02565 - accuracy 0.92944\n",
      "iteration 49 \t : \t loss 0.02522 - accuracy 0.93213\n",
      "iteration 50 \t : \t loss 0.02480 - accuracy 0.93433\n",
      "iteration 51 \t : \t loss 0.02438 - accuracy 0.93433\n",
      "iteration 52 \t : \t loss 0.02398 - accuracy 0.93555\n",
      "iteration 53 \t : \t loss 0.02358 - accuracy 0.93677\n",
      "iteration 54 \t : \t loss 0.02319 - accuracy 0.93823\n",
      "iteration 55 \t : \t loss 0.02280 - accuracy 0.93896\n",
      "iteration 56 \t : \t loss 0.02242 - accuracy 0.93970\n",
      "iteration 57 \t : \t loss 0.02205 - accuracy 0.94092\n",
      "iteration 58 \t : \t loss 0.02168 - accuracy 0.94214\n",
      "iteration 59 \t : \t loss 0.02132 - accuracy 0.94312\n",
      "iteration 60 \t : \t loss 0.02096 - accuracy 0.94434\n",
      "iteration 61 \t : \t loss 0.02061 - accuracy 0.94580\n",
      "iteration 62 \t : \t loss 0.02026 - accuracy 0.94702\n",
      "iteration 63 \t : \t loss 0.01992 - accuracy 0.94824\n",
      "iteration 64 \t : \t loss 0.01958 - accuracy 0.94971\n",
      "iteration 65 \t : \t loss 0.01924 - accuracy 0.95117\n",
      "iteration 66 \t : \t loss 0.01891 - accuracy 0.95215\n",
      "iteration 67 \t : \t loss 0.01858 - accuracy 0.95337\n",
      "iteration 68 \t : \t loss 0.01825 - accuracy 0.95483\n",
      "iteration 69 \t : \t loss 0.01793 - accuracy 0.95630\n",
      "iteration 70 \t : \t loss 0.01761 - accuracy 0.95728\n",
      "iteration 71 \t : \t loss 0.01730 - accuracy 0.95874\n",
      "iteration 72 \t : \t loss 0.01699 - accuracy 0.96045\n",
      "iteration 73 \t : \t loss 0.01668 - accuracy 0.96094\n",
      "iteration 74 \t : \t loss 0.01637 - accuracy 0.96191\n",
      "iteration 75 \t : \t loss 0.01607 - accuracy 0.96240\n",
      "iteration 76 \t : \t loss 0.01577 - accuracy 0.96338\n",
      "iteration 77 \t : \t loss 0.01547 - accuracy 0.96387\n",
      "iteration 78 \t : \t loss 0.01518 - accuracy 0.96436\n",
      "iteration 79 \t : \t loss 0.01489 - accuracy 0.96558\n",
      "iteration 80 \t : \t loss 0.01460 - accuracy 0.96655\n",
      "iteration 81 \t : \t loss 0.01432 - accuracy 0.96680\n",
      "iteration 82 \t : \t loss 0.01404 - accuracy 0.96704\n",
      "iteration 83 \t : \t loss 0.01376 - accuracy 0.96802\n",
      "iteration 84 \t : \t loss 0.01348 - accuracy 0.96899\n",
      "iteration 85 \t : \t loss 0.01321 - accuracy 0.96924\n",
      "iteration 86 \t : \t loss 0.01294 - accuracy 0.97021\n",
      "iteration 87 \t : \t loss 0.01267 - accuracy 0.97144\n",
      "iteration 88 \t : \t loss 0.01241 - accuracy 0.97168\n",
      "iteration 89 \t : \t loss 0.01215 - accuracy 0.97192\n",
      "iteration 90 \t : \t loss 0.01189 - accuracy 0.97266\n",
      "iteration 91 \t : \t loss 0.01164 - accuracy 0.97363\n",
      "iteration 92 \t : \t loss 0.01139 - accuracy 0.97437\n",
      "iteration 93 \t : \t loss 0.01114 - accuracy 0.97437\n",
      "iteration 94 \t : \t loss 0.01090 - accuracy 0.97534\n",
      "iteration 95 \t : \t loss 0.01066 - accuracy 0.97607\n",
      "iteration 96 \t : \t loss 0.01042 - accuracy 0.97705\n",
      "iteration 97 \t : \t loss 0.01019 - accuracy 0.97827\n",
      "iteration 98 \t : \t loss 0.00995 - accuracy 0.97852\n",
      "iteration 99 \t : \t loss 0.00973 - accuracy 0.97925\n",
      "iteration 100 \t : \t loss 0.00950 - accuracy 0.97998\n",
      "iteration 101 \t : \t loss 0.00928 - accuracy 0.97998\n",
      "iteration 102 \t : \t loss 0.00907 - accuracy 0.98120\n",
      "iteration 103 \t : \t loss 0.00886 - accuracy 0.98193\n",
      "iteration 104 \t : \t loss 0.00865 - accuracy 0.98364\n",
      "iteration 105 \t : \t loss 0.00844 - accuracy 0.98438\n",
      "iteration 106 \t : \t loss 0.00824 - accuracy 0.98560\n",
      "iteration 107 \t : \t loss 0.00804 - accuracy 0.98682\n",
      "iteration 108 \t : \t loss 0.00785 - accuracy 0.98779\n",
      "iteration 109 \t : \t loss 0.00766 - accuracy 0.98828\n",
      "iteration 110 \t : \t loss 0.00748 - accuracy 0.98877\n",
      "iteration 111 \t : \t loss 0.00730 - accuracy 0.98901\n",
      "iteration 112 \t : \t loss 0.00712 - accuracy 0.98926\n",
      "iteration 113 \t : \t loss 0.00694 - accuracy 0.98926\n",
      "iteration 114 \t : \t loss 0.00678 - accuracy 0.98926\n",
      "iteration 115 \t : \t loss 0.00661 - accuracy 0.98950\n",
      "iteration 116 \t : \t loss 0.00645 - accuracy 0.99023\n",
      "iteration 117 \t : \t loss 0.00629 - accuracy 0.99023\n",
      "iteration 118 \t : \t loss 0.00614 - accuracy 0.99097\n",
      "iteration 119 \t : \t loss 0.00599 - accuracy 0.99170\n",
      "iteration 120 \t : \t loss 0.00584 - accuracy 0.99219\n",
      "iteration 121 \t : \t loss 0.00570 - accuracy 0.99243\n",
      "iteration 122 \t : \t loss 0.00556 - accuracy 0.99292\n",
      "iteration 123 \t : \t loss 0.00543 - accuracy 0.99292\n",
      "iteration 124 \t : \t loss 0.00530 - accuracy 0.99292\n",
      "iteration 125 \t : \t loss 0.00517 - accuracy 0.99292\n",
      "iteration 126 \t : \t loss 0.00505 - accuracy 0.99316\n",
      "iteration 127 \t : \t loss 0.00493 - accuracy 0.99341\n",
      "iteration 128 \t : \t loss 0.00481 - accuracy 0.99341\n",
      "iteration 129 \t : \t loss 0.00469 - accuracy 0.99463\n",
      "iteration 130 \t : \t loss 0.00458 - accuracy 0.99561\n",
      "iteration 131 \t : \t loss 0.00447 - accuracy 0.99585\n",
      "iteration 132 \t : \t loss 0.00437 - accuracy 0.99585\n",
      "iteration 133 \t : \t loss 0.00426 - accuracy 0.99585\n",
      "iteration 134 \t : \t loss 0.00416 - accuracy 0.99585\n",
      "iteration 135 \t : \t loss 0.00407 - accuracy 0.99585\n",
      "iteration 136 \t : \t loss 0.00397 - accuracy 0.99609\n",
      "iteration 137 \t : \t loss 0.00388 - accuracy 0.99634\n",
      "iteration 138 \t : \t loss 0.00379 - accuracy 0.99634\n",
      "iteration 139 \t : \t loss 0.00370 - accuracy 0.99634\n",
      "iteration 140 \t : \t loss 0.00362 - accuracy 0.99658\n",
      "iteration 141 \t : \t loss 0.00354 - accuracy 0.99707\n",
      "iteration 142 \t : \t loss 0.00346 - accuracy 0.99707\n",
      "iteration 143 \t : \t loss 0.00338 - accuracy 0.99731\n",
      "iteration 144 \t : \t loss 0.00330 - accuracy 0.99731\n",
      "iteration 145 \t : \t loss 0.00323 - accuracy 0.99731\n",
      "iteration 146 \t : \t loss 0.00316 - accuracy 0.99756\n",
      "iteration 147 \t : \t loss 0.00309 - accuracy 0.99805\n",
      "iteration 148 \t : \t loss 0.00302 - accuracy 0.99829\n",
      "iteration 149 \t : \t loss 0.00295 - accuracy 0.99829\n",
      "iteration 150 \t : \t loss 0.00289 - accuracy 0.99854\n",
      "iteration 151 \t : \t loss 0.00283 - accuracy 0.99854\n",
      "iteration 152 \t : \t loss 0.00277 - accuracy 0.99854\n",
      "iteration 153 \t : \t loss 0.00271 - accuracy 0.99854\n",
      "iteration 154 \t : \t loss 0.00265 - accuracy 0.99854\n",
      "iteration 155 \t : \t loss 0.00260 - accuracy 0.99854\n",
      "iteration 156 \t : \t loss 0.00254 - accuracy 0.99854\n",
      "iteration 157 \t : \t loss 0.00249 - accuracy 0.99854\n",
      "iteration 158 \t : \t loss 0.00244 - accuracy 0.99878\n",
      "iteration 159 \t : \t loss 0.00239 - accuracy 0.99878\n",
      "iteration 160 \t : \t loss 0.00234 - accuracy 0.99927\n",
      "iteration 161 \t : \t loss 0.00230 - accuracy 0.99927\n",
      "iteration 162 \t : \t loss 0.00225 - accuracy 0.99951\n",
      "iteration 163 \t : \t loss 0.00221 - accuracy 0.99976\n",
      "iteration 164 \t : \t loss 0.00217 - accuracy 0.99976\n",
      "iteration 165 \t : \t loss 0.00212 - accuracy 0.99976\n",
      "iteration 166 \t : \t loss 0.00208 - accuracy 0.99976\n",
      "iteration 167 \t : \t loss 0.00205 - accuracy 0.99976\n",
      "iteration 168 \t : \t loss 0.00201 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00197 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00194 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00190 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00187 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00183 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00180 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00177 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00174 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00171 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00165 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00163 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00160 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00157 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00155 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00153 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00150 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00148 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00146 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00143 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00139 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.12139 - accuracy 0.67993\n",
      "iteration 1 \t : \t loss 0.06072 - accuracy 0.86816\n",
      "iteration 2 \t : \t loss 0.04769 - accuracy 0.88647\n",
      "iteration 3 \t : \t loss 0.04145 - accuracy 0.89502\n",
      "iteration 4 \t : \t loss 0.03759 - accuracy 0.90283\n",
      "iteration 5 \t : \t loss 0.03487 - accuracy 0.90771\n",
      "iteration 6 \t : \t loss 0.03279 - accuracy 0.91211\n",
      "iteration 7 \t : \t loss 0.03112 - accuracy 0.91528\n",
      "iteration 8 \t : \t loss 0.02972 - accuracy 0.91919\n",
      "iteration 9 \t : \t loss 0.02851 - accuracy 0.92310\n",
      "iteration 10 \t : \t loss 0.02745 - accuracy 0.92700\n",
      "iteration 11 \t : \t loss 0.02650 - accuracy 0.92847\n",
      "iteration 12 \t : \t loss 0.02564 - accuracy 0.93018\n",
      "iteration 13 \t : \t loss 0.02486 - accuracy 0.93237\n",
      "iteration 14 \t : \t loss 0.02413 - accuracy 0.93335\n",
      "iteration 15 \t : \t loss 0.02346 - accuracy 0.93530\n",
      "iteration 16 \t : \t loss 0.02283 - accuracy 0.93628\n",
      "iteration 17 \t : \t loss 0.02223 - accuracy 0.93726\n",
      "iteration 18 \t : \t loss 0.02167 - accuracy 0.93799\n",
      "iteration 19 \t : \t loss 0.02114 - accuracy 0.93970\n",
      "iteration 20 \t : \t loss 0.02064 - accuracy 0.94165\n",
      "iteration 21 \t : \t loss 0.02015 - accuracy 0.94312\n",
      "iteration 22 \t : \t loss 0.01969 - accuracy 0.94434\n",
      "iteration 23 \t : \t loss 0.01925 - accuracy 0.94507\n",
      "iteration 24 \t : \t loss 0.01883 - accuracy 0.94629\n",
      "iteration 25 \t : \t loss 0.01842 - accuracy 0.94702\n",
      "iteration 26 \t : \t loss 0.01803 - accuracy 0.94751\n",
      "iteration 27 \t : \t loss 0.01765 - accuracy 0.94824\n",
      "iteration 28 \t : \t loss 0.01729 - accuracy 0.94995\n",
      "iteration 29 \t : \t loss 0.01694 - accuracy 0.95239\n",
      "iteration 30 \t : \t loss 0.01660 - accuracy 0.95337\n",
      "iteration 31 \t : \t loss 0.01627 - accuracy 0.95435\n",
      "iteration 32 \t : \t loss 0.01595 - accuracy 0.95605\n",
      "iteration 33 \t : \t loss 0.01564 - accuracy 0.95703\n",
      "iteration 34 \t : \t loss 0.01533 - accuracy 0.95850\n",
      "iteration 35 \t : \t loss 0.01504 - accuracy 0.95996\n",
      "iteration 36 \t : \t loss 0.01476 - accuracy 0.96143\n",
      "iteration 37 \t : \t loss 0.01448 - accuracy 0.96265\n",
      "iteration 38 \t : \t loss 0.01421 - accuracy 0.96289\n",
      "iteration 39 \t : \t loss 0.01395 - accuracy 0.96411\n",
      "iteration 40 \t : \t loss 0.01369 - accuracy 0.96460\n",
      "iteration 41 \t : \t loss 0.01344 - accuracy 0.96631\n",
      "iteration 42 \t : \t loss 0.01320 - accuracy 0.96704\n",
      "iteration 43 \t : \t loss 0.01296 - accuracy 0.96777\n",
      "iteration 44 \t : \t loss 0.01273 - accuracy 0.96851\n",
      "iteration 45 \t : \t loss 0.01251 - accuracy 0.96948\n",
      "iteration 46 \t : \t loss 0.01229 - accuracy 0.97046\n",
      "iteration 47 \t : \t loss 0.01207 - accuracy 0.97070\n",
      "iteration 48 \t : \t loss 0.01186 - accuracy 0.97168\n",
      "iteration 49 \t : \t loss 0.01166 - accuracy 0.97217\n",
      "iteration 50 \t : \t loss 0.01145 - accuracy 0.97314\n",
      "iteration 51 \t : \t loss 0.01126 - accuracy 0.97388\n",
      "iteration 52 \t : \t loss 0.01106 - accuracy 0.97510\n",
      "iteration 53 \t : \t loss 0.01088 - accuracy 0.97583\n",
      "iteration 54 \t : \t loss 0.01069 - accuracy 0.97632\n",
      "iteration 55 \t : \t loss 0.01051 - accuracy 0.97656\n",
      "iteration 56 \t : \t loss 0.01034 - accuracy 0.97729\n",
      "iteration 57 \t : \t loss 0.01016 - accuracy 0.97778\n",
      "iteration 58 \t : \t loss 0.00999 - accuracy 0.97827\n",
      "iteration 59 \t : \t loss 0.00983 - accuracy 0.97852\n",
      "iteration 60 \t : \t loss 0.00967 - accuracy 0.97925\n",
      "iteration 61 \t : \t loss 0.00951 - accuracy 0.97949\n",
      "iteration 62 \t : \t loss 0.00935 - accuracy 0.97998\n",
      "iteration 63 \t : \t loss 0.00920 - accuracy 0.98071\n",
      "iteration 64 \t : \t loss 0.00905 - accuracy 0.98145\n",
      "iteration 65 \t : \t loss 0.00890 - accuracy 0.98218\n",
      "iteration 66 \t : \t loss 0.00876 - accuracy 0.98267\n",
      "iteration 67 \t : \t loss 0.00862 - accuracy 0.98291\n",
      "iteration 68 \t : \t loss 0.00848 - accuracy 0.98291\n",
      "iteration 69 \t : \t loss 0.00835 - accuracy 0.98291\n",
      "iteration 70 \t : \t loss 0.00821 - accuracy 0.98315\n",
      "iteration 71 \t : \t loss 0.00808 - accuracy 0.98364\n",
      "iteration 72 \t : \t loss 0.00796 - accuracy 0.98413\n",
      "iteration 73 \t : \t loss 0.00783 - accuracy 0.98511\n",
      "iteration 74 \t : \t loss 0.00771 - accuracy 0.98535\n",
      "iteration 75 \t : \t loss 0.00759 - accuracy 0.98560\n",
      "iteration 76 \t : \t loss 0.00747 - accuracy 0.98584\n",
      "iteration 77 \t : \t loss 0.00736 - accuracy 0.98584\n",
      "iteration 78 \t : \t loss 0.00724 - accuracy 0.98633\n",
      "iteration 79 \t : \t loss 0.00713 - accuracy 0.98682\n",
      "iteration 80 \t : \t loss 0.00702 - accuracy 0.98706\n",
      "iteration 81 \t : \t loss 0.00692 - accuracy 0.98706\n",
      "iteration 82 \t : \t loss 0.00681 - accuracy 0.98706\n",
      "iteration 83 \t : \t loss 0.00671 - accuracy 0.98828\n",
      "iteration 84 \t : \t loss 0.00661 - accuracy 0.98828\n",
      "iteration 85 \t : \t loss 0.00651 - accuracy 0.98877\n",
      "iteration 86 \t : \t loss 0.00641 - accuracy 0.98901\n",
      "iteration 87 \t : \t loss 0.00632 - accuracy 0.98999\n",
      "iteration 88 \t : \t loss 0.00622 - accuracy 0.99023\n",
      "iteration 89 \t : \t loss 0.00613 - accuracy 0.99072\n",
      "iteration 90 \t : \t loss 0.00604 - accuracy 0.99097\n",
      "iteration 91 \t : \t loss 0.00595 - accuracy 0.99121\n",
      "iteration 92 \t : \t loss 0.00587 - accuracy 0.99146\n",
      "iteration 93 \t : \t loss 0.00578 - accuracy 0.99146\n",
      "iteration 94 \t : \t loss 0.00570 - accuracy 0.99170\n",
      "iteration 95 \t : \t loss 0.00562 - accuracy 0.99194\n",
      "iteration 96 \t : \t loss 0.00554 - accuracy 0.99219\n",
      "iteration 97 \t : \t loss 0.00546 - accuracy 0.99219\n",
      "iteration 98 \t : \t loss 0.00538 - accuracy 0.99292\n",
      "iteration 99 \t : \t loss 0.00530 - accuracy 0.99292\n",
      "iteration 100 \t : \t loss 0.00523 - accuracy 0.99316\n",
      "iteration 101 \t : \t loss 0.00516 - accuracy 0.99341\n",
      "iteration 102 \t : \t loss 0.00508 - accuracy 0.99341\n",
      "iteration 103 \t : \t loss 0.00501 - accuracy 0.99365\n",
      "iteration 104 \t : \t loss 0.00495 - accuracy 0.99390\n",
      "iteration 105 \t : \t loss 0.00488 - accuracy 0.99390\n",
      "iteration 106 \t : \t loss 0.00481 - accuracy 0.99487\n",
      "iteration 107 \t : \t loss 0.00474 - accuracy 0.99487\n",
      "iteration 108 \t : \t loss 0.00468 - accuracy 0.99512\n",
      "iteration 109 \t : \t loss 0.00462 - accuracy 0.99512\n",
      "iteration 110 \t : \t loss 0.00455 - accuracy 0.99512\n",
      "iteration 111 \t : \t loss 0.00449 - accuracy 0.99536\n",
      "iteration 112 \t : \t loss 0.00443 - accuracy 0.99536\n",
      "iteration 113 \t : \t loss 0.00437 - accuracy 0.99536\n",
      "iteration 114 \t : \t loss 0.00432 - accuracy 0.99536\n",
      "iteration 115 \t : \t loss 0.00426 - accuracy 0.99536\n",
      "iteration 116 \t : \t loss 0.00420 - accuracy 0.99561\n",
      "iteration 117 \t : \t loss 0.00415 - accuracy 0.99561\n",
      "iteration 118 \t : \t loss 0.00410 - accuracy 0.99585\n",
      "iteration 119 \t : \t loss 0.00404 - accuracy 0.99609\n",
      "iteration 120 \t : \t loss 0.00399 - accuracy 0.99609\n",
      "iteration 121 \t : \t loss 0.00394 - accuracy 0.99658\n",
      "iteration 122 \t : \t loss 0.00389 - accuracy 0.99683\n",
      "iteration 123 \t : \t loss 0.00384 - accuracy 0.99683\n",
      "iteration 124 \t : \t loss 0.00379 - accuracy 0.99683\n",
      "iteration 125 \t : \t loss 0.00374 - accuracy 0.99683\n",
      "iteration 126 \t : \t loss 0.00369 - accuracy 0.99731\n",
      "iteration 127 \t : \t loss 0.00365 - accuracy 0.99731\n",
      "iteration 128 \t : \t loss 0.00360 - accuracy 0.99756\n",
      "iteration 129 \t : \t loss 0.00356 - accuracy 0.99756\n",
      "iteration 130 \t : \t loss 0.00351 - accuracy 0.99756\n",
      "iteration 131 \t : \t loss 0.00347 - accuracy 0.99756\n",
      "iteration 132 \t : \t loss 0.00343 - accuracy 0.99756\n",
      "iteration 133 \t : \t loss 0.00339 - accuracy 0.99756\n",
      "iteration 134 \t : \t loss 0.00334 - accuracy 0.99756\n",
      "iteration 135 \t : \t loss 0.00330 - accuracy 0.99756\n",
      "iteration 136 \t : \t loss 0.00326 - accuracy 0.99756\n",
      "iteration 137 \t : \t loss 0.00322 - accuracy 0.99780\n",
      "iteration 138 \t : \t loss 0.00319 - accuracy 0.99780\n",
      "iteration 139 \t : \t loss 0.00315 - accuracy 0.99780\n",
      "iteration 140 \t : \t loss 0.00311 - accuracy 0.99780\n",
      "iteration 141 \t : \t loss 0.00307 - accuracy 0.99780\n",
      "iteration 142 \t : \t loss 0.00304 - accuracy 0.99780\n",
      "iteration 143 \t : \t loss 0.00300 - accuracy 0.99780\n",
      "iteration 144 \t : \t loss 0.00297 - accuracy 0.99780\n",
      "iteration 145 \t : \t loss 0.00293 - accuracy 0.99780\n",
      "iteration 146 \t : \t loss 0.00290 - accuracy 0.99780\n",
      "iteration 147 \t : \t loss 0.00286 - accuracy 0.99780\n",
      "iteration 148 \t : \t loss 0.00283 - accuracy 0.99780\n",
      "iteration 149 \t : \t loss 0.00280 - accuracy 0.99780\n",
      "iteration 150 \t : \t loss 0.00277 - accuracy 0.99780\n",
      "iteration 151 \t : \t loss 0.00274 - accuracy 0.99780\n",
      "iteration 152 \t : \t loss 0.00271 - accuracy 0.99780\n",
      "iteration 153 \t : \t loss 0.00268 - accuracy 0.99780\n",
      "iteration 154 \t : \t loss 0.00265 - accuracy 0.99780\n",
      "iteration 155 \t : \t loss 0.00262 - accuracy 0.99780\n",
      "iteration 156 \t : \t loss 0.00259 - accuracy 0.99780\n",
      "iteration 157 \t : \t loss 0.00256 - accuracy 0.99780\n",
      "iteration 158 \t : \t loss 0.00253 - accuracy 0.99780\n",
      "iteration 159 \t : \t loss 0.00250 - accuracy 0.99780\n",
      "iteration 160 \t : \t loss 0.00248 - accuracy 0.99780\n",
      "iteration 161 \t : \t loss 0.00245 - accuracy 0.99780\n",
      "iteration 162 \t : \t loss 0.00242 - accuracy 0.99780\n",
      "iteration 163 \t : \t loss 0.00240 - accuracy 0.99780\n",
      "iteration 164 \t : \t loss 0.00237 - accuracy 0.99780\n",
      "iteration 165 \t : \t loss 0.00235 - accuracy 0.99780\n",
      "iteration 166 \t : \t loss 0.00232 - accuracy 0.99780\n",
      "iteration 167 \t : \t loss 0.00230 - accuracy 0.99780\n",
      "iteration 168 \t : \t loss 0.00227 - accuracy 0.99780\n",
      "iteration 169 \t : \t loss 0.00225 - accuracy 0.99805\n",
      "iteration 170 \t : \t loss 0.00223 - accuracy 0.99805\n",
      "iteration 171 \t : \t loss 0.00220 - accuracy 0.99805\n",
      "iteration 172 \t : \t loss 0.00218 - accuracy 0.99805\n",
      "iteration 173 \t : \t loss 0.00216 - accuracy 0.99805\n",
      "iteration 174 \t : \t loss 0.00214 - accuracy 0.99805\n",
      "iteration 175 \t : \t loss 0.00211 - accuracy 0.99805\n",
      "iteration 176 \t : \t loss 0.00209 - accuracy 0.99805\n",
      "iteration 177 \t : \t loss 0.00207 - accuracy 0.99805\n",
      "iteration 178 \t : \t loss 0.00205 - accuracy 0.99805\n",
      "iteration 179 \t : \t loss 0.00203 - accuracy 0.99829\n",
      "iteration 180 \t : \t loss 0.00201 - accuracy 0.99829\n",
      "iteration 181 \t : \t loss 0.00199 - accuracy 0.99829\n",
      "iteration 182 \t : \t loss 0.00197 - accuracy 0.99829\n",
      "iteration 183 \t : \t loss 0.00195 - accuracy 0.99854\n",
      "iteration 184 \t : \t loss 0.00193 - accuracy 0.99854\n",
      "iteration 185 \t : \t loss 0.00192 - accuracy 0.99854\n",
      "iteration 186 \t : \t loss 0.00190 - accuracy 0.99902\n",
      "iteration 187 \t : \t loss 0.00188 - accuracy 0.99902\n",
      "iteration 188 \t : \t loss 0.00186 - accuracy 0.99902\n",
      "iteration 189 \t : \t loss 0.00185 - accuracy 0.99902\n",
      "iteration 190 \t : \t loss 0.00183 - accuracy 0.99927\n",
      "iteration 191 \t : \t loss 0.00181 - accuracy 0.99927\n",
      "iteration 192 \t : \t loss 0.00179 - accuracy 0.99927\n",
      "iteration 193 \t : \t loss 0.00178 - accuracy 0.99976\n",
      "iteration 194 \t : \t loss 0.00176 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00175 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00173 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00171 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00170 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.23324 - accuracy 0.10547\n",
      "iteration 1 \t : \t loss 0.23209 - accuracy 0.10693\n",
      "iteration 2 \t : \t loss 0.23156 - accuracy 0.10742\n",
      "iteration 3 \t : \t loss 0.23100 - accuracy 0.10815\n",
      "iteration 4 \t : \t loss 0.23037 - accuracy 0.11060\n",
      "iteration 5 \t : \t loss 0.22964 - accuracy 0.11230\n",
      "iteration 6 \t : \t loss 0.22875 - accuracy 0.11938\n",
      "iteration 7 \t : \t loss 0.22758 - accuracy 0.13477\n",
      "iteration 8 \t : \t loss 0.22594 - accuracy 0.15820\n",
      "iteration 9 \t : \t loss 0.22349 - accuracy 0.19165\n",
      "iteration 10 \t : \t loss 0.21952 - accuracy 0.23926\n",
      "iteration 11 \t : \t loss 0.21269 - accuracy 0.28979\n",
      "iteration 12 \t : \t loss 0.20147 - accuracy 0.32983\n",
      "iteration 13 \t : \t loss 0.18658 - accuracy 0.36816\n",
      "iteration 14 \t : \t loss 0.17104 - accuracy 0.41357\n",
      "iteration 15 \t : \t loss 0.15603 - accuracy 0.45410\n",
      "iteration 16 \t : \t loss 0.14268 - accuracy 0.48828\n",
      "iteration 17 \t : \t loss 0.13236 - accuracy 0.52026\n",
      "iteration 18 \t : \t loss 0.12459 - accuracy 0.54761\n",
      "iteration 19 \t : \t loss 0.11816 - accuracy 0.57300\n",
      "iteration 20 \t : \t loss 0.11221 - accuracy 0.60596\n",
      "iteration 21 \t : \t loss 0.10634 - accuracy 0.63477\n",
      "iteration 22 \t : \t loss 0.10041 - accuracy 0.66528\n",
      "iteration 23 \t : \t loss 0.09451 - accuracy 0.68823\n",
      "iteration 24 \t : \t loss 0.08890 - accuracy 0.71021\n",
      "iteration 25 \t : \t loss 0.08387 - accuracy 0.73022\n",
      "iteration 26 \t : \t loss 0.07957 - accuracy 0.74390\n",
      "iteration 27 \t : \t loss 0.07595 - accuracy 0.75610\n",
      "iteration 28 \t : \t loss 0.07287 - accuracy 0.76831\n",
      "iteration 29 \t : \t loss 0.07019 - accuracy 0.77661\n",
      "iteration 30 \t : \t loss 0.06779 - accuracy 0.78442\n",
      "iteration 31 \t : \t loss 0.06560 - accuracy 0.79102\n",
      "iteration 32 \t : \t loss 0.06356 - accuracy 0.80078\n",
      "iteration 33 \t : \t loss 0.06164 - accuracy 0.80786\n",
      "iteration 34 \t : \t loss 0.05979 - accuracy 0.81689\n",
      "iteration 35 \t : \t loss 0.05802 - accuracy 0.82251\n",
      "iteration 36 \t : \t loss 0.05629 - accuracy 0.82690\n",
      "iteration 37 \t : \t loss 0.05461 - accuracy 0.83228\n",
      "iteration 38 \t : \t loss 0.05298 - accuracy 0.83911\n",
      "iteration 39 \t : \t loss 0.05138 - accuracy 0.84570\n",
      "iteration 40 \t : \t loss 0.04982 - accuracy 0.85400\n",
      "iteration 41 \t : \t loss 0.04831 - accuracy 0.85693\n",
      "iteration 42 \t : \t loss 0.04684 - accuracy 0.86255\n",
      "iteration 43 \t : \t loss 0.04542 - accuracy 0.86621\n",
      "iteration 44 \t : \t loss 0.04406 - accuracy 0.86914\n",
      "iteration 45 \t : \t loss 0.04274 - accuracy 0.87402\n",
      "iteration 46 \t : \t loss 0.04149 - accuracy 0.87988\n",
      "iteration 47 \t : \t loss 0.04029 - accuracy 0.88452\n",
      "iteration 48 \t : \t loss 0.03914 - accuracy 0.88696\n",
      "iteration 49 \t : \t loss 0.03805 - accuracy 0.88843\n",
      "iteration 50 \t : \t loss 0.03700 - accuracy 0.89062\n",
      "iteration 51 \t : \t loss 0.03600 - accuracy 0.89331\n",
      "iteration 52 \t : \t loss 0.03504 - accuracy 0.89844\n",
      "iteration 53 \t : \t loss 0.03412 - accuracy 0.89966\n",
      "iteration 54 \t : \t loss 0.03324 - accuracy 0.90259\n",
      "iteration 55 \t : \t loss 0.03238 - accuracy 0.90552\n",
      "iteration 56 \t : \t loss 0.03155 - accuracy 0.90796\n",
      "iteration 57 \t : \t loss 0.03075 - accuracy 0.91138\n",
      "iteration 58 \t : \t loss 0.02997 - accuracy 0.91479\n",
      "iteration 59 \t : \t loss 0.02922 - accuracy 0.91553\n",
      "iteration 60 \t : \t loss 0.02848 - accuracy 0.91943\n",
      "iteration 61 \t : \t loss 0.02776 - accuracy 0.91943\n",
      "iteration 62 \t : \t loss 0.02705 - accuracy 0.92139\n",
      "iteration 63 \t : \t loss 0.02636 - accuracy 0.92236\n",
      "iteration 64 \t : \t loss 0.02568 - accuracy 0.92383\n",
      "iteration 65 \t : \t loss 0.02502 - accuracy 0.92725\n",
      "iteration 66 \t : \t loss 0.02437 - accuracy 0.93018\n",
      "iteration 67 \t : \t loss 0.02373 - accuracy 0.93042\n",
      "iteration 68 \t : \t loss 0.02310 - accuracy 0.93164\n",
      "iteration 69 \t : \t loss 0.02248 - accuracy 0.93286\n",
      "iteration 70 \t : \t loss 0.02187 - accuracy 0.93555\n",
      "iteration 71 \t : \t loss 0.02127 - accuracy 0.93750\n",
      "iteration 72 \t : \t loss 0.02068 - accuracy 0.93945\n",
      "iteration 73 \t : \t loss 0.02009 - accuracy 0.94189\n",
      "iteration 74 \t : \t loss 0.01952 - accuracy 0.94360\n",
      "iteration 75 \t : \t loss 0.01896 - accuracy 0.94653\n",
      "iteration 76 \t : \t loss 0.01840 - accuracy 0.94800\n",
      "iteration 77 \t : \t loss 0.01786 - accuracy 0.94946\n",
      "iteration 78 \t : \t loss 0.01732 - accuracy 0.95093\n",
      "iteration 79 \t : \t loss 0.01679 - accuracy 0.95312\n",
      "iteration 80 \t : \t loss 0.01627 - accuracy 0.95532\n",
      "iteration 81 \t : \t loss 0.01576 - accuracy 0.95801\n",
      "iteration 82 \t : \t loss 0.01526 - accuracy 0.95898\n",
      "iteration 83 \t : \t loss 0.01477 - accuracy 0.96094\n",
      "iteration 84 \t : \t loss 0.01429 - accuracy 0.96191\n",
      "iteration 85 \t : \t loss 0.01382 - accuracy 0.96484\n",
      "iteration 86 \t : \t loss 0.01336 - accuracy 0.96680\n",
      "iteration 87 \t : \t loss 0.01291 - accuracy 0.96851\n",
      "iteration 88 \t : \t loss 0.01247 - accuracy 0.97144\n",
      "iteration 89 \t : \t loss 0.01204 - accuracy 0.97314\n",
      "iteration 90 \t : \t loss 0.01162 - accuracy 0.97461\n",
      "iteration 91 \t : \t loss 0.01121 - accuracy 0.97583\n",
      "iteration 92 \t : \t loss 0.01081 - accuracy 0.97632\n",
      "iteration 93 \t : \t loss 0.01043 - accuracy 0.97778\n",
      "iteration 94 \t : \t loss 0.01006 - accuracy 0.97925\n",
      "iteration 95 \t : \t loss 0.00970 - accuracy 0.98047\n",
      "iteration 96 \t : \t loss 0.00935 - accuracy 0.98071\n",
      "iteration 97 \t : \t loss 0.00902 - accuracy 0.98145\n",
      "iteration 98 \t : \t loss 0.00869 - accuracy 0.98218\n",
      "iteration 99 \t : \t loss 0.00838 - accuracy 0.98364\n",
      "iteration 100 \t : \t loss 0.00808 - accuracy 0.98560\n",
      "iteration 101 \t : \t loss 0.00780 - accuracy 0.98633\n",
      "iteration 102 \t : \t loss 0.00752 - accuracy 0.98706\n",
      "iteration 103 \t : \t loss 0.00725 - accuracy 0.98804\n",
      "iteration 104 \t : \t loss 0.00699 - accuracy 0.98877\n",
      "iteration 105 \t : \t loss 0.00674 - accuracy 0.98950\n",
      "iteration 106 \t : \t loss 0.00649 - accuracy 0.98999\n",
      "iteration 107 \t : \t loss 0.00626 - accuracy 0.99048\n",
      "iteration 108 \t : \t loss 0.00604 - accuracy 0.99072\n",
      "iteration 109 \t : \t loss 0.00582 - accuracy 0.99146\n",
      "iteration 110 \t : \t loss 0.00561 - accuracy 0.99194\n",
      "iteration 111 \t : \t loss 0.00541 - accuracy 0.99292\n",
      "iteration 112 \t : \t loss 0.00522 - accuracy 0.99341\n",
      "iteration 113 \t : \t loss 0.00503 - accuracy 0.99365\n",
      "iteration 114 \t : \t loss 0.00485 - accuracy 0.99463\n",
      "iteration 115 \t : \t loss 0.00468 - accuracy 0.99487\n",
      "iteration 116 \t : \t loss 0.00452 - accuracy 0.99512\n",
      "iteration 117 \t : \t loss 0.00436 - accuracy 0.99536\n",
      "iteration 118 \t : \t loss 0.00421 - accuracy 0.99536\n",
      "iteration 119 \t : \t loss 0.00407 - accuracy 0.99634\n",
      "iteration 120 \t : \t loss 0.00393 - accuracy 0.99707\n",
      "iteration 121 \t : \t loss 0.00380 - accuracy 0.99731\n",
      "iteration 122 \t : \t loss 0.00367 - accuracy 0.99780\n",
      "iteration 123 \t : \t loss 0.00355 - accuracy 0.99780\n",
      "iteration 124 \t : \t loss 0.00344 - accuracy 0.99780\n",
      "iteration 125 \t : \t loss 0.00332 - accuracy 0.99780\n",
      "iteration 126 \t : \t loss 0.00322 - accuracy 0.99780\n",
      "iteration 127 \t : \t loss 0.00312 - accuracy 0.99829\n",
      "iteration 128 \t : \t loss 0.00302 - accuracy 0.99829\n",
      "iteration 129 \t : \t loss 0.00292 - accuracy 0.99829\n",
      "iteration 130 \t : \t loss 0.00283 - accuracy 0.99878\n",
      "iteration 131 \t : \t loss 0.00275 - accuracy 0.99878\n",
      "iteration 132 \t : \t loss 0.00266 - accuracy 0.99878\n",
      "iteration 133 \t : \t loss 0.00258 - accuracy 0.99902\n",
      "iteration 134 \t : \t loss 0.00251 - accuracy 0.99902\n",
      "iteration 135 \t : \t loss 0.00243 - accuracy 0.99902\n",
      "iteration 136 \t : \t loss 0.00236 - accuracy 0.99902\n",
      "iteration 137 \t : \t loss 0.00230 - accuracy 0.99927\n",
      "iteration 138 \t : \t loss 0.00223 - accuracy 0.99927\n",
      "iteration 139 \t : \t loss 0.00217 - accuracy 0.99951\n",
      "iteration 140 \t : \t loss 0.00211 - accuracy 0.99951\n",
      "iteration 141 \t : \t loss 0.00206 - accuracy 0.99951\n",
      "iteration 142 \t : \t loss 0.00200 - accuracy 0.99976\n",
      "iteration 143 \t : \t loss 0.00195 - accuracy 1.00000\n",
      "iteration 144 \t : \t loss 0.00190 - accuracy 1.00000\n",
      "iteration 145 \t : \t loss 0.00185 - accuracy 1.00000\n",
      "iteration 146 \t : \t loss 0.00181 - accuracy 1.00000\n",
      "iteration 147 \t : \t loss 0.00176 - accuracy 1.00000\n",
      "iteration 148 \t : \t loss 0.00172 - accuracy 1.00000\n",
      "iteration 149 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 150 \t : \t loss 0.00164 - accuracy 1.00000\n",
      "iteration 151 \t : \t loss 0.00160 - accuracy 1.00000\n",
      "iteration 152 \t : \t loss 0.00157 - accuracy 1.00000\n",
      "iteration 153 \t : \t loss 0.00153 - accuracy 1.00000\n",
      "iteration 154 \t : \t loss 0.00150 - accuracy 1.00000\n",
      "iteration 155 \t : \t loss 0.00147 - accuracy 1.00000\n",
      "iteration 156 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 157 \t : \t loss 0.00140 - accuracy 1.00000\n",
      "iteration 158 \t : \t loss 0.00138 - accuracy 1.00000\n",
      "iteration 159 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 160 \t : \t loss 0.00132 - accuracy 1.00000\n",
      "iteration 161 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 162 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 163 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00111 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00107 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00105 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00103 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00102 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00100 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00097 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00095 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00092 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00091 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00089 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00086 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00084 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00079 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00075 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00074 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00073 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00072 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00071 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00070 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.12344 - accuracy 0.69775\n",
      "iteration 1 \t : \t loss 0.05922 - accuracy 0.87427\n",
      "iteration 2 \t : \t loss 0.04555 - accuracy 0.89404\n",
      "iteration 3 \t : \t loss 0.03913 - accuracy 0.90527\n",
      "iteration 4 \t : \t loss 0.03521 - accuracy 0.91357\n",
      "iteration 5 \t : \t loss 0.03247 - accuracy 0.91895\n",
      "iteration 6 \t : \t loss 0.03038 - accuracy 0.92383\n",
      "iteration 7 \t : \t loss 0.02871 - accuracy 0.92725\n",
      "iteration 8 \t : \t loss 0.02731 - accuracy 0.93115\n",
      "iteration 9 \t : \t loss 0.02611 - accuracy 0.93286\n",
      "iteration 10 \t : \t loss 0.02506 - accuracy 0.93384\n",
      "iteration 11 \t : \t loss 0.02412 - accuracy 0.93506\n",
      "iteration 12 \t : \t loss 0.02327 - accuracy 0.93677\n",
      "iteration 13 \t : \t loss 0.02249 - accuracy 0.93848\n",
      "iteration 14 \t : \t loss 0.02177 - accuracy 0.94092\n",
      "iteration 15 \t : \t loss 0.02111 - accuracy 0.94214\n",
      "iteration 16 \t : \t loss 0.02049 - accuracy 0.94312\n",
      "iteration 17 \t : \t loss 0.01990 - accuracy 0.94409\n",
      "iteration 18 \t : \t loss 0.01935 - accuracy 0.94604\n",
      "iteration 19 \t : \t loss 0.01882 - accuracy 0.94849\n",
      "iteration 20 \t : \t loss 0.01832 - accuracy 0.94873\n",
      "iteration 21 \t : \t loss 0.01785 - accuracy 0.95044\n",
      "iteration 22 \t : \t loss 0.01740 - accuracy 0.95117\n",
      "iteration 23 \t : \t loss 0.01696 - accuracy 0.95215\n",
      "iteration 24 \t : \t loss 0.01654 - accuracy 0.95435\n",
      "iteration 25 \t : \t loss 0.01614 - accuracy 0.95581\n",
      "iteration 26 \t : \t loss 0.01576 - accuracy 0.95703\n",
      "iteration 27 \t : \t loss 0.01539 - accuracy 0.95850\n",
      "iteration 28 \t : \t loss 0.01503 - accuracy 0.95947\n",
      "iteration 29 \t : \t loss 0.01468 - accuracy 0.96045\n",
      "iteration 30 \t : \t loss 0.01435 - accuracy 0.96143\n",
      "iteration 31 \t : \t loss 0.01402 - accuracy 0.96216\n",
      "iteration 32 \t : \t loss 0.01371 - accuracy 0.96362\n",
      "iteration 33 \t : \t loss 0.01340 - accuracy 0.96509\n",
      "iteration 34 \t : \t loss 0.01311 - accuracy 0.96680\n",
      "iteration 35 \t : \t loss 0.01282 - accuracy 0.96729\n",
      "iteration 36 \t : \t loss 0.01255 - accuracy 0.96777\n",
      "iteration 37 \t : \t loss 0.01228 - accuracy 0.96875\n",
      "iteration 38 \t : \t loss 0.01202 - accuracy 0.97021\n",
      "iteration 39 \t : \t loss 0.01176 - accuracy 0.97070\n",
      "iteration 40 \t : \t loss 0.01152 - accuracy 0.97144\n",
      "iteration 41 \t : \t loss 0.01128 - accuracy 0.97217\n",
      "iteration 42 \t : \t loss 0.01104 - accuracy 0.97266\n",
      "iteration 43 \t : \t loss 0.01082 - accuracy 0.97314\n",
      "iteration 44 \t : \t loss 0.01060 - accuracy 0.97388\n",
      "iteration 45 \t : \t loss 0.01038 - accuracy 0.97534\n",
      "iteration 46 \t : \t loss 0.01017 - accuracy 0.97583\n",
      "iteration 47 \t : \t loss 0.00997 - accuracy 0.97632\n",
      "iteration 48 \t : \t loss 0.00977 - accuracy 0.97681\n",
      "iteration 49 \t : \t loss 0.00958 - accuracy 0.97729\n",
      "iteration 50 \t : \t loss 0.00939 - accuracy 0.97827\n",
      "iteration 51 \t : \t loss 0.00920 - accuracy 0.97876\n",
      "iteration 52 \t : \t loss 0.00902 - accuracy 0.97900\n",
      "iteration 53 \t : \t loss 0.00885 - accuracy 0.97900\n",
      "iteration 54 \t : \t loss 0.00868 - accuracy 0.97925\n",
      "iteration 55 \t : \t loss 0.00851 - accuracy 0.97998\n",
      "iteration 56 \t : \t loss 0.00835 - accuracy 0.98047\n",
      "iteration 57 \t : \t loss 0.00819 - accuracy 0.98120\n",
      "iteration 58 \t : \t loss 0.00804 - accuracy 0.98169\n",
      "iteration 59 \t : \t loss 0.00788 - accuracy 0.98242\n",
      "iteration 60 \t : \t loss 0.00774 - accuracy 0.98291\n",
      "iteration 61 \t : \t loss 0.00759 - accuracy 0.98364\n",
      "iteration 62 \t : \t loss 0.00745 - accuracy 0.98389\n",
      "iteration 63 \t : \t loss 0.00731 - accuracy 0.98511\n",
      "iteration 64 \t : \t loss 0.00718 - accuracy 0.98535\n",
      "iteration 65 \t : \t loss 0.00705 - accuracy 0.98584\n",
      "iteration 66 \t : \t loss 0.00692 - accuracy 0.98633\n",
      "iteration 67 \t : \t loss 0.00679 - accuracy 0.98706\n",
      "iteration 68 \t : \t loss 0.00667 - accuracy 0.98755\n",
      "iteration 69 \t : \t loss 0.00655 - accuracy 0.98828\n",
      "iteration 70 \t : \t loss 0.00643 - accuracy 0.98853\n",
      "iteration 71 \t : \t loss 0.00632 - accuracy 0.98853\n",
      "iteration 72 \t : \t loss 0.00620 - accuracy 0.98853\n",
      "iteration 73 \t : \t loss 0.00609 - accuracy 0.98877\n",
      "iteration 74 \t : \t loss 0.00599 - accuracy 0.98877\n",
      "iteration 75 \t : \t loss 0.00588 - accuracy 0.98877\n",
      "iteration 76 \t : \t loss 0.00578 - accuracy 0.98877\n",
      "iteration 77 \t : \t loss 0.00568 - accuracy 0.98901\n",
      "iteration 78 \t : \t loss 0.00558 - accuracy 0.98950\n",
      "iteration 79 \t : \t loss 0.00548 - accuracy 0.98999\n",
      "iteration 80 \t : \t loss 0.00539 - accuracy 0.99023\n",
      "iteration 81 \t : \t loss 0.00530 - accuracy 0.99048\n",
      "iteration 82 \t : \t loss 0.00521 - accuracy 0.99072\n",
      "iteration 83 \t : \t loss 0.00512 - accuracy 0.99072\n",
      "iteration 84 \t : \t loss 0.00503 - accuracy 0.99097\n",
      "iteration 85 \t : \t loss 0.00495 - accuracy 0.99170\n",
      "iteration 86 \t : \t loss 0.00486 - accuracy 0.99219\n",
      "iteration 87 \t : \t loss 0.00478 - accuracy 0.99219\n",
      "iteration 88 \t : \t loss 0.00470 - accuracy 0.99268\n",
      "iteration 89 \t : \t loss 0.00463 - accuracy 0.99316\n",
      "iteration 90 \t : \t loss 0.00455 - accuracy 0.99316\n",
      "iteration 91 \t : \t loss 0.00448 - accuracy 0.99341\n",
      "iteration 92 \t : \t loss 0.00440 - accuracy 0.99341\n",
      "iteration 93 \t : \t loss 0.00433 - accuracy 0.99365\n",
      "iteration 94 \t : \t loss 0.00426 - accuracy 0.99390\n",
      "iteration 95 \t : \t loss 0.00419 - accuracy 0.99414\n",
      "iteration 96 \t : \t loss 0.00412 - accuracy 0.99414\n",
      "iteration 97 \t : \t loss 0.00406 - accuracy 0.99463\n",
      "iteration 98 \t : \t loss 0.00399 - accuracy 0.99463\n",
      "iteration 99 \t : \t loss 0.00393 - accuracy 0.99463\n",
      "iteration 100 \t : \t loss 0.00387 - accuracy 0.99512\n",
      "iteration 101 \t : \t loss 0.00381 - accuracy 0.99536\n",
      "iteration 102 \t : \t loss 0.00375 - accuracy 0.99536\n",
      "iteration 103 \t : \t loss 0.00369 - accuracy 0.99536\n",
      "iteration 104 \t : \t loss 0.00363 - accuracy 0.99561\n",
      "iteration 105 \t : \t loss 0.00358 - accuracy 0.99585\n",
      "iteration 106 \t : \t loss 0.00352 - accuracy 0.99609\n",
      "iteration 107 \t : \t loss 0.00347 - accuracy 0.99634\n",
      "iteration 108 \t : \t loss 0.00342 - accuracy 0.99658\n",
      "iteration 109 \t : \t loss 0.00337 - accuracy 0.99683\n",
      "iteration 110 \t : \t loss 0.00332 - accuracy 0.99707\n",
      "iteration 111 \t : \t loss 0.00327 - accuracy 0.99707\n",
      "iteration 112 \t : \t loss 0.00322 - accuracy 0.99780\n",
      "iteration 113 \t : \t loss 0.00317 - accuracy 0.99780\n",
      "iteration 114 \t : \t loss 0.00312 - accuracy 0.99780\n",
      "iteration 115 \t : \t loss 0.00308 - accuracy 0.99805\n",
      "iteration 116 \t : \t loss 0.00303 - accuracy 0.99805\n",
      "iteration 117 \t : \t loss 0.00299 - accuracy 0.99805\n",
      "iteration 118 \t : \t loss 0.00295 - accuracy 0.99805\n",
      "iteration 119 \t : \t loss 0.00290 - accuracy 0.99805\n",
      "iteration 120 \t : \t loss 0.00286 - accuracy 0.99829\n",
      "iteration 121 \t : \t loss 0.00282 - accuracy 0.99829\n",
      "iteration 122 \t : \t loss 0.00278 - accuracy 0.99829\n",
      "iteration 123 \t : \t loss 0.00274 - accuracy 0.99854\n",
      "iteration 124 \t : \t loss 0.00271 - accuracy 0.99854\n",
      "iteration 125 \t : \t loss 0.00267 - accuracy 0.99854\n",
      "iteration 126 \t : \t loss 0.00263 - accuracy 0.99854\n",
      "iteration 127 \t : \t loss 0.00260 - accuracy 0.99854\n",
      "iteration 128 \t : \t loss 0.00256 - accuracy 0.99854\n",
      "iteration 129 \t : \t loss 0.00252 - accuracy 0.99854\n",
      "iteration 130 \t : \t loss 0.00249 - accuracy 0.99854\n",
      "iteration 131 \t : \t loss 0.00246 - accuracy 0.99878\n",
      "iteration 132 \t : \t loss 0.00242 - accuracy 0.99878\n",
      "iteration 133 \t : \t loss 0.00239 - accuracy 0.99878\n",
      "iteration 134 \t : \t loss 0.00236 - accuracy 0.99878\n",
      "iteration 135 \t : \t loss 0.00233 - accuracy 0.99878\n",
      "iteration 136 \t : \t loss 0.00230 - accuracy 0.99878\n",
      "iteration 137 \t : \t loss 0.00227 - accuracy 0.99878\n",
      "iteration 138 \t : \t loss 0.00224 - accuracy 0.99878\n",
      "iteration 139 \t : \t loss 0.00221 - accuracy 0.99878\n",
      "iteration 140 \t : \t loss 0.00218 - accuracy 0.99878\n",
      "iteration 141 \t : \t loss 0.00216 - accuracy 0.99878\n",
      "iteration 142 \t : \t loss 0.00213 - accuracy 0.99878\n",
      "iteration 143 \t : \t loss 0.00210 - accuracy 0.99878\n",
      "iteration 144 \t : \t loss 0.00207 - accuracy 0.99878\n",
      "iteration 145 \t : \t loss 0.00205 - accuracy 0.99878\n",
      "iteration 146 \t : \t loss 0.00202 - accuracy 0.99878\n",
      "iteration 147 \t : \t loss 0.00200 - accuracy 0.99878\n",
      "iteration 148 \t : \t loss 0.00197 - accuracy 0.99878\n",
      "iteration 149 \t : \t loss 0.00195 - accuracy 0.99878\n",
      "iteration 150 \t : \t loss 0.00193 - accuracy 0.99878\n",
      "iteration 151 \t : \t loss 0.00190 - accuracy 0.99878\n",
      "iteration 152 \t : \t loss 0.00188 - accuracy 0.99878\n",
      "iteration 153 \t : \t loss 0.00186 - accuracy 0.99878\n",
      "iteration 154 \t : \t loss 0.00184 - accuracy 0.99878\n",
      "iteration 155 \t : \t loss 0.00182 - accuracy 0.99878\n",
      "iteration 156 \t : \t loss 0.00179 - accuracy 0.99878\n",
      "iteration 157 \t : \t loss 0.00177 - accuracy 0.99878\n",
      "iteration 158 \t : \t loss 0.00175 - accuracy 0.99878\n",
      "iteration 159 \t : \t loss 0.00173 - accuracy 0.99878\n",
      "iteration 160 \t : \t loss 0.00171 - accuracy 0.99878\n",
      "iteration 161 \t : \t loss 0.00169 - accuracy 0.99878\n",
      "iteration 162 \t : \t loss 0.00167 - accuracy 0.99902\n",
      "iteration 163 \t : \t loss 0.00166 - accuracy 0.99902\n",
      "iteration 164 \t : \t loss 0.00164 - accuracy 0.99902\n",
      "iteration 165 \t : \t loss 0.00162 - accuracy 0.99902\n",
      "iteration 166 \t : \t loss 0.00160 - accuracy 0.99902\n",
      "iteration 167 \t : \t loss 0.00158 - accuracy 0.99902\n",
      "iteration 168 \t : \t loss 0.00157 - accuracy 0.99902\n",
      "iteration 169 \t : \t loss 0.00155 - accuracy 0.99902\n",
      "iteration 170 \t : \t loss 0.00153 - accuracy 0.99902\n",
      "iteration 171 \t : \t loss 0.00152 - accuracy 0.99902\n",
      "iteration 172 \t : \t loss 0.00150 - accuracy 0.99902\n",
      "iteration 173 \t : \t loss 0.00149 - accuracy 0.99902\n",
      "iteration 174 \t : \t loss 0.00147 - accuracy 0.99927\n",
      "iteration 175 \t : \t loss 0.00145 - accuracy 0.99927\n",
      "iteration 176 \t : \t loss 0.00144 - accuracy 0.99927\n",
      "iteration 177 \t : \t loss 0.00142 - accuracy 0.99951\n",
      "iteration 178 \t : \t loss 0.00141 - accuracy 0.99951\n",
      "iteration 179 \t : \t loss 0.00140 - accuracy 0.99951\n",
      "iteration 180 \t : \t loss 0.00138 - accuracy 0.99951\n",
      "iteration 181 \t : \t loss 0.00137 - accuracy 0.99951\n",
      "iteration 182 \t : \t loss 0.00135 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00134 - accuracy 0.99976\n",
      "iteration 184 \t : \t loss 0.00133 - accuracy 0.99976\n",
      "iteration 185 \t : \t loss 0.00131 - accuracy 0.99976\n",
      "iteration 186 \t : \t loss 0.00130 - accuracy 0.99976\n",
      "iteration 187 \t : \t loss 0.00129 - accuracy 0.99976\n",
      "iteration 188 \t : \t loss 0.00128 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00123 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00118 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.23375 - accuracy 0.09961\n",
      "iteration 1 \t : \t loss 0.23320 - accuracy 0.10181\n",
      "iteration 2 \t : \t loss 0.23301 - accuracy 0.10181\n",
      "iteration 3 \t : \t loss 0.23282 - accuracy 0.10205\n",
      "iteration 4 \t : \t loss 0.23266 - accuracy 0.10156\n",
      "iteration 5 \t : \t loss 0.23250 - accuracy 0.09985\n",
      "iteration 6 \t : \t loss 0.23236 - accuracy 0.10181\n",
      "iteration 7 \t : \t loss 0.23223 - accuracy 0.10107\n",
      "iteration 8 \t : \t loss 0.23211 - accuracy 0.10474\n",
      "iteration 9 \t : \t loss 0.23199 - accuracy 0.10498\n",
      "iteration 10 \t : \t loss 0.23188 - accuracy 0.10498\n",
      "iteration 11 \t : \t loss 0.23178 - accuracy 0.10498\n",
      "iteration 12 \t : \t loss 0.23169 - accuracy 0.10815\n",
      "iteration 13 \t : \t loss 0.23160 - accuracy 0.10864\n",
      "iteration 14 \t : \t loss 0.23152 - accuracy 0.10767\n",
      "iteration 15 \t : \t loss 0.23144 - accuracy 0.10791\n",
      "iteration 16 \t : \t loss 0.23137 - accuracy 0.10815\n",
      "iteration 17 \t : \t loss 0.23130 - accuracy 0.10693\n",
      "iteration 18 \t : \t loss 0.23123 - accuracy 0.10693\n",
      "iteration 19 \t : \t loss 0.23117 - accuracy 0.10693\n",
      "iteration 20 \t : \t loss 0.23111 - accuracy 0.10791\n",
      "iteration 21 \t : \t loss 0.23105 - accuracy 0.10840\n",
      "iteration 22 \t : \t loss 0.23100 - accuracy 0.10669\n",
      "iteration 23 \t : \t loss 0.23095 - accuracy 0.10669\n",
      "iteration 24 \t : \t loss 0.23090 - accuracy 0.10938\n",
      "iteration 25 \t : \t loss 0.23086 - accuracy 0.10767\n",
      "iteration 26 \t : \t loss 0.23081 - accuracy 0.11157\n",
      "iteration 27 \t : \t loss 0.23077 - accuracy 0.11401\n",
      "iteration 28 \t : \t loss 0.23073 - accuracy 0.11328\n",
      "iteration 29 \t : \t loss 0.23069 - accuracy 0.11133\n",
      "iteration 30 \t : \t loss 0.23066 - accuracy 0.11133\n",
      "iteration 31 \t : \t loss 0.23062 - accuracy 0.11133\n",
      "iteration 32 \t : \t loss 0.23059 - accuracy 0.11353\n",
      "iteration 33 \t : \t loss 0.23056 - accuracy 0.11426\n",
      "iteration 34 \t : \t loss 0.23053 - accuracy 0.11426\n",
      "iteration 35 \t : \t loss 0.23050 - accuracy 0.11353\n",
      "iteration 36 \t : \t loss 0.23047 - accuracy 0.11572\n",
      "iteration 37 \t : \t loss 0.23045 - accuracy 0.11450\n",
      "iteration 38 \t : \t loss 0.23042 - accuracy 0.11304\n",
      "iteration 39 \t : \t loss 0.23040 - accuracy 0.11353\n",
      "iteration 40 \t : \t loss 0.23037 - accuracy 0.11597\n",
      "iteration 41 \t : \t loss 0.23035 - accuracy 0.11621\n",
      "iteration 42 \t : \t loss 0.23033 - accuracy 0.11548\n",
      "iteration 43 \t : \t loss 0.23031 - accuracy 0.11621\n",
      "iteration 44 \t : \t loss 0.23028 - accuracy 0.11792\n",
      "iteration 45 \t : \t loss 0.23026 - accuracy 0.11694\n",
      "iteration 46 \t : \t loss 0.23024 - accuracy 0.11646\n",
      "iteration 47 \t : \t loss 0.23022 - accuracy 0.11646\n",
      "iteration 48 \t : \t loss 0.23020 - accuracy 0.11768\n",
      "iteration 49 \t : \t loss 0.23018 - accuracy 0.11890\n",
      "iteration 50 \t : \t loss 0.23017 - accuracy 0.11841\n",
      "iteration 51 \t : \t loss 0.23015 - accuracy 0.11841\n",
      "iteration 52 \t : \t loss 0.23013 - accuracy 0.11768\n",
      "iteration 53 \t : \t loss 0.23011 - accuracy 0.11743\n",
      "iteration 54 \t : \t loss 0.23009 - accuracy 0.11768\n",
      "iteration 55 \t : \t loss 0.23007 - accuracy 0.11743\n",
      "iteration 56 \t : \t loss 0.23005 - accuracy 0.11768\n",
      "iteration 57 \t : \t loss 0.23003 - accuracy 0.11865\n",
      "iteration 58 \t : \t loss 0.23001 - accuracy 0.11938\n",
      "iteration 59 \t : \t loss 0.22999 - accuracy 0.12012\n",
      "iteration 60 \t : \t loss 0.22997 - accuracy 0.12036\n",
      "iteration 61 \t : \t loss 0.22995 - accuracy 0.12134\n",
      "iteration 62 \t : \t loss 0.22993 - accuracy 0.12207\n",
      "iteration 63 \t : \t loss 0.22991 - accuracy 0.12329\n",
      "iteration 64 \t : \t loss 0.22988 - accuracy 0.12500\n",
      "iteration 65 \t : \t loss 0.22986 - accuracy 0.12451\n",
      "iteration 66 \t : \t loss 0.22983 - accuracy 0.12476\n",
      "iteration 67 \t : \t loss 0.22980 - accuracy 0.12451\n",
      "iteration 68 \t : \t loss 0.22977 - accuracy 0.12500\n",
      "iteration 69 \t : \t loss 0.22973 - accuracy 0.12598\n",
      "iteration 70 \t : \t loss 0.22969 - accuracy 0.12695\n",
      "iteration 71 \t : \t loss 0.22965 - accuracy 0.12817\n",
      "iteration 72 \t : \t loss 0.22959 - accuracy 0.12964\n",
      "iteration 73 \t : \t loss 0.22954 - accuracy 0.13330\n",
      "iteration 74 \t : \t loss 0.22947 - accuracy 0.13867\n",
      "iteration 75 \t : \t loss 0.22939 - accuracy 0.14087\n",
      "iteration 76 \t : \t loss 0.22930 - accuracy 0.14404\n",
      "iteration 77 \t : \t loss 0.22918 - accuracy 0.14966\n",
      "iteration 78 \t : \t loss 0.22904 - accuracy 0.15625\n",
      "iteration 79 \t : \t loss 0.22886 - accuracy 0.16406\n",
      "iteration 80 \t : \t loss 0.22862 - accuracy 0.17261\n",
      "iteration 81 \t : \t loss 0.22830 - accuracy 0.18384\n",
      "iteration 82 \t : \t loss 0.22785 - accuracy 0.19385\n",
      "iteration 83 \t : \t loss 0.22718 - accuracy 0.20850\n",
      "iteration 84 \t : \t loss 0.22614 - accuracy 0.22021\n",
      "iteration 85 \t : \t loss 0.22439 - accuracy 0.22705\n",
      "iteration 86 \t : \t loss 0.22120 - accuracy 0.23584\n",
      "iteration 87 \t : \t loss 0.21515 - accuracy 0.24365\n",
      "iteration 88 \t : \t loss 0.20531 - accuracy 0.25732\n",
      "iteration 89 \t : \t loss 0.19443 - accuracy 0.27563\n",
      "iteration 90 \t : \t loss 0.18566 - accuracy 0.29419\n",
      "iteration 91 \t : \t loss 0.17932 - accuracy 0.29810\n",
      "iteration 92 \t : \t loss 0.17475 - accuracy 0.29688\n",
      "iteration 93 \t : \t loss 0.17127 - accuracy 0.30444\n",
      "iteration 94 \t : \t loss 0.16842 - accuracy 0.31567\n",
      "iteration 95 \t : \t loss 0.16599 - accuracy 0.32520\n",
      "iteration 96 \t : \t loss 0.16386 - accuracy 0.33716\n",
      "iteration 97 \t : \t loss 0.16201 - accuracy 0.34180\n",
      "iteration 98 \t : \t loss 0.16042 - accuracy 0.34839\n",
      "iteration 99 \t : \t loss 0.15911 - accuracy 0.35083\n",
      "iteration 100 \t : \t loss 0.15806 - accuracy 0.36108\n",
      "iteration 101 \t : \t loss 0.15704 - accuracy 0.36792\n",
      "iteration 102 \t : \t loss 0.15582 - accuracy 0.37329\n",
      "iteration 103 \t : \t loss 0.15446 - accuracy 0.37720\n",
      "iteration 104 \t : \t loss 0.15306 - accuracy 0.38062\n",
      "iteration 105 \t : \t loss 0.15167 - accuracy 0.39062\n",
      "iteration 106 \t : \t loss 0.15029 - accuracy 0.39771\n",
      "iteration 107 \t : \t loss 0.14893 - accuracy 0.40698\n",
      "iteration 108 \t : \t loss 0.14759 - accuracy 0.41357\n",
      "iteration 109 \t : \t loss 0.14626 - accuracy 0.41943\n",
      "iteration 110 \t : \t loss 0.14497 - accuracy 0.42651\n",
      "iteration 111 \t : \t loss 0.14370 - accuracy 0.43066\n",
      "iteration 112 \t : \t loss 0.14248 - accuracy 0.43652\n",
      "iteration 113 \t : \t loss 0.14133 - accuracy 0.44165\n",
      "iteration 114 \t : \t loss 0.14024 - accuracy 0.44653\n",
      "iteration 115 \t : \t loss 0.13906 - accuracy 0.45068\n",
      "iteration 116 \t : \t loss 0.13771 - accuracy 0.45923\n",
      "iteration 117 \t : \t loss 0.13643 - accuracy 0.46240\n",
      "iteration 118 \t : \t loss 0.13526 - accuracy 0.46680\n",
      "iteration 119 \t : \t loss 0.13409 - accuracy 0.47095\n",
      "iteration 120 \t : \t loss 0.13290 - accuracy 0.47607\n",
      "iteration 121 \t : \t loss 0.13171 - accuracy 0.48193\n",
      "iteration 122 \t : \t loss 0.13053 - accuracy 0.48755\n",
      "iteration 123 \t : \t loss 0.12935 - accuracy 0.49170\n",
      "iteration 124 \t : \t loss 0.12818 - accuracy 0.49731\n",
      "iteration 125 \t : \t loss 0.12700 - accuracy 0.50220\n",
      "iteration 126 \t : \t loss 0.12583 - accuracy 0.50903\n",
      "iteration 127 \t : \t loss 0.12466 - accuracy 0.51294\n",
      "iteration 128 \t : \t loss 0.12349 - accuracy 0.51978\n",
      "iteration 129 \t : \t loss 0.12228 - accuracy 0.52588\n",
      "iteration 130 \t : \t loss 0.12097 - accuracy 0.53589\n",
      "iteration 131 \t : \t loss 0.11918 - accuracy 0.54175\n",
      "iteration 132 \t : \t loss 0.11916 - accuracy 0.54736\n",
      "iteration 133 \t : \t loss 0.12378 - accuracy 0.51562\n",
      "iteration 134 \t : \t loss 0.11455 - accuracy 0.57178\n",
      "iteration 135 \t : \t loss 0.11673 - accuracy 0.55347\n",
      "iteration 136 \t : \t loss 0.11383 - accuracy 0.56763\n",
      "iteration 137 \t : \t loss 0.11360 - accuracy 0.57202\n",
      "iteration 138 \t : \t loss 0.11525 - accuracy 0.55713\n",
      "iteration 139 \t : \t loss 0.11300 - accuracy 0.57104\n",
      "iteration 140 \t : \t loss 0.10947 - accuracy 0.58618\n",
      "iteration 141 \t : \t loss 0.11168 - accuracy 0.57617\n",
      "iteration 142 \t : \t loss 0.10738 - accuracy 0.59814\n",
      "iteration 143 \t : \t loss 0.11028 - accuracy 0.57983\n",
      "iteration 144 \t : \t loss 0.10684 - accuracy 0.60400\n",
      "iteration 145 \t : \t loss 0.10556 - accuracy 0.60254\n",
      "iteration 146 \t : \t loss 0.10051 - accuracy 0.62939\n",
      "iteration 147 \t : \t loss 0.10819 - accuracy 0.59619\n",
      "iteration 148 \t : \t loss 0.10066 - accuracy 0.62012\n",
      "iteration 149 \t : \t loss 0.10329 - accuracy 0.59717\n",
      "iteration 150 \t : \t loss 0.10090 - accuracy 0.61963\n",
      "iteration 151 \t : \t loss 0.09860 - accuracy 0.62866\n",
      "iteration 152 \t : \t loss 0.10006 - accuracy 0.61353\n",
      "iteration 153 \t : \t loss 0.09346 - accuracy 0.64722\n",
      "iteration 154 \t : \t loss 0.09751 - accuracy 0.61523\n",
      "iteration 155 \t : \t loss 0.09773 - accuracy 0.62793\n",
      "iteration 156 \t : \t loss 0.09429 - accuracy 0.64746\n",
      "iteration 157 \t : \t loss 0.09146 - accuracy 0.65210\n",
      "iteration 158 \t : \t loss 0.09280 - accuracy 0.63843\n",
      "iteration 159 \t : \t loss 0.09282 - accuracy 0.64526\n",
      "iteration 160 \t : \t loss 0.08753 - accuracy 0.67334\n",
      "iteration 161 \t : \t loss 0.08929 - accuracy 0.65601\n",
      "iteration 162 \t : \t loss 0.08836 - accuracy 0.66528\n",
      "iteration 163 \t : \t loss 0.08573 - accuracy 0.67773\n",
      "iteration 164 \t : \t loss 0.08517 - accuracy 0.67358\n",
      "iteration 165 \t : \t loss 0.08188 - accuracy 0.68823\n",
      "iteration 166 \t : \t loss 0.08480 - accuracy 0.68604\n",
      "iteration 167 \t : \t loss 0.08202 - accuracy 0.69385\n",
      "iteration 168 \t : \t loss 0.07695 - accuracy 0.70947\n",
      "iteration 169 \t : \t loss 0.08045 - accuracy 0.69727\n",
      "iteration 170 \t : \t loss 0.07954 - accuracy 0.70386\n",
      "iteration 171 \t : \t loss 0.07755 - accuracy 0.71094\n",
      "iteration 172 \t : \t loss 0.07070 - accuracy 0.73804\n",
      "iteration 173 \t : \t loss 0.07017 - accuracy 0.73804\n",
      "iteration 174 \t : \t loss 0.07136 - accuracy 0.72437\n",
      "iteration 175 \t : \t loss 0.06518 - accuracy 0.76685\n",
      "iteration 176 \t : \t loss 0.07103 - accuracy 0.73682\n",
      "iteration 177 \t : \t loss 0.06394 - accuracy 0.77271\n",
      "iteration 178 \t : \t loss 0.06056 - accuracy 0.78882\n",
      "iteration 179 \t : \t loss 0.05913 - accuracy 0.78979\n",
      "iteration 180 \t : \t loss 0.05857 - accuracy 0.78882\n",
      "iteration 181 \t : \t loss 0.05616 - accuracy 0.79956\n",
      "iteration 182 \t : \t loss 0.05298 - accuracy 0.81079\n",
      "iteration 183 \t : \t loss 0.06302 - accuracy 0.77222\n",
      "iteration 184 \t : \t loss 0.05507 - accuracy 0.80347\n",
      "iteration 185 \t : \t loss 0.04932 - accuracy 0.83203\n",
      "iteration 186 \t : \t loss 0.04704 - accuracy 0.84131\n",
      "iteration 187 \t : \t loss 0.04755 - accuracy 0.83252\n",
      "iteration 188 \t : \t loss 0.05499 - accuracy 0.80518\n",
      "iteration 189 \t : \t loss 0.04334 - accuracy 0.85645\n",
      "iteration 190 \t : \t loss 0.04216 - accuracy 0.86304\n",
      "iteration 191 \t : \t loss 0.05100 - accuracy 0.83179\n",
      "iteration 192 \t : \t loss 0.04049 - accuracy 0.87598\n",
      "iteration 193 \t : \t loss 0.03945 - accuracy 0.87329\n",
      "iteration 194 \t : \t loss 0.04750 - accuracy 0.84326\n",
      "iteration 195 \t : \t loss 0.04563 - accuracy 0.84888\n",
      "iteration 196 \t : \t loss 0.03478 - accuracy 0.89844\n",
      "iteration 197 \t : \t loss 0.03306 - accuracy 0.90747\n",
      "iteration 198 \t : \t loss 0.04347 - accuracy 0.86230\n",
      "iteration 199 \t : \t loss 0.03151 - accuracy 0.91089\n",
      "iteration 0 \t : \t loss 0.11934 - accuracy 0.70679\n",
      "iteration 1 \t : \t loss 0.05481 - accuracy 0.88794\n",
      "iteration 2 \t : \t loss 0.04198 - accuracy 0.90552\n",
      "iteration 3 \t : \t loss 0.03610 - accuracy 0.91748\n",
      "iteration 4 \t : \t loss 0.03252 - accuracy 0.92383\n",
      "iteration 5 \t : \t loss 0.03002 - accuracy 0.92798\n",
      "iteration 6 \t : \t loss 0.02811 - accuracy 0.93335\n",
      "iteration 7 \t : \t loss 0.02658 - accuracy 0.93604\n",
      "iteration 8 \t : \t loss 0.02530 - accuracy 0.93726\n",
      "iteration 9 \t : \t loss 0.02420 - accuracy 0.94019\n",
      "iteration 10 \t : \t loss 0.02324 - accuracy 0.94263\n",
      "iteration 11 \t : \t loss 0.02237 - accuracy 0.94580\n",
      "iteration 12 \t : \t loss 0.02159 - accuracy 0.94775\n",
      "iteration 13 \t : \t loss 0.02088 - accuracy 0.94922\n",
      "iteration 14 \t : \t loss 0.02022 - accuracy 0.94995\n",
      "iteration 15 \t : \t loss 0.01960 - accuracy 0.95166\n",
      "iteration 16 \t : \t loss 0.01903 - accuracy 0.95264\n",
      "iteration 17 \t : \t loss 0.01849 - accuracy 0.95337\n",
      "iteration 18 \t : \t loss 0.01798 - accuracy 0.95483\n",
      "iteration 19 \t : \t loss 0.01749 - accuracy 0.95508\n",
      "iteration 20 \t : \t loss 0.01703 - accuracy 0.95605\n",
      "iteration 21 \t : \t loss 0.01659 - accuracy 0.95728\n",
      "iteration 22 \t : \t loss 0.01617 - accuracy 0.95850\n",
      "iteration 23 \t : \t loss 0.01576 - accuracy 0.95923\n",
      "iteration 24 \t : \t loss 0.01538 - accuracy 0.96069\n",
      "iteration 25 \t : \t loss 0.01500 - accuracy 0.96167\n",
      "iteration 26 \t : \t loss 0.01464 - accuracy 0.96191\n",
      "iteration 27 \t : \t loss 0.01429 - accuracy 0.96362\n",
      "iteration 28 \t : \t loss 0.01396 - accuracy 0.96582\n",
      "iteration 29 \t : \t loss 0.01364 - accuracy 0.96680\n",
      "iteration 30 \t : \t loss 0.01332 - accuracy 0.96680\n",
      "iteration 31 \t : \t loss 0.01302 - accuracy 0.96729\n",
      "iteration 32 \t : \t loss 0.01272 - accuracy 0.96899\n",
      "iteration 33 \t : \t loss 0.01244 - accuracy 0.96924\n",
      "iteration 34 \t : \t loss 0.01216 - accuracy 0.97070\n",
      "iteration 35 \t : \t loss 0.01189 - accuracy 0.97095\n",
      "iteration 36 \t : \t loss 0.01163 - accuracy 0.97119\n",
      "iteration 37 \t : \t loss 0.01137 - accuracy 0.97192\n",
      "iteration 38 \t : \t loss 0.01112 - accuracy 0.97217\n",
      "iteration 39 \t : \t loss 0.01088 - accuracy 0.97266\n",
      "iteration 40 \t : \t loss 0.01065 - accuracy 0.97339\n",
      "iteration 41 \t : \t loss 0.01042 - accuracy 0.97339\n",
      "iteration 42 \t : \t loss 0.01019 - accuracy 0.97412\n",
      "iteration 43 \t : \t loss 0.00998 - accuracy 0.97485\n",
      "iteration 44 \t : \t loss 0.00976 - accuracy 0.97632\n",
      "iteration 45 \t : \t loss 0.00955 - accuracy 0.97705\n",
      "iteration 46 \t : \t loss 0.00935 - accuracy 0.97827\n",
      "iteration 47 \t : \t loss 0.00915 - accuracy 0.97827\n",
      "iteration 48 \t : \t loss 0.00896 - accuracy 0.97925\n",
      "iteration 49 \t : \t loss 0.00877 - accuracy 0.97998\n",
      "iteration 50 \t : \t loss 0.00859 - accuracy 0.97998\n",
      "iteration 51 \t : \t loss 0.00841 - accuracy 0.98096\n",
      "iteration 52 \t : \t loss 0.00823 - accuracy 0.98120\n",
      "iteration 53 \t : \t loss 0.00806 - accuracy 0.98169\n",
      "iteration 54 \t : \t loss 0.00789 - accuracy 0.98193\n",
      "iteration 55 \t : \t loss 0.00773 - accuracy 0.98315\n",
      "iteration 56 \t : \t loss 0.00757 - accuracy 0.98364\n",
      "iteration 57 \t : \t loss 0.00741 - accuracy 0.98389\n",
      "iteration 58 \t : \t loss 0.00726 - accuracy 0.98413\n",
      "iteration 59 \t : \t loss 0.00711 - accuracy 0.98438\n",
      "iteration 60 \t : \t loss 0.00696 - accuracy 0.98438\n",
      "iteration 61 \t : \t loss 0.00682 - accuracy 0.98486\n",
      "iteration 62 \t : \t loss 0.00668 - accuracy 0.98511\n",
      "iteration 63 \t : \t loss 0.00655 - accuracy 0.98535\n",
      "iteration 64 \t : \t loss 0.00641 - accuracy 0.98560\n",
      "iteration 65 \t : \t loss 0.00628 - accuracy 0.98657\n",
      "iteration 66 \t : \t loss 0.00616 - accuracy 0.98682\n",
      "iteration 67 \t : \t loss 0.00603 - accuracy 0.98755\n",
      "iteration 68 \t : \t loss 0.00591 - accuracy 0.98755\n",
      "iteration 69 \t : \t loss 0.00579 - accuracy 0.98779\n",
      "iteration 70 \t : \t loss 0.00568 - accuracy 0.98804\n",
      "iteration 71 \t : \t loss 0.00557 - accuracy 0.98877\n",
      "iteration 72 \t : \t loss 0.00545 - accuracy 0.98877\n",
      "iteration 73 \t : \t loss 0.00535 - accuracy 0.98877\n",
      "iteration 74 \t : \t loss 0.00524 - accuracy 0.98901\n",
      "iteration 75 \t : \t loss 0.00514 - accuracy 0.98926\n",
      "iteration 76 \t : \t loss 0.00504 - accuracy 0.98975\n",
      "iteration 77 \t : \t loss 0.00494 - accuracy 0.98999\n",
      "iteration 78 \t : \t loss 0.00484 - accuracy 0.99023\n",
      "iteration 79 \t : \t loss 0.00475 - accuracy 0.99023\n",
      "iteration 80 \t : \t loss 0.00465 - accuracy 0.99048\n",
      "iteration 81 \t : \t loss 0.00456 - accuracy 0.99121\n",
      "iteration 82 \t : \t loss 0.00448 - accuracy 0.99194\n",
      "iteration 83 \t : \t loss 0.00439 - accuracy 0.99243\n",
      "iteration 84 \t : \t loss 0.00431 - accuracy 0.99292\n",
      "iteration 85 \t : \t loss 0.00422 - accuracy 0.99292\n",
      "iteration 86 \t : \t loss 0.00414 - accuracy 0.99316\n",
      "iteration 87 \t : \t loss 0.00407 - accuracy 0.99316\n",
      "iteration 88 \t : \t loss 0.00399 - accuracy 0.99341\n",
      "iteration 89 \t : \t loss 0.00391 - accuracy 0.99341\n",
      "iteration 90 \t : \t loss 0.00384 - accuracy 0.99341\n",
      "iteration 91 \t : \t loss 0.00377 - accuracy 0.99365\n",
      "iteration 92 \t : \t loss 0.00370 - accuracy 0.99390\n",
      "iteration 93 \t : \t loss 0.00363 - accuracy 0.99438\n",
      "iteration 94 \t : \t loss 0.00356 - accuracy 0.99438\n",
      "iteration 95 \t : \t loss 0.00350 - accuracy 0.99463\n",
      "iteration 96 \t : \t loss 0.00343 - accuracy 0.99463\n",
      "iteration 97 \t : \t loss 0.00337 - accuracy 0.99463\n",
      "iteration 98 \t : \t loss 0.00331 - accuracy 0.99463\n",
      "iteration 99 \t : \t loss 0.00325 - accuracy 0.99487\n",
      "iteration 100 \t : \t loss 0.00319 - accuracy 0.99487\n",
      "iteration 101 \t : \t loss 0.00314 - accuracy 0.99536\n",
      "iteration 102 \t : \t loss 0.00308 - accuracy 0.99585\n",
      "iteration 103 \t : \t loss 0.00302 - accuracy 0.99585\n",
      "iteration 104 \t : \t loss 0.00297 - accuracy 0.99585\n",
      "iteration 105 \t : \t loss 0.00292 - accuracy 0.99609\n",
      "iteration 106 \t : \t loss 0.00287 - accuracy 0.99609\n",
      "iteration 107 \t : \t loss 0.00282 - accuracy 0.99609\n",
      "iteration 108 \t : \t loss 0.00277 - accuracy 0.99609\n",
      "iteration 109 \t : \t loss 0.00272 - accuracy 0.99658\n",
      "iteration 110 \t : \t loss 0.00267 - accuracy 0.99658\n",
      "iteration 111 \t : \t loss 0.00263 - accuracy 0.99658\n",
      "iteration 112 \t : \t loss 0.00258 - accuracy 0.99683\n",
      "iteration 113 \t : \t loss 0.00254 - accuracy 0.99707\n",
      "iteration 114 \t : \t loss 0.00250 - accuracy 0.99707\n",
      "iteration 115 \t : \t loss 0.00245 - accuracy 0.99707\n",
      "iteration 116 \t : \t loss 0.00241 - accuracy 0.99707\n",
      "iteration 117 \t : \t loss 0.00237 - accuracy 0.99731\n",
      "iteration 118 \t : \t loss 0.00233 - accuracy 0.99731\n",
      "iteration 119 \t : \t loss 0.00230 - accuracy 0.99731\n",
      "iteration 120 \t : \t loss 0.00226 - accuracy 0.99780\n",
      "iteration 121 \t : \t loss 0.00222 - accuracy 0.99780\n",
      "iteration 122 \t : \t loss 0.00219 - accuracy 0.99780\n",
      "iteration 123 \t : \t loss 0.00215 - accuracy 0.99805\n",
      "iteration 124 \t : \t loss 0.00212 - accuracy 0.99805\n",
      "iteration 125 \t : \t loss 0.00208 - accuracy 0.99829\n",
      "iteration 126 \t : \t loss 0.00205 - accuracy 0.99829\n",
      "iteration 127 \t : \t loss 0.00202 - accuracy 0.99854\n",
      "iteration 128 \t : \t loss 0.00199 - accuracy 0.99854\n",
      "iteration 129 \t : \t loss 0.00196 - accuracy 0.99854\n",
      "iteration 130 \t : \t loss 0.00193 - accuracy 0.99854\n",
      "iteration 131 \t : \t loss 0.00190 - accuracy 0.99854\n",
      "iteration 132 \t : \t loss 0.00187 - accuracy 0.99854\n",
      "iteration 133 \t : \t loss 0.00184 - accuracy 0.99854\n",
      "iteration 134 \t : \t loss 0.00181 - accuracy 0.99854\n",
      "iteration 135 \t : \t loss 0.00178 - accuracy 0.99854\n",
      "iteration 136 \t : \t loss 0.00176 - accuracy 0.99854\n",
      "iteration 137 \t : \t loss 0.00173 - accuracy 0.99854\n",
      "iteration 138 \t : \t loss 0.00171 - accuracy 0.99854\n",
      "iteration 139 \t : \t loss 0.00168 - accuracy 0.99878\n",
      "iteration 140 \t : \t loss 0.00166 - accuracy 0.99902\n",
      "iteration 141 \t : \t loss 0.00163 - accuracy 0.99902\n",
      "iteration 142 \t : \t loss 0.00161 - accuracy 0.99927\n",
      "iteration 143 \t : \t loss 0.00159 - accuracy 0.99927\n",
      "iteration 144 \t : \t loss 0.00157 - accuracy 0.99927\n",
      "iteration 145 \t : \t loss 0.00155 - accuracy 0.99927\n",
      "iteration 146 \t : \t loss 0.00152 - accuracy 0.99927\n",
      "iteration 147 \t : \t loss 0.00150 - accuracy 0.99951\n",
      "iteration 148 \t : \t loss 0.00148 - accuracy 0.99951\n",
      "iteration 149 \t : \t loss 0.00146 - accuracy 0.99951\n",
      "iteration 150 \t : \t loss 0.00144 - accuracy 0.99951\n",
      "iteration 151 \t : \t loss 0.00142 - accuracy 0.99951\n",
      "iteration 152 \t : \t loss 0.00141 - accuracy 0.99951\n",
      "iteration 153 \t : \t loss 0.00139 - accuracy 0.99951\n",
      "iteration 154 \t : \t loss 0.00137 - accuracy 0.99951\n",
      "iteration 155 \t : \t loss 0.00135 - accuracy 0.99951\n",
      "iteration 156 \t : \t loss 0.00133 - accuracy 0.99951\n",
      "iteration 157 \t : \t loss 0.00132 - accuracy 0.99951\n",
      "iteration 158 \t : \t loss 0.00130 - accuracy 0.99951\n",
      "iteration 159 \t : \t loss 0.00129 - accuracy 0.99951\n",
      "iteration 160 \t : \t loss 0.00127 - accuracy 0.99951\n",
      "iteration 161 \t : \t loss 0.00125 - accuracy 0.99951\n",
      "iteration 162 \t : \t loss 0.00124 - accuracy 0.99951\n",
      "iteration 163 \t : \t loss 0.00122 - accuracy 0.99951\n",
      "iteration 164 \t : \t loss 0.00121 - accuracy 0.99951\n",
      "iteration 165 \t : \t loss 0.00119 - accuracy 0.99951\n",
      "iteration 166 \t : \t loss 0.00118 - accuracy 0.99951\n",
      "iteration 167 \t : \t loss 0.00117 - accuracy 0.99951\n",
      "iteration 168 \t : \t loss 0.00115 - accuracy 0.99951\n",
      "iteration 169 \t : \t loss 0.00114 - accuracy 0.99951\n",
      "iteration 170 \t : \t loss 0.00113 - accuracy 0.99951\n",
      "iteration 171 \t : \t loss 0.00111 - accuracy 0.99951\n",
      "iteration 172 \t : \t loss 0.00110 - accuracy 0.99951\n",
      "iteration 173 \t : \t loss 0.00109 - accuracy 0.99951\n",
      "iteration 174 \t : \t loss 0.00108 - accuracy 0.99951\n",
      "iteration 175 \t : \t loss 0.00106 - accuracy 0.99951\n",
      "iteration 176 \t : \t loss 0.00105 - accuracy 0.99951\n",
      "iteration 177 \t : \t loss 0.00104 - accuracy 0.99951\n",
      "iteration 178 \t : \t loss 0.00103 - accuracy 0.99951\n",
      "iteration 179 \t : \t loss 0.00102 - accuracy 0.99976\n",
      "iteration 180 \t : \t loss 0.00101 - accuracy 0.99976\n",
      "iteration 181 \t : \t loss 0.00100 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00099 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00097 - accuracy 0.99976\n",
      "iteration 184 \t : \t loss 0.00096 - accuracy 0.99976\n",
      "iteration 185 \t : \t loss 0.00095 - accuracy 0.99976\n",
      "iteration 186 \t : \t loss 0.00094 - accuracy 0.99976\n",
      "iteration 187 \t : \t loss 0.00093 - accuracy 0.99976\n",
      "iteration 188 \t : \t loss 0.00093 - accuracy 0.99976\n",
      "iteration 189 \t : \t loss 0.00092 - accuracy 0.99976\n",
      "iteration 190 \t : \t loss 0.00091 - accuracy 0.99976\n",
      "iteration 191 \t : \t loss 0.00090 - accuracy 0.99976\n",
      "iteration 192 \t : \t loss 0.00089 - accuracy 0.99976\n",
      "iteration 193 \t : \t loss 0.00088 - accuracy 0.99976\n",
      "iteration 194 \t : \t loss 0.00087 - accuracy 0.99976\n",
      "iteration 195 \t : \t loss 0.00086 - accuracy 0.99976\n",
      "iteration 196 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00084 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.23305 - accuracy 0.10596\n",
      "iteration 1 \t : \t loss 0.23245 - accuracy 0.10742\n",
      "iteration 2 \t : \t loss 0.23232 - accuracy 0.10815\n",
      "iteration 3 \t : \t loss 0.23219 - accuracy 0.10815\n",
      "iteration 4 \t : \t loss 0.23208 - accuracy 0.10840\n",
      "iteration 5 \t : \t loss 0.23197 - accuracy 0.10864\n",
      "iteration 6 \t : \t loss 0.23187 - accuracy 0.11011\n",
      "iteration 7 \t : \t loss 0.23178 - accuracy 0.11035\n",
      "iteration 8 \t : \t loss 0.23169 - accuracy 0.11157\n",
      "iteration 9 \t : \t loss 0.23161 - accuracy 0.11206\n",
      "iteration 10 \t : \t loss 0.23153 - accuracy 0.11206\n",
      "iteration 11 \t : \t loss 0.23146 - accuracy 0.11206\n",
      "iteration 12 \t : \t loss 0.23139 - accuracy 0.11206\n",
      "iteration 13 \t : \t loss 0.23133 - accuracy 0.11304\n",
      "iteration 14 \t : \t loss 0.23127 - accuracy 0.11304\n",
      "iteration 15 \t : \t loss 0.23121 - accuracy 0.11255\n",
      "iteration 16 \t : \t loss 0.23116 - accuracy 0.11255\n",
      "iteration 17 \t : \t loss 0.23111 - accuracy 0.11255\n",
      "iteration 18 \t : \t loss 0.23106 - accuracy 0.11255\n",
      "iteration 19 \t : \t loss 0.23102 - accuracy 0.11255\n",
      "iteration 20 \t : \t loss 0.23098 - accuracy 0.11206\n",
      "iteration 21 \t : \t loss 0.23094 - accuracy 0.11206\n",
      "iteration 22 \t : \t loss 0.23090 - accuracy 0.11230\n",
      "iteration 23 \t : \t loss 0.23086 - accuracy 0.11230\n",
      "iteration 24 \t : \t loss 0.23083 - accuracy 0.11304\n",
      "iteration 25 \t : \t loss 0.23080 - accuracy 0.11426\n",
      "iteration 26 \t : \t loss 0.23077 - accuracy 0.11279\n",
      "iteration 27 \t : \t loss 0.23074 - accuracy 0.11279\n",
      "iteration 28 \t : \t loss 0.23071 - accuracy 0.11279\n",
      "iteration 29 \t : \t loss 0.23069 - accuracy 0.11279\n",
      "iteration 30 \t : \t loss 0.23066 - accuracy 0.11401\n",
      "iteration 31 \t : \t loss 0.23064 - accuracy 0.11401\n",
      "iteration 32 \t : \t loss 0.23062 - accuracy 0.11401\n",
      "iteration 33 \t : \t loss 0.23060 - accuracy 0.11621\n",
      "iteration 34 \t : \t loss 0.23058 - accuracy 0.11621\n",
      "iteration 35 \t : \t loss 0.23056 - accuracy 0.11621\n",
      "iteration 36 \t : \t loss 0.23054 - accuracy 0.11621\n",
      "iteration 37 \t : \t loss 0.23053 - accuracy 0.11621\n",
      "iteration 38 \t : \t loss 0.23051 - accuracy 0.11670\n",
      "iteration 39 \t : \t loss 0.23049 - accuracy 0.11572\n",
      "iteration 40 \t : \t loss 0.23048 - accuracy 0.11572\n",
      "iteration 41 \t : \t loss 0.23047 - accuracy 0.11572\n",
      "iteration 42 \t : \t loss 0.23045 - accuracy 0.11621\n",
      "iteration 43 \t : \t loss 0.23044 - accuracy 0.11475\n",
      "iteration 44 \t : \t loss 0.23043 - accuracy 0.11475\n",
      "iteration 45 \t : \t loss 0.23042 - accuracy 0.11523\n",
      "iteration 46 \t : \t loss 0.23041 - accuracy 0.11499\n",
      "iteration 47 \t : \t loss 0.23040 - accuracy 0.11499\n",
      "iteration 48 \t : \t loss 0.23039 - accuracy 0.11450\n",
      "iteration 49 \t : \t loss 0.23038 - accuracy 0.11450\n",
      "iteration 50 \t : \t loss 0.23037 - accuracy 0.11450\n",
      "iteration 51 \t : \t loss 0.23036 - accuracy 0.11475\n",
      "iteration 52 \t : \t loss 0.23036 - accuracy 0.11475\n",
      "iteration 53 \t : \t loss 0.23035 - accuracy 0.11475\n",
      "iteration 54 \t : \t loss 0.23034 - accuracy 0.11377\n",
      "iteration 55 \t : \t loss 0.23034 - accuracy 0.11279\n",
      "iteration 56 \t : \t loss 0.23033 - accuracy 0.11328\n",
      "iteration 57 \t : \t loss 0.23032 - accuracy 0.11328\n",
      "iteration 58 \t : \t loss 0.23032 - accuracy 0.11328\n",
      "iteration 59 \t : \t loss 0.23031 - accuracy 0.11523\n",
      "iteration 60 \t : \t loss 0.23031 - accuracy 0.11499\n",
      "iteration 61 \t : \t loss 0.23030 - accuracy 0.11572\n",
      "iteration 62 \t : \t loss 0.23030 - accuracy 0.11523\n",
      "iteration 63 \t : \t loss 0.23029 - accuracy 0.11523\n",
      "iteration 64 \t : \t loss 0.23029 - accuracy 0.11523\n",
      "iteration 65 \t : \t loss 0.23028 - accuracy 0.11621\n",
      "iteration 66 \t : \t loss 0.23028 - accuracy 0.11572\n",
      "iteration 67 \t : \t loss 0.23028 - accuracy 0.11572\n",
      "iteration 68 \t : \t loss 0.23027 - accuracy 0.11523\n",
      "iteration 69 \t : \t loss 0.23027 - accuracy 0.11401\n",
      "iteration 70 \t : \t loss 0.23027 - accuracy 0.11353\n",
      "iteration 71 \t : \t loss 0.23026 - accuracy 0.11328\n",
      "iteration 72 \t : \t loss 0.23026 - accuracy 0.11328\n",
      "iteration 73 \t : \t loss 0.23026 - accuracy 0.11401\n",
      "iteration 74 \t : \t loss 0.23025 - accuracy 0.11475\n",
      "iteration 75 \t : \t loss 0.23025 - accuracy 0.11450\n",
      "iteration 76 \t : \t loss 0.23025 - accuracy 0.11450\n",
      "iteration 77 \t : \t loss 0.23024 - accuracy 0.11377\n",
      "iteration 78 \t : \t loss 0.23024 - accuracy 0.11377\n",
      "iteration 79 \t : \t loss 0.23024 - accuracy 0.11377\n",
      "iteration 80 \t : \t loss 0.23024 - accuracy 0.11426\n",
      "iteration 81 \t : \t loss 0.23023 - accuracy 0.11572\n",
      "iteration 82 \t : \t loss 0.23023 - accuracy 0.11572\n",
      "iteration 83 \t : \t loss 0.23023 - accuracy 0.11572\n",
      "iteration 84 \t : \t loss 0.23023 - accuracy 0.11743\n",
      "iteration 85 \t : \t loss 0.23023 - accuracy 0.11743\n",
      "iteration 86 \t : \t loss 0.23022 - accuracy 0.11743\n",
      "iteration 87 \t : \t loss 0.23022 - accuracy 0.11743\n",
      "iteration 88 \t : \t loss 0.23022 - accuracy 0.11743\n",
      "iteration 89 \t : \t loss 0.23022 - accuracy 0.11768\n",
      "iteration 90 \t : \t loss 0.23021 - accuracy 0.11719\n",
      "iteration 91 \t : \t loss 0.23021 - accuracy 0.11816\n",
      "iteration 92 \t : \t loss 0.23021 - accuracy 0.11816\n",
      "iteration 93 \t : \t loss 0.23021 - accuracy 0.11816\n",
      "iteration 94 \t : \t loss 0.23021 - accuracy 0.11816\n",
      "iteration 95 \t : \t loss 0.23021 - accuracy 0.11816\n",
      "iteration 96 \t : \t loss 0.23020 - accuracy 0.11743\n",
      "iteration 97 \t : \t loss 0.23020 - accuracy 0.11743\n",
      "iteration 98 \t : \t loss 0.23020 - accuracy 0.11743\n",
      "iteration 99 \t : \t loss 0.23020 - accuracy 0.11743\n",
      "iteration 100 \t : \t loss 0.23020 - accuracy 0.11743\n",
      "iteration 101 \t : \t loss 0.23020 - accuracy 0.11743\n",
      "iteration 102 \t : \t loss 0.23019 - accuracy 0.11743\n",
      "iteration 103 \t : \t loss 0.23019 - accuracy 0.11841\n",
      "iteration 104 \t : \t loss 0.23019 - accuracy 0.11841\n",
      "iteration 105 \t : \t loss 0.23019 - accuracy 0.11841\n",
      "iteration 106 \t : \t loss 0.23019 - accuracy 0.11841\n",
      "iteration 107 \t : \t loss 0.23019 - accuracy 0.11841\n",
      "iteration 108 \t : \t loss 0.23018 - accuracy 0.11841\n",
      "iteration 109 \t : \t loss 0.23018 - accuracy 0.11841\n",
      "iteration 110 \t : \t loss 0.23018 - accuracy 0.11841\n",
      "iteration 111 \t : \t loss 0.23018 - accuracy 0.11841\n",
      "iteration 112 \t : \t loss 0.23018 - accuracy 0.11841\n",
      "iteration 113 \t : \t loss 0.23018 - accuracy 0.11694\n",
      "iteration 114 \t : \t loss 0.23018 - accuracy 0.11694\n",
      "iteration 115 \t : \t loss 0.23017 - accuracy 0.11694\n",
      "iteration 116 \t : \t loss 0.23017 - accuracy 0.11694\n",
      "iteration 117 \t : \t loss 0.23017 - accuracy 0.11694\n",
      "iteration 118 \t : \t loss 0.23017 - accuracy 0.11694\n",
      "iteration 119 \t : \t loss 0.23017 - accuracy 0.11694\n",
      "iteration 120 \t : \t loss 0.23017 - accuracy 0.11499\n",
      "iteration 121 \t : \t loss 0.23017 - accuracy 0.11499\n",
      "iteration 122 \t : \t loss 0.23017 - accuracy 0.11572\n",
      "iteration 123 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 124 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 125 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 126 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 127 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 128 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 129 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 130 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 131 \t : \t loss 0.23016 - accuracy 0.11572\n",
      "iteration 132 \t : \t loss 0.23015 - accuracy 0.11572\n",
      "iteration 133 \t : \t loss 0.23015 - accuracy 0.11572\n",
      "iteration 134 \t : \t loss 0.23015 - accuracy 0.11572\n",
      "iteration 135 \t : \t loss 0.23015 - accuracy 0.11621\n",
      "iteration 136 \t : \t loss 0.23015 - accuracy 0.11621\n",
      "iteration 137 \t : \t loss 0.23015 - accuracy 0.11621\n",
      "iteration 138 \t : \t loss 0.23015 - accuracy 0.11621\n",
      "iteration 139 \t : \t loss 0.23015 - accuracy 0.11621\n",
      "iteration 140 \t : \t loss 0.23015 - accuracy 0.11621\n",
      "iteration 141 \t : \t loss 0.23014 - accuracy 0.11621\n",
      "iteration 142 \t : \t loss 0.23014 - accuracy 0.11621\n",
      "iteration 143 \t : \t loss 0.23014 - accuracy 0.11621\n",
      "iteration 144 \t : \t loss 0.23014 - accuracy 0.11621\n",
      "iteration 145 \t : \t loss 0.23014 - accuracy 0.11621\n",
      "iteration 146 \t : \t loss 0.23014 - accuracy 0.11621\n",
      "iteration 147 \t : \t loss 0.23014 - accuracy 0.11621\n",
      "iteration 148 \t : \t loss 0.23014 - accuracy 0.11670\n",
      "iteration 149 \t : \t loss 0.23014 - accuracy 0.11670\n",
      "iteration 150 \t : \t loss 0.23014 - accuracy 0.11670\n",
      "iteration 151 \t : \t loss 0.23014 - accuracy 0.11670\n",
      "iteration 152 \t : \t loss 0.23013 - accuracy 0.11670\n",
      "iteration 153 \t : \t loss 0.23013 - accuracy 0.11670\n",
      "iteration 154 \t : \t loss 0.23013 - accuracy 0.11670\n",
      "iteration 155 \t : \t loss 0.23013 - accuracy 0.11670\n",
      "iteration 156 \t : \t loss 0.23013 - accuracy 0.11670\n",
      "iteration 157 \t : \t loss 0.23013 - accuracy 0.11670\n",
      "iteration 158 \t : \t loss 0.23013 - accuracy 0.11670\n",
      "iteration 159 \t : \t loss 0.23013 - accuracy 0.11841\n",
      "iteration 160 \t : \t loss 0.23013 - accuracy 0.11841\n",
      "iteration 161 \t : \t loss 0.23013 - accuracy 0.11841\n",
      "iteration 162 \t : \t loss 0.23013 - accuracy 0.11841\n",
      "iteration 163 \t : \t loss 0.23013 - accuracy 0.11841\n",
      "iteration 164 \t : \t loss 0.23013 - accuracy 0.11841\n",
      "iteration 165 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 166 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 167 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 168 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 169 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 170 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 171 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 172 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 173 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 174 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 175 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 176 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 177 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 178 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 179 \t : \t loss 0.23012 - accuracy 0.11841\n",
      "iteration 180 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 181 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 182 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 183 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 184 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 185 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 186 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 187 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 188 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 189 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 190 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 191 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 192 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 193 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 194 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 195 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 196 \t : \t loss 0.23011 - accuracy 0.11841\n",
      "iteration 197 \t : \t loss 0.23010 - accuracy 0.11841\n",
      "iteration 198 \t : \t loss 0.23010 - accuracy 0.11841\n",
      "iteration 199 \t : \t loss 0.23010 - accuracy 0.11841\n",
      "iteration 0 \t : \t loss 0.11543 - accuracy 0.72217\n",
      "iteration 1 \t : \t loss 0.05238 - accuracy 0.89624\n",
      "iteration 2 \t : \t loss 0.03979 - accuracy 0.90820\n",
      "iteration 3 \t : \t loss 0.03398 - accuracy 0.91553\n",
      "iteration 4 \t : \t loss 0.03045 - accuracy 0.92090\n",
      "iteration 5 \t : \t loss 0.02797 - accuracy 0.92603\n",
      "iteration 6 \t : \t loss 0.02608 - accuracy 0.93042\n",
      "iteration 7 \t : \t loss 0.02455 - accuracy 0.93359\n",
      "iteration 8 \t : \t loss 0.02326 - accuracy 0.93555\n",
      "iteration 9 \t : \t loss 0.02214 - accuracy 0.93945\n",
      "iteration 10 \t : \t loss 0.02115 - accuracy 0.94141\n",
      "iteration 11 \t : \t loss 0.02025 - accuracy 0.94385\n",
      "iteration 12 \t : \t loss 0.01944 - accuracy 0.94458\n",
      "iteration 13 \t : \t loss 0.01869 - accuracy 0.94629\n",
      "iteration 14 \t : \t loss 0.01800 - accuracy 0.94922\n",
      "iteration 15 \t : \t loss 0.01736 - accuracy 0.95093\n",
      "iteration 16 \t : \t loss 0.01676 - accuracy 0.95337\n",
      "iteration 17 \t : \t loss 0.01619 - accuracy 0.95435\n",
      "iteration 18 \t : \t loss 0.01567 - accuracy 0.95581\n",
      "iteration 19 \t : \t loss 0.01517 - accuracy 0.95654\n",
      "iteration 20 \t : \t loss 0.01470 - accuracy 0.95898\n",
      "iteration 21 \t : \t loss 0.01425 - accuracy 0.96069\n",
      "iteration 22 \t : \t loss 0.01382 - accuracy 0.96240\n",
      "iteration 23 \t : \t loss 0.01342 - accuracy 0.96436\n",
      "iteration 24 \t : \t loss 0.01303 - accuracy 0.96533\n",
      "iteration 25 \t : \t loss 0.01266 - accuracy 0.96655\n",
      "iteration 26 \t : \t loss 0.01230 - accuracy 0.96729\n",
      "iteration 27 \t : \t loss 0.01196 - accuracy 0.96777\n",
      "iteration 28 \t : \t loss 0.01163 - accuracy 0.96875\n",
      "iteration 29 \t : \t loss 0.01131 - accuracy 0.96948\n",
      "iteration 30 \t : \t loss 0.01101 - accuracy 0.97021\n",
      "iteration 31 \t : \t loss 0.01071 - accuracy 0.97144\n",
      "iteration 32 \t : \t loss 0.01043 - accuracy 0.97266\n",
      "iteration 33 \t : \t loss 0.01016 - accuracy 0.97363\n",
      "iteration 34 \t : \t loss 0.00989 - accuracy 0.97363\n",
      "iteration 35 \t : \t loss 0.00964 - accuracy 0.97461\n",
      "iteration 36 \t : \t loss 0.00939 - accuracy 0.97559\n",
      "iteration 37 \t : \t loss 0.00916 - accuracy 0.97681\n",
      "iteration 38 \t : \t loss 0.00893 - accuracy 0.97852\n",
      "iteration 39 \t : \t loss 0.00870 - accuracy 0.97974\n",
      "iteration 40 \t : \t loss 0.00849 - accuracy 0.98022\n",
      "iteration 41 \t : \t loss 0.00828 - accuracy 0.98022\n",
      "iteration 42 \t : \t loss 0.00808 - accuracy 0.98120\n",
      "iteration 43 \t : \t loss 0.00788 - accuracy 0.98242\n",
      "iteration 44 \t : \t loss 0.00770 - accuracy 0.98340\n",
      "iteration 45 \t : \t loss 0.00751 - accuracy 0.98438\n",
      "iteration 46 \t : \t loss 0.00734 - accuracy 0.98486\n",
      "iteration 47 \t : \t loss 0.00717 - accuracy 0.98511\n",
      "iteration 48 \t : \t loss 0.00700 - accuracy 0.98535\n",
      "iteration 49 \t : \t loss 0.00684 - accuracy 0.98584\n",
      "iteration 50 \t : \t loss 0.00668 - accuracy 0.98657\n",
      "iteration 51 \t : \t loss 0.00653 - accuracy 0.98706\n",
      "iteration 52 \t : \t loss 0.00638 - accuracy 0.98755\n",
      "iteration 53 \t : \t loss 0.00624 - accuracy 0.98779\n",
      "iteration 54 \t : \t loss 0.00610 - accuracy 0.98804\n",
      "iteration 55 \t : \t loss 0.00597 - accuracy 0.98828\n",
      "iteration 56 \t : \t loss 0.00584 - accuracy 0.98853\n",
      "iteration 57 \t : \t loss 0.00571 - accuracy 0.98901\n",
      "iteration 58 \t : \t loss 0.00558 - accuracy 0.98950\n",
      "iteration 59 \t : \t loss 0.00546 - accuracy 0.98975\n",
      "iteration 60 \t : \t loss 0.00535 - accuracy 0.98975\n",
      "iteration 61 \t : \t loss 0.00523 - accuracy 0.98999\n",
      "iteration 62 \t : \t loss 0.00512 - accuracy 0.99023\n",
      "iteration 63 \t : \t loss 0.00501 - accuracy 0.99023\n",
      "iteration 64 \t : \t loss 0.00490 - accuracy 0.99072\n",
      "iteration 65 \t : \t loss 0.00480 - accuracy 0.99097\n",
      "iteration 66 \t : \t loss 0.00470 - accuracy 0.99146\n",
      "iteration 67 \t : \t loss 0.00460 - accuracy 0.99146\n",
      "iteration 68 \t : \t loss 0.00451 - accuracy 0.99170\n",
      "iteration 69 \t : \t loss 0.00441 - accuracy 0.99170\n",
      "iteration 70 \t : \t loss 0.00432 - accuracy 0.99170\n",
      "iteration 71 \t : \t loss 0.00424 - accuracy 0.99219\n",
      "iteration 72 \t : \t loss 0.00415 - accuracy 0.99243\n",
      "iteration 73 \t : \t loss 0.00406 - accuracy 0.99268\n",
      "iteration 74 \t : \t loss 0.00398 - accuracy 0.99292\n",
      "iteration 75 \t : \t loss 0.00390 - accuracy 0.99292\n",
      "iteration 76 \t : \t loss 0.00382 - accuracy 0.99341\n",
      "iteration 77 \t : \t loss 0.00375 - accuracy 0.99365\n",
      "iteration 78 \t : \t loss 0.00367 - accuracy 0.99365\n",
      "iteration 79 \t : \t loss 0.00360 - accuracy 0.99365\n",
      "iteration 80 \t : \t loss 0.00353 - accuracy 0.99390\n",
      "iteration 81 \t : \t loss 0.00346 - accuracy 0.99414\n",
      "iteration 82 \t : \t loss 0.00340 - accuracy 0.99438\n",
      "iteration 83 \t : \t loss 0.00333 - accuracy 0.99438\n",
      "iteration 84 \t : \t loss 0.00327 - accuracy 0.99438\n",
      "iteration 85 \t : \t loss 0.00320 - accuracy 0.99463\n",
      "iteration 86 \t : \t loss 0.00314 - accuracy 0.99463\n",
      "iteration 87 \t : \t loss 0.00308 - accuracy 0.99463\n",
      "iteration 88 \t : \t loss 0.00303 - accuracy 0.99463\n",
      "iteration 89 \t : \t loss 0.00297 - accuracy 0.99487\n",
      "iteration 90 \t : \t loss 0.00292 - accuracy 0.99512\n",
      "iteration 91 \t : \t loss 0.00286 - accuracy 0.99561\n",
      "iteration 92 \t : \t loss 0.00281 - accuracy 0.99634\n",
      "iteration 93 \t : \t loss 0.00276 - accuracy 0.99707\n",
      "iteration 94 \t : \t loss 0.00271 - accuracy 0.99707\n",
      "iteration 95 \t : \t loss 0.00266 - accuracy 0.99707\n",
      "iteration 96 \t : \t loss 0.00262 - accuracy 0.99731\n",
      "iteration 97 \t : \t loss 0.00257 - accuracy 0.99756\n",
      "iteration 98 \t : \t loss 0.00253 - accuracy 0.99756\n",
      "iteration 99 \t : \t loss 0.00248 - accuracy 0.99756\n",
      "iteration 100 \t : \t loss 0.00244 - accuracy 0.99756\n",
      "iteration 101 \t : \t loss 0.00240 - accuracy 0.99756\n",
      "iteration 102 \t : \t loss 0.00236 - accuracy 0.99756\n",
      "iteration 103 \t : \t loss 0.00232 - accuracy 0.99780\n",
      "iteration 104 \t : \t loss 0.00228 - accuracy 0.99780\n",
      "iteration 105 \t : \t loss 0.00224 - accuracy 0.99780\n",
      "iteration 106 \t : \t loss 0.00221 - accuracy 0.99780\n",
      "iteration 107 \t : \t loss 0.00217 - accuracy 0.99780\n",
      "iteration 108 \t : \t loss 0.00213 - accuracy 0.99780\n",
      "iteration 109 \t : \t loss 0.00210 - accuracy 0.99780\n",
      "iteration 110 \t : \t loss 0.00207 - accuracy 0.99780\n",
      "iteration 111 \t : \t loss 0.00203 - accuracy 0.99780\n",
      "iteration 112 \t : \t loss 0.00200 - accuracy 0.99780\n",
      "iteration 113 \t : \t loss 0.00197 - accuracy 0.99805\n",
      "iteration 114 \t : \t loss 0.00194 - accuracy 0.99805\n",
      "iteration 115 \t : \t loss 0.00191 - accuracy 0.99805\n",
      "iteration 116 \t : \t loss 0.00188 - accuracy 0.99805\n",
      "iteration 117 \t : \t loss 0.00185 - accuracy 0.99805\n",
      "iteration 118 \t : \t loss 0.00183 - accuracy 0.99805\n",
      "iteration 119 \t : \t loss 0.00180 - accuracy 0.99829\n",
      "iteration 120 \t : \t loss 0.00177 - accuracy 0.99829\n",
      "iteration 121 \t : \t loss 0.00175 - accuracy 0.99854\n",
      "iteration 122 \t : \t loss 0.00172 - accuracy 0.99854\n",
      "iteration 123 \t : \t loss 0.00170 - accuracy 0.99854\n",
      "iteration 124 \t : \t loss 0.00167 - accuracy 0.99854\n",
      "iteration 125 \t : \t loss 0.00165 - accuracy 0.99854\n",
      "iteration 126 \t : \t loss 0.00163 - accuracy 0.99854\n",
      "iteration 127 \t : \t loss 0.00160 - accuracy 0.99854\n",
      "iteration 128 \t : \t loss 0.00158 - accuracy 0.99854\n",
      "iteration 129 \t : \t loss 0.00156 - accuracy 0.99878\n",
      "iteration 130 \t : \t loss 0.00154 - accuracy 0.99878\n",
      "iteration 131 \t : \t loss 0.00152 - accuracy 0.99878\n",
      "iteration 132 \t : \t loss 0.00150 - accuracy 0.99878\n",
      "iteration 133 \t : \t loss 0.00148 - accuracy 0.99878\n",
      "iteration 134 \t : \t loss 0.00146 - accuracy 0.99878\n",
      "iteration 135 \t : \t loss 0.00144 - accuracy 0.99878\n",
      "iteration 136 \t : \t loss 0.00142 - accuracy 0.99878\n",
      "iteration 137 \t : \t loss 0.00140 - accuracy 0.99878\n",
      "iteration 138 \t : \t loss 0.00138 - accuracy 0.99878\n",
      "iteration 139 \t : \t loss 0.00137 - accuracy 0.99878\n",
      "iteration 140 \t : \t loss 0.00135 - accuracy 0.99878\n",
      "iteration 141 \t : \t loss 0.00133 - accuracy 0.99878\n",
      "iteration 142 \t : \t loss 0.00132 - accuracy 0.99878\n",
      "iteration 143 \t : \t loss 0.00130 - accuracy 0.99878\n",
      "iteration 144 \t : \t loss 0.00128 - accuracy 0.99878\n",
      "iteration 145 \t : \t loss 0.00127 - accuracy 0.99878\n",
      "iteration 146 \t : \t loss 0.00125 - accuracy 0.99878\n",
      "iteration 147 \t : \t loss 0.00124 - accuracy 0.99878\n",
      "iteration 148 \t : \t loss 0.00122 - accuracy 0.99878\n",
      "iteration 149 \t : \t loss 0.00121 - accuracy 0.99878\n",
      "iteration 150 \t : \t loss 0.00119 - accuracy 0.99878\n",
      "iteration 151 \t : \t loss 0.00118 - accuracy 0.99878\n",
      "iteration 152 \t : \t loss 0.00117 - accuracy 0.99878\n",
      "iteration 153 \t : \t loss 0.00115 - accuracy 0.99878\n",
      "iteration 154 \t : \t loss 0.00114 - accuracy 0.99878\n",
      "iteration 155 \t : \t loss 0.00113 - accuracy 0.99902\n",
      "iteration 156 \t : \t loss 0.00112 - accuracy 0.99902\n",
      "iteration 157 \t : \t loss 0.00110 - accuracy 0.99902\n",
      "iteration 158 \t : \t loss 0.00109 - accuracy 0.99902\n",
      "iteration 159 \t : \t loss 0.00108 - accuracy 0.99902\n",
      "iteration 160 \t : \t loss 0.00107 - accuracy 0.99902\n",
      "iteration 161 \t : \t loss 0.00106 - accuracy 0.99902\n",
      "iteration 162 \t : \t loss 0.00104 - accuracy 0.99902\n",
      "iteration 163 \t : \t loss 0.00103 - accuracy 0.99902\n",
      "iteration 164 \t : \t loss 0.00102 - accuracy 0.99902\n",
      "iteration 165 \t : \t loss 0.00101 - accuracy 0.99902\n",
      "iteration 166 \t : \t loss 0.00100 - accuracy 0.99902\n",
      "iteration 167 \t : \t loss 0.00099 - accuracy 0.99902\n",
      "iteration 168 \t : \t loss 0.00098 - accuracy 0.99927\n",
      "iteration 169 \t : \t loss 0.00097 - accuracy 0.99927\n",
      "iteration 170 \t : \t loss 0.00096 - accuracy 0.99927\n",
      "iteration 171 \t : \t loss 0.00095 - accuracy 0.99927\n",
      "iteration 172 \t : \t loss 0.00094 - accuracy 0.99927\n",
      "iteration 173 \t : \t loss 0.00093 - accuracy 0.99927\n",
      "iteration 174 \t : \t loss 0.00092 - accuracy 0.99927\n",
      "iteration 175 \t : \t loss 0.00091 - accuracy 0.99927\n",
      "iteration 176 \t : \t loss 0.00091 - accuracy 0.99927\n",
      "iteration 177 \t : \t loss 0.00090 - accuracy 0.99927\n",
      "iteration 178 \t : \t loss 0.00089 - accuracy 0.99951\n",
      "iteration 179 \t : \t loss 0.00088 - accuracy 0.99951\n",
      "iteration 180 \t : \t loss 0.00087 - accuracy 0.99951\n",
      "iteration 181 \t : \t loss 0.00086 - accuracy 0.99951\n",
      "iteration 182 \t : \t loss 0.00085 - accuracy 0.99951\n",
      "iteration 183 \t : \t loss 0.00085 - accuracy 0.99951\n",
      "iteration 184 \t : \t loss 0.00084 - accuracy 0.99951\n",
      "iteration 185 \t : \t loss 0.00083 - accuracy 0.99951\n",
      "iteration 186 \t : \t loss 0.00082 - accuracy 0.99951\n",
      "iteration 187 \t : \t loss 0.00082 - accuracy 0.99951\n",
      "iteration 188 \t : \t loss 0.00081 - accuracy 0.99951\n",
      "iteration 189 \t : \t loss 0.00080 - accuracy 0.99951\n",
      "iteration 190 \t : \t loss 0.00079 - accuracy 0.99951\n",
      "iteration 191 \t : \t loss 0.00079 - accuracy 0.99951\n",
      "iteration 192 \t : \t loss 0.00078 - accuracy 0.99951\n",
      "iteration 193 \t : \t loss 0.00077 - accuracy 0.99951\n",
      "iteration 194 \t : \t loss 0.00077 - accuracy 0.99951\n",
      "iteration 195 \t : \t loss 0.00076 - accuracy 0.99951\n",
      "iteration 196 \t : \t loss 0.00075 - accuracy 0.99951\n",
      "iteration 197 \t : \t loss 0.00075 - accuracy 0.99951\n",
      "iteration 198 \t : \t loss 0.00074 - accuracy 0.99951\n",
      "iteration 199 \t : \t loss 0.00073 - accuracy 0.99951\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_samples = 60000\n",
    "q = 200\n",
    "n_layers_l = [2, 3, 5, 7]\n",
    "\n",
    "error_rates_1, error_rates_2 = [], []\n",
    "\n",
    "for n_layers in n_layers_l:\n",
    "    # Initialization\n",
    "    DNN_trained = init_DBN(p, q, n_layers)\n",
    "    DNN_pre_trained = copy.deepcopy(DNN_trained)\n",
    "    \n",
    "    # Training\n",
    "    DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train=False, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "    DNN_pre_trained = principal_mnist(n_samples, DNN_pre_trained, pre_train=True, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    error_rates_1.append(test_DNN(DNN_trained, test_image, test_label))\n",
    "    error_rates_2.append(test_DNN(DNN_pre_trained, test_image, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_NAmcLXD7ww3",
    "outputId": "1b59972f-bae8-4c41-a9fb-65d222eff60f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09299999999999997, 0.09609999999999996, 0.1896, 0.8865]\n",
      "[0.05249999999999999, 0.05449999999999999, 0.054200000000000026, 0.04949999999999999]\n"
     ]
    }
   ],
   "source": [
    "print(error_rates_1)\n",
    "print(error_rates_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "id": "IKt7C0NkJ47C",
    "outputId": "d6ee5979-4bd9-43eb-f2c7-21b2cfa014bb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAGNCAYAAAC7R71WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeZyVc//H8denmpaptGjRPm4kKXS3CEUk1Y3SJuFOiuiXXbLfynpbQvcdkS2iJK2IhOKm7pRdId1UWpQtRUVT398f32umM2fOzJyZZuY6M+f9fDzmceZc6+dc5zpnPvP9fq/PZc45RERERCR8ZcIOQEREREQ8JWYiIiIiCUKJmYiIiEiCUGImIiIikiCUmImIiIgkCCVmIiIiIglCiZmIJAQzG2RmzswGRU1fbWarw4lKCqI0vmdm1sbM5pvZj8F5+nEey3cKlhtVTCFKKVEu7AAkPGYWTxG7E51zC4s6lpIg4w+Ncy4t3EgkbMEf21vQ5yMpmNl+wCtARWAS8CPwfahBSamlxEwARucyb3VxBSGSg85hByBJrx1QB7jROXdn2MFI6abETHDOjQo7BpGcOOf+F3YMkvTqB48bQo1CkoLGmEnczGxUMGaik5mdbWZLzOy3jC6+vOYHy9Qzs4eCMSh/mtkPZjbDzFrH2F/mmCMz62ZmC83s13i6YM1sYrDuX8zsUjP71Mx2mNnCYH55M7vEzOaa2Roz+8PMfjazN8yse9S2OgX7bAI0Cbab8TMxatlmwb6/C17fJjObbGaH5uM4xx1b1HoNzexfZvZ18Fp/NrP3zezmfVy2tZlNN7PNQSxrzOxhM6uX3+MeLHOwmU0zs1/M7HczW2Rmp+byurKNV4o6N04Mzo1tZrbVzF4xs8Ny2FbT4LVk2bflML4tp3jw3ZgACyLPh6jl4j7X89ifC15fLTObYGYbg/dhuZmdH2P5XF9LxvaipkV+dgeY2Qdmtt3MNpjZ/WZWIVjupCCWrcExnGRm++cSezUzG2dm681sp5mtMLPLzMxyWP5oM3vRzL4Pjtl3ZvaomdWPsezCIObyZvYPM/sqOC4Tcz2ge9fvbGavBef+H2a20sz+aWbVIpZJC97Xp4NJT0W834Pi2U+M/bY2s7Fm9kmw753B53CMmdWIWvaiYF+35LCtA8xsl5l9FjW9nJn9n5n9N3ivtpvZR+a/V8pELZsW7GNi8PmYav6zvsfMOgXL/CU491bZ3u+Lz8zskdzefykYtZhJQVwNdAFeAhYA1eKZb2YHAu/i//t8C5gCNAL6AaeaWR/n3Msx9tcX6Aa8CjyCT5DiNRboiB8fMhfYHUyvGcxbBMwHfgDqAacDc83sQufc48Gyq/HdvVcEzx+M2H7mAGAz6wbMAFKC174KaAj0Dl7fic65D+OIOT+xZey7DTAvWPedII5UoDkwCritgMueBkwHDHgRWAO0BoYBPc2sg3Pu2xivIeZxN7NDgMXA/vj382PgYGBW8Dy/TgN6svfcaA78DWhrZs2dcz9GvJZm+GNaI4jrU+AvwMwgxng9CJwBnID/g706eoF9ONdzUh14D/gT/z5UCLb1pJntcc49ndvK+XAp0B3/fiwETgGuBGqa2WzgefyxmwAcC5wL1ArWiVYeeCOI/fngeR/8uXEoMDxyYTMbHGz3D2AO8B1wCHABcLqZtXfOrY2xn+lAW/w5MAvYnNeLNLOLgPHA78C0YJ1OwLXBvo5zzm0BtuA/+0fhz7PZ7P3M5zr4PxcXAr2At/HHpwz+M3UV0N3MjnbObQuWfQ64BxhiZrc753ZHbWsw/u/4oxGvLeP7pyvwFTAZ2AmcCPwbOBr4e4y4DgKWACuD/VYCtpr/B2wpsB/+czIdP9buwGA744CfCngsJBbnnH6S9Adwwc+oHH6ui1p+VLD870CrGNvLa/68YP6NUdOPBdLxH+4qEdMHBcvvAbrl87VNDNZdDxwYY34FoGGM6dWAz4GfgUpR81YDq3PYXw3gF/yg4OZR81oAvwEfxhl7vmLD/8H7Nni9Z8dYr2EBl60SvCe7gY5Ry10bbOP1fB7314P5l0dN7xlxPg7K67hHnBvpQOeoeXcF80ZGTX8zmD4sanr3nPady3uUca53ymF+vs71OD+njwNlI6Y3D7a1IodjE/O1BPMW5vB6fgUOizoXlwfnwE/ACRHzyuD/cXDAUTHeM4dPTitETK8J/C+Yd3zE9Kb4pHMV0CBqW52D/c+Mmr4w2M6nQK14jmWwXhN88rcVaBY17+FgmxPyc0xz2E+nYJ1RMfZfNsbyQ4Llr42aPi6YflrUdAO+wX/fVovxXv476nwpCzwRzOsZMT0t4hy7M0ZclxLjMxvMq0zU96R+9v0n9AD0E+Kbv/fDmNPPlqjlMz7wD+SwvRzn41uOHL7FJSXG/EnB/IER0zK+DGcW4LVNzOnLJI51r4r+wxFMX03OidnlwTrDc5j/QDC/eX7jySs2fCuEA2bHsX5+lj0nWHZyjHnl2JvgNY7nuEecA9/k8IdpIflPzJ6NsZ0Dg3kvRkxrFEz7GigTY535sfady7HJONc7Fca5nse+HP6P734x5r0dzI/1D03M10LuidltMZb/RzDvmRjzzgvmnRfjPXNEJfRR8T0V4/Nxag4xz8QnoVVjnC89Y62Ty/G8kZyTkBr4hG0HWRPKXI9pDvvpRIzELJflDZ8YvxU1/fBgOy9FTe8aTH8yYloZfAK9ESgXYx/V8f/ovhAxLS3YzveRrzlifkZiNjQ/x1k/Bf9RV6bgnIs53iMX7xdgfqvg8T/OuV0x5r+F7xZpBTyTz/3lNxYAzOxw4BrgeHxXYcWoRRrkYz/HBI9HWuy6RU2Dx8OAFXltLJ+xtQ8e4+kKzM+yfw0e34qe4ZxLN7N38F/qrYDoLqbczoF3XfYuGfB/aE+II65Iy2JM+y54jByvc1TwuNg5tyfGOu8CJ+dz3znZl3M9J18757bGmB75Wn/LV5SxxTqeGQPeP4gxb33w2DDGvHR813G0hcFjq4hpGZ+fE8ysbYx16uBbfJrGiCO/3w+5nde/mNlH+M9dM+CTfG47T0FX40XAWfhWz2pkHe+d5XvHObc8+Kx1N7NGzrmM93xo8PhIxOJN8a2SXwM35TCUbwf+eyjaJ865P2JMnwPcCTxkZl3xrcHv4VtqXY4vVApMiZkURF71e2LNzxiHtjGHdTKmVy/A/vIbC2bWHv/FXA7fxTUH/5/yHvaOJ6mQj/1kDIC9MI/lquS1oQLElnHM1pO3/Cxb2O9ZxvY25bC9grzPW6InBEkj+D/k8e47p+kFsS/HLSfZXmcgPXgsm8P8/Po1l33kNi8lxrwfc0jAM97nyLGpGZ+fa/KIL9bnJ7/nTVG8P/kxFT/G7Bv8mLXv8V2r4MexxvreeRifLF4A3GJmBwA9gI+dc5GJacZxPIS9F6jEEvdxdM6tMbN2+FbVbvgxswDfmdl9zrl/5bIfKQAlZlIQef2XFGt+xpf6ATmsUy9qufzsL7+xANyEH9yarUComV2PT37yIyPuI51zn+Zz3X2NLeOPdjwtfPlZtrDfs4zl6uawvZz2UxgyWpty2ndO0wtiX45bYchoEcz2/W5mRZVsxFLLzMrGSM4yjkvk68/4vVoOLYM5KkCrTeT7szzG/CJ7f4ILb3rhB/13d86lR8wrA4zMYdUZ+H8ehpjZrcQY9B8V80znXG/yJ8fj6Jz7AuhvZuWAI/Gty5cCY83sd+fcE/ncl+RC5TKkuHwUPHYIPtzRTgwe47lqsTAcDPwcnfgEcupO203OLRP/DR477mNckP/YMvadYymNAi6b8Z51ip4RvIcZrzXe9yzyHIh1HLPtpxBlXEF3THS5gECHfG4vI9mI9TrCPtd/CR4bxZjXpoj2GUs5/MUO0ToFjx9FTCvMz09ecjuvq+NbpXcCXxTBvg8OHudEJmWBdvh/yLIJusQfx/9DdTq+5ew3/NWTkb7E//PVPugyLVTOuXTn3AfOubuBAcHkMwp7P8lOiZkUC+fcOvwA6zT2lp0AfO0i4Gz8H5SZxRTSanwJgCOiYhmCH1Qby09AbTOL9eX5FP4L8Zag2T8LMyuTUROoCGJ7KVinh5kNiJ5pZg0LuOws/BWgA4Lu1UhX4AfZv+FilzDIJuIcOBC4JGq/Pcn/+LK4BTEuxP9hvChq393I//iyjPIAjWPsK+xzfRm+1exsM0uN2HdNfOmF4nSXBTXQImK4KXj6VMRy44BdwANm1pQo5muVFVbS9mywr0vN7OCoebfhy0I8m8N4q321OnjsFDnRzOoAD+Wx7gT8PwTj8J+hyW5vWQ3AJ074qzHrAf+K9V1lvr5e83gDDuquRZdEgr2tzNvj3ZbER12ZknHfv5zMcs4VtF5PtIvxg0bvNbNT8H9AMmo77QHOj/6iKUIP4pOcd83sBXwXQBt8y8mL+Npp0d7E10t6LRiM+wd+wOxLzrmfzKwv/o/tf83sTXw3icO/xmPw4z+iB/Hvc2zOuT/NrB++FMXkoEbTf4N9HYYvN1CuAMv+FtSWmga8bWbT8IP8W+PrW31PVJITh+H4OmYPBufAJ/hkqRc+aTw9n9vL777fAx42s7+xt45ZH/xYn57s7QbMy4Jg2bvMrAVBK5Vz7vZgfmjnunNuo5k9h68x9bGZvYJPNv6Gr1vXKrf1C9FG/Hipz81sDn4cWl980vCwc+6diJi/DM61J4HlZvYavp5WCj757Yiv59dsX4Nyzq02syvwidCHwWfsB/w/BsfgW52u3df95GAp/rzobWaL8Bed1MW3YH9FLncWcM6tDd7LHsGk6G7MDLfhuxsvxtdkews/prQOfuzZcfgrU/O8CCnwd+AiM3sXX+rkF3zNs9Px34EP5rKuFETYl4XqJ7wf8i6XkeXycPKu3ZTr/GCZBvjCjmvwdYt+xLfMtI2x7KDoGPLx2iYG66blssxp+KRkG76163X8ANuY+8XX7BkPrMMPenbAxKhl0vD/0X6N7w7Ziv+inwSckY/48xVbsE5j/CDhb4Nj+xO+YOQN+7hsW3zC+UOw7NrgONQv4HE/GJ9gbsGXgVgMnJrLcV9NzuUyYp4bxCgJEUxvhh+vE73vEcE6+XmPzsV3ke4I1nUFPdfj+Jxmey25HW98QnRvcK5m1Ae7Hp9051YuI9tnN49zrhOxa3WtDn6q4ROg9fg/4l8AlwGWw+tpGbymNcHyP+Nr9z0KnBS17MLoY57P43oK/nP1S7CvVfgWxer5OQa5bD+nY1MT/9lbjf+O+B/+qsfUWOd61LoZ9f6W5rFvwydUbwbH8M/gPXgXuAFoFLFsGjG+yyLmHx2cx58E29oRHKungBYFPf76yfnHggMvIpK0ghams/EFR78KOx6RWILejVuAC5wG3JdaSsxEJCkEg/7rOOe+j5reGV+b6Svn3OGhBCeSBzOrim+JT8G3eGlsVymlMWYikizK42svLcB3L6fjq6p3wXf1DM9lXZFQmNmp+KK4p+PHo41QUla6qcVMRJJCUKLjQeAkfKX6VPy4r3eAfzrnPspldZFQmNlE/K2vNuEvjrjJxb57hZQSSsxEREREEoTqmImIiIgkiFIxxqxWrVouLS0t7DBERERE8vTBBx/86JyrHWteqUjM0tLSWLZsWdhhiIiIiOTJzNbkNE9dmSIiIiIJQomZiIiISIJQYiYiIiKSIJSYiYiIiCQIJWYiIiIiCaJUXJUZj61bt7J582Z27doVdigi+yQlJYU6deqw3377hR2KiIgUsqRIzLZu3cqmTZto0KABlSpVwszCDkmkQJxz7Nixg/Xr1wMoORMRKWWSoitz8+bNNGjQgNTUVCVlUqKZGampqTRo0IDNmzeHHY6IiBSypEjMdu3aRaVKlcIOQ6TQVKpUSd3yIiKlUFIkZoBayqRU0fksIlI6JU1iJiIiIpLolJiJiIiIALNmwW+/hRuDErMSYtSoUZgZXbt2zTavb9++dOrUqfiDKiSbN29m1KhRrF69ulC326lTJ/r27Zvv9dLS0hgxYkShxiIiIontpZegd2+4665w40iKchmlyeuvv87SpUtp27Zt2KEUms2bNzN69Gg6depEWlpaoW334YcfJiUlJd/rzZw5k/3337/Q4hARkcS2fDmcfTb89a9w443hxqIWsxKkZs2atGzZkjvuuCPsULLYtWsXu3fvLpZ97dixI+5lmzdvziGHHJLvfbRq1YrGjRvnez0RESl5fvoJevSAKlV8V2ZqarjxKDErQcyMG2+8kTlz5vDZZ5/luuzHH39M586dSU1NpUaNGpxzzjls2rQp13UmTpyImbF06VI6duxIpUqVaNq0KTNnzsyyXEYX4YQJEzjooIOoWLEiGzZsAODxxx/n8MMPp0KFCjRp0oR77rkn132uXr2ali1bAnDiiSdiZplXHC5cuBAzY968efTo0YMqVapwySWXADBmzBjatm1LtWrVqFu3LqeffjqrVq2KGWeGUaNGUatWLT766CPat29PamoqrVq14j//+U+W9aK7MgcNGkSbNm2YP38+RxxxBJUrV6ZDhw4sX748y3q//PILZ511FpUrV6Z+/frcfffdjBgxolBbAUVEpPDs2gX9+sH69TBzJjRsGHZESsxKnH79+nHIIYfk2mr2ww8/0KlTJ7Zv387kyZP597//zdtvv02XLl34888/89xH//796dmzJzNmzKBly5b069ePTz75JMsy7733HuPHj+fuu+/mpZdeolq1atx7770MGzaMM844g5dffplhw4Zx8803M27cuBz3Va9ePZ577jkAHnroIRYvXszixYuzLDNkyBCOPPJI5syZw5AhQwBYt24dl1xyCbNnz+axxx5j9+7dHHvssfz666+5vrbt27dz3nnncdFFFzF9+nQqVKhA79692b59e67rrV27lmuuuYYbb7yRKVOmsHnzZvr3749zLnOZQYMGMX/+fMaOHcuECRN4/fXXmTp1aq7bFRGR8Fx5JSxYABMmQPv2YUfjJe0YsyuugI8/DmffRx0FDz5YsHXLlCnD9ddfz5AhQ7j11ltp2rRptmXGjBkDwLx58zJv2XPIIYfQvn17pk+fzoABA3LdxwUXXJDZYtS1a1eaN2/OXXfdxfPPP5+5zJYtW/j444+pW7cu4G97NXr0aG666SZuueUWALp06cL27du5/fbbGTZsGGXLls22rwoVKnDEEUcAvuuxfYxPRr9+/bjtttuyTHvggQcyf9+9ezddunShTp06zJ49m4EDB+b42nbs2MGDDz7ISSedBPjEsFWrVrzzzjt069Ytx/V+/vln3nvvvcyu0T179tCrVy+++uormjVrxueff86cOXN44YUX6NevHwCdO3emUaNGVKlSJcftiohIOB59FB56CEaMgFz+bBQ7tZiVQOeeey6NGzfmrhwuHXn//fc55ZRTstxH8eijjyYtLY133303z+336tUr8/cyZcrQs2dP3n///SzLtG7dOjMpA1i8eDG///47/fr1Iz09PfPnpJNOYtOmTaxbtw7nXJZ58Y5LO/XUU7NN++9//0uXLl3Yf//9KVeuHKmpqfz222+sXLky122VL18+yxWszZs3B3wLXG7S0tKyjFeLXm/ZsmUAnH766ZnLVKpUiZNPPjnX7YqISPF75x245BLo3h3++c+wo8kqaVvMCtpilQjKlSvHyJEjueyyyxg1alS2+Rs3buTwww/PNr1u3br8/PPPeW6/Tp062Z5v3Lgx27Yi/fjjjwAx9wvw3Xff8e2333LiiSdmTjvhhBNYuHBhnvFE72vt2rWccsoptGvXjkcffZT69etTvnx5Tj31VHbu3JnrtqpWrUqZMnv/HylfvjxAnutVr149y/Po9b7//nuqVq1KxYoVsyxXu3btXLcrIiLFa/Vq6NMHDjoIpkyBGJ05oUraxKykGzx4MLfffjt33313tnn16tWLeYPrTZs20bp16zy3vXnz5izlIjZv3ky9evWyLBN9S6CaNWsC8PLLL2dLpAAOPfRQAJYuXZo5rWrVqnnGEmtfr732Gtu3b2f27NlUrlwZgPT09LiSzqJywAEHsG3bNnbu3JklOfvhhx9Ci0lERLL67Td/BWZ6uq9bVq1a2BFlp8SshKpQoQIjRozg+uuvp3Xr1lnqdR199NGMHz+ebdu2ZSY/S5cuZfXq1XTo0CHPbc+cOZPDDjsM8GOpZs+eTbt27XJd55hjjqFSpUps2LAhZtdjhjZt2mSbFm+rVYYdO3ZQpkwZypXbe/q+8MILpKenx7V+Uch4XXPmzOHMM88EfJzz58+POwEVEZGis2cP/P3vvmbZa69BAaopFQslZiXYRRddxJ133smiRYs44YQTMqdfddVVjB8/nq5du3Lttdfy22+/cd1119GyZUv69OmT53Yff/xxypcvT4sWLXj88cdZtWoVU6ZMyXWd6tWrM2rUKC6//HLWrFnD8ccfz549e1i5ciULFizIVnIjUuPGjalUqRJPP/001apVIyUlJWYCl+Gkk05i9+7dnH/++QwZMoTly5dz3333ZetuLE4tWrTg9NNPZ9iwYWzbto0DDjiA+++/n9TU1CxdpyIiEo5Ro3ydsgcfhC5dwo4mZ/qLUYKlpqZy5ZVXZpteu3ZtFixYQMWKFRkwYADDhw+nY8eOzJ8/P7N1KjfPP/88M2fO5IwzzuCTTz5h6tSptGrVKs/1Ro4cyYQJE3j11Vfp2bMnAwYM4LnnnqNjx465rlexYkUee+wxPvjgA0444YQ872rQsmVLJk6cyJIlSzjttNOYPHky06ZNo1rIbdITJ07k5JNP5rLLLmPw4MGccMIJdOvWLctFGCIiUvxeeAFuuw2GDIHLLgs7mtxZZB2mkqpNmzYu46q4WL744ovMrjnJ2cSJEzn//PPZtm2bSjwUgvT0dFq0aMHRRx/N008/Xejb13ktIpK3Dz+EDh387ZbefBMqVAg7IjCzD5xzMbuG1JUpUkimTZvGhg0baNmyJVu3buWxxx7j66+/5plnngk7NBGRpLRpE/TsCbVqwfTpiZGU5UWJmUghqVy5Mk899RSrVq1i9+7dtGzZkpdeeinPCydERKTw/fEH9OoFP/8M770HMQoGJCQlZpJp0KBBDBo0KOwwSqy//e1v/O1vfws7DBGRpOccXHwxLF4M06b5O+6UFBr8LyIiIqXKgw/CxIlwyy3Qt2/Y0eSPEjMREREpNebN8/e/7N0b/vGPsKPJPyVmIiIiUiqsXAn9+0OLFvD001ASy0iWwJBFREREstqyBU4/HcqXhzlzoKRWfdLgfxERESnRdu+Gs86Cb7/1tcqaNAk7ooJTYiYiIiIl2siRfmzZY49BHjebSXjqyiwBnnjiCcyMdevWZZl+7bXXYmY8++yzWabPnz8fM2PRokWsXr0aM+Pll1/OnH/PPfewcOHCbPsxM8aNG1ckr6EovP/++4waNarQt1uQ4xDrOIuISNGbOBHuvx8uvRQuuCDsaPadErMS4NhjjwVg0aJFWaYvWrSI1NTUmNMrVKhA69atqVevHosXL6ZDhw6Z83NKzEqa999/n9GjRxf6dhcvXky/fv3ytU6s4ywiIkVr8WK46CLo3NknZ6WBErMSoFmzZtSsWTNLArZr1y6WLVvGwIEDYyZmrVu3pkKFClSoUIH27dtTvXr14g47Tzt27CiW/Tjn2LlzZ9zLt2/fnrr5LBGdyMdZRKQ0+u47X9m/USN/k/JypWRwlhKzEsDMOOaYY7IkYB999BEA//d//8fnn3/Otm3bANizZw9LlizhuOOOA7J3saWlpfHTTz8xevRozAwzy9J6tnv3bm644QZq165NnTp1GD58OH/88Ueu8Q0aNIg2bdowa9YsmjVrRsWKFenQoQMrVqzI9jruv/9+rrjiCmrXrk3Lli0B2LlzJyNHjqRRo0ZUqFCBI488krlz5+a6z4kTJ3LppZdmbtfM6NSpEwCjRo2iVq1avPvuu7Rt25aKFSsybdo0fv/9dy655BIOPfRQUlNTOfDAAxk+fDhbt27NFmdkV2anTp3o27cvkydP5uCDD2a//faje/fuWbqWY3VlpqWlMWLECB544AEaNmxIjRo1OOuss9iyZUuW/X366acce+yxVKxYkcMPP5y5c+fSpk0b3YVBRCQH27fDGWf4xzlzoGbNsCMqPErMSohjjz2Wjz/+OLOVafHixbRu3ZoWLVpQrVo1lixZAsDy5cv59ddfMxOzaDNnzqRatWoMGTKExYsXs3jxYv76179mzh8zZgwbNmzg2Wef5ZprruHRRx9l7Nixeca3Zs0arrrqKm6++WYmT57Mr7/+SteuXbO1VN17771s3LiRSZMm8a9//QuAvn37MnHiRG644QZeeukl2rZtS48ePfj4449z3N+pp57K1VdfnXksFi9ezMMPP5w5f/v27Zx33nlccMEFvPbaa7Rr147t27eze/du7rjjDl599VVuu+023nrrrbi6LZcsWcK4ceMYM2YMEyZM4MMPP2To0KF5rvfCCy/w5ptvMmHCBO6++25efvllbrjhhixxdu3alR07djBlyhRuuukmrrzyStauXZvntkVEkpFzMHgwfPQRTJkCzZuHHVHhKiUNfwVwxRWQyx/+InXUUf5+Eflw3HHHsWvXLpYuXcrxxx/PokWLOOaYYzAz2rdvz6JFizj55JMzW9UyxqVFa9WqFeXKlaNhw4a0b98+2/y0tDQmTpwIQNeuXXnvvfeYMWMGI0eOzDW+H3/8kdmzZ2fut3Xr1hx00EFMnDiRiy++OHO5evXqMXXq1Mznb775Jq+88goLFy7khBNOAOCUU05h5cqV3HHHHUybNi3m/mrXrk1aWhpAzNexY8cO7r//fnr27Jll+vjx4zN/T09P58ADD6RDhw6sXbuWxo0b5/j6tm7dyiuvvEKNGjUA+P7777nyyivZsWMHlSpVynG9lJQUZs2aRbmgjX3FihU8//zzmUnkU089xU8//cSyZcto0KABAAcddBBHH310jtsUEUlmd94JU6fC3XfDqaeGHU3hK/YWMzPrZmZfmdkqM7suxvzGZrbAzD4ys0/NTHeFBtq2bUu5cuUyE6+MxAzITMwyph9yyCHUrl27QPs55ZRTsjxv3rx5tqtBY6lTp0YSSAMAACAASURBVE6WZLBJkya0bt2a999/P8ty0Tf5fuONNzjggAM47rjjSE9Pz/zp3Lkzy5YtA3z3bOS8PXv25BmPmdG9e/ds0ydNmkSrVq2oUqUKKSkpmYP1V65cmev22rZtm5mUgT8uAOvXr891vRNPPDEzKctYb/PmzezatQuApUuX0rp168ykDKBdu3b5HuMmIpIMZs+Gm26Cc8+Fa64JO5qiUawtZmZWFngI6AKsA5aa2RznXORgpJuAF5xz482sOTAXSCv0YPLZYhW21NRUjjrqKBYtWsS6detYt25dZiJ0zDHHMGbMGJxzLFq0aJ+uDIwevF6+fPm4Bs7XqVMn5rSNGzdmmRadcPz44498//33pKSkZFu/bNmyANx6661Zrr685ZZb8iyTUaNGDcqXL59l2syZMxk4cCDDhg3jzjvvpGbNmmzcuJFevXrl+RpjHRegQOs55/jjjz9ISUnh+++/j5lEFzSxFhEprT77DM45B9q18/XKzMKOqGgUd1dmO2CVc+4bADN7HugJRCZmDtgv+L0asKFYI0xgxx13HM899xyLFi0iLS2NAw44APAtLNu2bWPhwoWsWrUqz27HorB58+aY0w4//PAs0yzqk1SzZk0aNGjArFmzctz20KFDOe200zKf169fP894ovcDMG3aNI4++ugsY9HefvvtPLdVlA444AC++uqrbNN/+OGHEKIREUlMP/4IPXpAtWowcyZUrBh2REWnuBOzBsB3Ec/XAdGDaUYBr5vZpUBl4ORYGzKzocBQINexQaXJsccey9ixY3n66aczuzEB9ttvPw4//HDuu+8+gBwH/meItxUsPzZv3syiRYsyW/HWrl3Lhx9+yPnnn5/rep07d2bMmDFUqVKFZs2axVymfv36MZOxyFarinF8Snfs2EGFChWyTHvuuefyXK8otW3blsmTJ7N+/frM7sz333+fTZs2hRqXiEii+PNP6NsXNm6E//wH4vjfvERLxKsyBwATnXMNgb8Bk8wsW5zOuQnOuTbOuTbJ0u2TkfS8+uqrWRIz8N2Zr776KjVq1OCwww7LdTvNmjXLHHC/bNmyzFIb+6JWrVqce+65TJ48mZkzZ3LaaadRp06dPEs+dOnSha5du9KlSxfGjRvHggULmD17NqNHj+b666/P83UAjB07lqVLl8ZseYre1zvvvMMdd9zBG2+8wVVXXcWbb76Zr9dZ2M4//3z2339/TjvtNGbNmsWUKVM499xzqV27NmXKJOLHU0SkeF1+Obz9NjzxBLRtG3Y0Ra+4v/nXA40injcMpkUaArwA4JxbDFQEahVLdAmuYcOGNG7cGOdczMQsY3qsbrxI9957L5UrV+bUU0+lbdu2fPDBB/scW5MmTbjvvvsYNWoUZ511FlWrVmXevHl5tmSZGTNmzGDw4ME8+OCDdO3alYsuuiiuKvodO3bkmmuuYezYsRx99NFcdNFFuS5/0UUXcfXVVzN27Fh69+7NmjVrmDx5cr5fa2FKTU3ltddeo1KlSvTv359Ro0Zxzz33UL16dfbbb7+8NyAiUoqNHw+PPALXXuvHlyUDc84V387MygErgc74hGwpcLZzbnnEMq8CU51zE83sMOBNoIHLJdA2bdq4jCv4Yvniiy/ybEWSghs0aBCff/45ub0HEr9vv/2Wpk2bMmHChFy7gnVei0hptmABdOkC3bvDrFkQXA9WKpjZB865NrHmFesYM+dcupldAswDygJPOueWm9mtwDLn3BzgauAxM7sSfyHAoNySMpGS7q677qJ+/fo0adKEtWvXctddd1G7dm369OkTdmgiIqH45hs/ruzQQ+G550pXUpaXYi8w65ybiy+BETntHxG/rwByH70uUoqYGaNHj2bDhg1UqFCBjh07ct9996krU0SS0tat/gpM5/ztlpLtqzB5K/9Locm4U4AUzHXXXcd112WrtSwiknT27PHFY7/8EubNg4MOCjui4qfETERERBLCTTfBSy/Bv/8NnTuHHU04dD2+iIiIhG7KFLjrLhg6FIYPDzua8CRNYqbrB6Q00fksIqXJsmUweDAcf7xvLSutt1uKR1IkZikpKezYsSPsMEQKzY4dO2LeX1REpKTZuBHOOAPq1oUXX4So2xwnnaRIzOrUqcP69evZvn27WhqkRHPOsX37dtavXx/zxvEiIiXJzp3Qqxds2eKvwEySG/nkKikG/2eUHdiwYQO7du0KORqRfZOSkkLdunVVTkNESjTn/HiyJUtgxgw44oiwI0oMSZGYgU/O9IdMREQkMYwZA5Mmwa23+lYz8ZKiK1NEREQSx9y5MHIk9OvnS2TIXkrMREREpNh88QUMGABHHQVPPZXcV2DGosRMREREisUvv/jbLVWs6G9MXrly2BElnqQZYyYiIiLhSU+HM8+ENWtgwQJo3DjsiBKTEjMREREpciNGwBtvwBNPwHHHhR1N4lJXpoiIiBSpJ56AsWPhiit8hX/JmRIzERERKTLvvgvDhsEpp8C994YdTeJTYiYiIiJFYu1a6N0b0tLg+eehnAZQ5UmJmYiIiBS633/3V2D++ae/3VKNGmFHVDIodxUREZFCtWcPnHcefPYZvPwyNGsWdkQlhxIzERERKVS33w7Tp8N990H37mFHU7KoK1NEREQKzfTpcMstvsXsqqvCjqbkUWImIiIiheKTT2DgQGjfHh55RLdbKgglZiIiIrLPNm/2g/1r1oSZM/1tlyT/NMZMRERE9smff0KfPj45e/ddOOCAsCMquZSYiYiISIE5B8OH+4Ts+eehdeuwIyrZ1JUpIiIiBTZuHDz+ONx4I/TvH3Y0JZ8SMxERESmQN96AK6+Enj3h1lvDjqZ0UGImIiIi+fb113DmmXDYYTBpEpRRRlEodBhFREQkX3791beSlSnjb7dUtWrYEZUeGvwvIiIicdu9G84+27eYzZ8PBx4YdkSlixIzERERidsNN8DcuTB+PHTqFHY0pY+6MkVERCQuzz4L99wDw4bBxReHHU3ppMRMRERE8rRkCVxwgW8lGzs27GhKLyVmIiIikqv166FXL6hfH6ZNg5SUsCMqvTTGTERERHK0YweccQZs2wavvw61aoUdUemmxExERERics53X37wAcyaBS1ahB1R6afETERERGK65x6YPBnuuAN69Ag7muSgMWYiIiKSzUsvwfXXw1ln+UcpHkrMREREJIvly30R2b/+FZ54AszCjih5KDETERGRTD/95Lstq1Tx48pSU8OOKLlojJmIiIgAsGsX9OsH69bB229Dw4ZhR5R8lJiJiIgIAFddBQsWwNNPQ/v2YUeTnNSVKSIiIkyYAOPGwYgRMHBg2NEkLyVmIiIiSe6dd2D4cOjWDf75z7CjSW5KzERERJLY6tXQpw8cdBBMmQJly4YdUXJTYiYiIpKkfvvNX4GZng5z5kD16mFHJBr8LyIikoT27IG//93XLHv1VWjaNOyIBJSYiYiIJKVRo3ydsgcfhFNOCTsayaCuTBERkSTzwgtw220weDBcdlnY0UgkJWYiIiJJ5MMPYdAgOPZYePhh3W4p0SgxExERSRKbNkHPnlCrFsyYARUqhB2RRNMYMxERkSTwxx/Qqxf8/DO8+y7UrRt2RBKLEjMREZFSzjm4+GJYvNiPL2vVKuyIJCfqyhQRESnlxo6FiRPhH//wNymXxKXETEREpBSbNw+uvtp3Y95yS9jRSF6UmImIiJRSK1dC//7QogU88wyU0V/9hKe3SEREpBTasgVOPx1SUmD2bKhSJeyIJB4a/C8iIlLK7N4NZ50F33wDb70FaWlhRyTxUmImIiJSylx7rR9bNmECdOwYdjSSH+rKFBERKUWefhrGjIFLLoELLww7GskvJWYiIiKlxOLFMHQodO4MDzwQdjRSEErMRERESoHvvvMlMRo18kVky2mwUomkt01ERKSE274dzjjDP771FtSsGXZEUlDF3mJmZt3M7CszW2Vm1+WwzJlmtsLMlpvZ5OKOUUREpKRwDgYPho8+gsmToXnzsCOSfVGsLWZmVhZ4COgCrAOWmtkc59yKiGUOAa4HjnPO/WJmdYozRhERkZLkzjth6lT45z/htNPCjkb2VXG3mLUDVjnnvnHO/Qk8D/SMWuZC4CHn3C8AzrnNxRyjiIhIiTB7Ntx0E5xzDowcGXY0UhiKOzFrAHwX8XxdMC1SU6Cpmb1nZv81s26xNmRmQ81smZkt++GHH4ooXBERkcT02Wc+IWvbFh57DMzCjkgKQyJelVkOOAToBAwAHjOz6tELOecmOOfaOOfa1K5du5hDFBERCc+PP0KPHrDffjBrFlSqFHZEUliK+6rM9UCjiOcNg2mR1gFLnHO7gG/NbCU+UVtaPCGKiIgkrl27oG9f2LgR3nkH6tcPOyIpTMXdYrYUOMTMDjSz8sBZwJyoZWbhW8sws1r4rs1vijNIERGRRHXZZfD22/DEE9CuXdjRSGEr1sTMOZcOXALMA74AXnDOLTezW82sR7DYPOAnM1sBLACucc79VJxxioiIJKLx4+GRR/y9MM85J+xopCiYcy7sGPZZmzZt3LJly8IOQ0REpMgsWABdukC3bv5qzLJlw45ICsrMPnDOtYk1LxEH/4uIiEiEb77x48qaNvVFZJWUlV5KzERERBLY1q3+CkznYM4cfyWmlF66V6aIiEiC2rMHzj0XvvwS5s2Dgw8OOyIpakrMREREEtTNN8NLL8G//w2dO4cdjRQHdWWKiIgkoClT/H0wL7wQhg8POxopLkrMREREEsyyZTB4MHTsCOPG6XZLySRfXZlm1hxoja/e/6Rz7nszOxjY5JzbVhQBioiIJJONG+GMM6BuXZg+HcqXDzsiKU5xJWZmVgV4EugDpAfrvQZ8D9wJrAVGFFGMIiIiSWHnTujVC7ZsgUWLQLeCTj7xdmXeDxwLnAxUBSIbVecC3Qo5LhERkaTiHAwdCkuWwDPPwBFHhB2RhCHerszewOXOuQVmFl3Wbg3QpHDDEhERSS5jxsCkSTB6NPTuHXY0EpZ4W8wqATndr7IqsLtwwhEREUk+c+fCyJHQr58vkSHJK97EbCkwMId5fYFFhROOiIhIcvniCxgwAI48Ep56SldgJrt4uzJvBuab2RvANMABfzOzK/GJ2fFFFJ+IiEip9csv/nZLFSv6G5NXrhx2RBK2uFrMnHP/AToDFYBx+MH/o4G/ACc755YWWYQiIiKlUHo69O8Pa9bAjBnQuHHYEUkiiLuOmXPuPaCjmVUCagBbnHPbiywyERGRUmzECJg/H554Ao47LuxoJFHE1WJmZk+a2YEAzrkdzrkNGUmZmTUxsyeLMkgREZHS5IknYOxYuOIKX+FfJEO8g/8HATmVuasFnFco0YiIiJRy774Lw4ZBly5w771hRyOJJj/3ynQ5TG8B/FAIsYiIiJRqa9f6GmVpaTB1KpTL140RJRnkeEqY2eXA5cFTB8wysz+iFqsI1AUmFkl0IiIipcTvv0PPnvDHHzBnDtSoEXZEkohyy9VXANPxV2BeBSwANkYt8yfwJfBCkUQnIiJSCuzZA4MGwaefwssvQ7NmYUckiSrHxMw5Nx+YD2Bm24DHnXPriyswERGR0uL22+HFF+G++6B797CjkUQWV++2c250UQciIiJSGk2fDrfcAgMHwlVXhR2NJLq4hx2a2THAEKApfmxZFs65doUYl4iISIn3ySc+IWvfHh59VLdbkrzFW8esC/AO0BDogL8K8zfgSGB/4POiClBERKQk2rzZ326pRg1f2b9itiYNkeziLZdxKzAWODV4frNz7iR869kuYGHhhyYiIlIy/fkn9Onjk7NZs6BevbAjkpIi3sSsOfAqsAdfOqMygHNuDTAKuLEoghMRESlpnIPhw30h2aeegjZtwo5ISpJ4E7OdQBnnnMOXzDgoYt5WfBeniIhI0hs3Dh5/HG64Ac46K+xopKSJd/D/J8Ch+PIZbwLXm9l6fB2zW4HPiiY8ERGRkuONN+DKK30h2dtuCzsaKYnibTF7kL23ZLoB+B2Yhy86WwcYXvihiYiIlBxffw1nnumLx06aBGXyc9NDkUC8dczmRvy+3sxaAwcDlYAvnXN/FlF8IiIiCe/XX30rWZky/nZLVauGHZGUVHnm82ZW0cxWmlm3jGnO+9o596mSMhERSWa7d8M55/gWsxdfhL/8JeyIpCTLs8XMObfTzKrjr8gUERGRCDfcAK+8Ag8/DJ06hR2NlHTx9oA/B5xflIGIiIiUNM8+C/fcA8OG+R+RfRXvVZlrgTPNbCm+ntkm9l4MAL53c3xhByciIpKoliyBCy7wrWRjx4YdjZQW8SZmY4LHekDrGPMdoMRMRESSwvr10KsX1K8P06ZBSkrYEUlpEe9VmbroV0REBNixwydl27bB669DrVphRySlSbwtZiIiIknPOd99uWwZzJwJLVqEHZGUNkrMRERE4nTPPTB5Mtxxh69bJlLY1EUpIiISh5deguuvh/79/aNIUVBiJiIikofly+Hss+Gvf4UnnwSzsCOS0kqJmYiISC5++gl69IDKlWHWLEhNDTsiKc0KdEsmERGRZLBrl78x+bp1Pilr2DDsiKS00y2ZREREcnDVVfDWWzBxIrRvH3Y0kgx0SyYREZEYJkyAcePg6qvhvPPCjkaShW7JJCIiEuWdd2D4cOjWDe6+O+xoJJnolkwiIiIRVq+GPn3goINgyhQoWzbsiCSZ6JZMIiIigd9+84Vj09NhzhyoXj3siCTZqPK/iIgIsGcPDBwIn38Or74KTZuGHZEko7gTs+DKzIuADkBN4GfgP8AE59yWoglPRESkeIwa5e9/+cADcMopYUcjySquLkozOwj4DLgVqIy/GKBy8PzTYL6IiEiJ9MILcNttMHgwXH552NFIMou3xewBYAvQ3jm3PmOimTUA5gL3A7qdq4iIlDgffgiDBsGxx8LDD+t2SxKueAf1dwL+EZmUAQTPbwVOLOS4REREitymTX6wf61aMGMGVKgQdkSS7OJtMXNAThcMlyFrTTMREZGE98cf0Lu3vxfme+9B3bphRyQSf4vZAuA2M2sSOTF4fivwZmEHJiIiUlScg2HDYNEiePppaNUq7IhEvHhbzK7EJ19fm9mH+Mr/dfDFZr8Driqa8ERERArf2LHw1FPwj39Av35hRyOyV1wtZs65b4FmwGXAciAFWAFcAhzmnFtdVAGKiIgUpnnz/P0ve/WCW24JOxqRrPJsMTOzisAc4E7n3CPAI0UelYiISBFYuRL694cWLeCZZ6CM7msjCSbPU9I5txNoS86D/0VERBLeli3QowekpMDs2VClStgRiWQX7/8Kc4AzijIQERGRorJ7NwwYAP/7H0yfDmlpYUckElu8g//nAfeaWT18QdlNRJXIcM7NLeTYRERECsW118Jrr8GECXD88WFHI5KzeBOzZ4PH3sFPtNzqnImIiITm6adhzBi45BK48MKwoxHJXbxdmQfm8fOXeHdoZt3M7CszW2Vm1+WyXB8zc2bWJt5ti4iIRFq8GIYOhZNOgvvvDzsakbzFe1XmY/irMhfuy87MrCzwENAFWAcsNbM5zrkVUctVBS4HluzL/kREJHmtW+dLYjRq5G9SnpISdkQieSvuqzLbAaucc9845/4Enif2zc9vA+4GdhbCPkVEJMls3+7vgbl9O8yZA/vvH3ZEIvEp7qsyG+DvFJBhXTAtk5n9FWjknHulEPYnIiJJxjkYPBg++ggmT4bmzcOOSCR+CXVVppmVAe4HBsWx7FBgKEDjxo33ddciIlJK3HknTJ0K//wnnHZa2NGI5I855/JeyGxPHos451yeXZ1mdgwwyjnXNXh+fbDyXcHzasD/gN+CVQ4AfgZ6OOeW5bTdNm3auGXLcpwtIiJJYvZsOOMMOOccmDQJzMKOSCQ7M/vAORfz4sZ4W8wOLKRYlgKHmNmBwHrgLODsjJnOuV+BWhnPzWwhMCK3pExERATgs8/g3HOhbVt47DElZVIyxZWYOefWFMbOnHPpZnYJvmu0LPCkc265md0KLHPOzSmM/YiISHL58Ud/u6WqVWHmTKhUKeyIRAomx8TMzM4GXnPO/RwxrTGwwTmXHjGtPjDIOXdnPDsMxqLNjZr2jxyW7RTPNkVEJHnt2gV9+8LGjfDOO9CgQd7riCSq3K7KnAQcnPEkqEH2LXBE1HKN8OUtREREit1ll8Hbb8Pjj0O7dmFHI7JvckvMYvXOq8deREQSxvjx8MgjMHKkH18mUtLFW8dMREQkoSxYAJdeCqee6ktkiJQGSsxERKTE+eYbP66saVNfRLZsYdybRiQB5JWYxSpylnfhMxERkSKybZu/AtM5f7ul/fYLOyKRwpNXuYx5ZpYeNe3NqGnx1kITERHZJ3v2+LFkX34J8+bBwQfnvY5ISZJbUjW62KIQERGJw803+1ayf/0LOncOOxqRwpdjYuacU2ImIiIJY8oUP8j/wgvhkkvCjkakaGjwv4iIJLxly2DwYOjYEcaN0+2WpPRSYiYiIglt40Z/Y/K6dWH6dChfPuyIRIqOBu6LiEjC2rkTevWCX36BRYugdu2wIxIpWkrMREQkITkHQ4fCkiW+pezII8OOSKToqStTREQS0pgxMGkSjB4NvXuHHY1I8VBiJiIiCWfuXH//y7594aabwo5GpPgoMRMRkYTy5ZcwYIDvupw4EcroL5UkEZ3uIiKSMH75xd9uqUIFmD0bKlcOOyKR4qXB/yIikhDS06F/f1i9GhYsgMaNw45IpPgpMRMRkYQwYgTMnw9PPAHHHRd2NCLhUFemiIiE7oknYOxYuPxyX+FfJFkpMRMRkVC99x4MGwZdusB994UdjUi4lJiJiEho1q71NcrS0mDqVCinATaS5PQREBGRUPz+O/Ts6W+79PbbUKNG2BGJhE+JmYiIFLs9e2DQIPjkE3jlFWjWLOyIRBKDEjMRESl2t98OL74I994L3buHHY1I4tAYMxERKVbTp8Mtt8DAgXD11WFHI5JYlJiJiEix+eQTn5C1bw+PPgpmYUckkliUmImISLHYvNnfbqlGDZgxAypWDDsikcSjMWYiIlLk/vwT+vb1ydl//gP16oUdkUhiUmImIiJFyjkYPtwnZJMnQ5s2YUckkrjUlSkiIkVq3Dh4/HG44QYYMCDsaEQSmxIzEREpMm+8AVde6ceW3XZb2NGIJD4lZiIiUiRWrYIzz/TFY599FsroL45InvQxERGRQvfrr76VzAzmzIGqVcOOSKRk0OB/EREpVLt3wznnwNdfw+uvw1/+EnZEIiWHEjMRESlUN9zg73/58MNw4olhRyNSsqgrU0RECs2zz8I998DFF8OwYWFHI1LyKDETEZFC8f77cMEF0KkT/OtfYUcjUjIpMRMRkX22fj2ccYav6D9tGqSkhB2RSMmkMWYiIrJPduyAXr1g2zaYNw9q1Qo7IpGSS4mZiIgUmHO++3LpUpg1C1q2DDsikZJNXZkiIlJg99zj7395++3Qs2fY0YiUfErMRESkQF5+Ga6/Hvr39yUyRGTfKTETEZF8W74czj4bWrWCJ5/0Ff5FZN8pMRMRkXz56Sd/u6XUVD+uLDU17IhESg8N/hcRkbjt2uVvTL5uHSxcCI0ahR2RSOmixExEROJ21VXw1lswcSIcc0zY0YiUPurKFBGRuEyYAOPGwdVXw3nnhR2NSOmkxExERPL0zjswfDh06wZ33x12NCKllxIzERHJ1erV0KcPHHQQTJkCZcuGHZFI6aXETEREcvTbb75w7K5dMGcOVK8edkQipZsG/4uISEx79sDAgfD55/Dqq9C0adgRiZR+SsxERCSmUaNg5kx44AE45ZSwoxFJDurKFBGRbF54AW67Dc4/Hy6/POxoRJKHEjMREcnio49g0CA49lgYP163WxIpTkrMREQk06ZNfrD//vvDjBlQoULYEYkkF40xExERAP74A3r3hh9/hPfeg7p1w45IJPkoMRMREZyDYcNg0SI/vqxVq7AjEklO6soUERHGjoWnnoKbb4Z+/cKORiR5KTETEUlyr7/u73/Zq5cvkSEi4VFiJiKSxFauhP794fDD4ZlnoIz+KoiESh9BEZEktWUL9OgB5cr52y1VqRJ2RCKiwf8iIklo924YMAD+9z94801ISws7IhGBEFrMzKybmX1lZqvM7LoY868ysxVm9qmZvWlmTYo7RhGR0u7aa+G11+Chh+D448OORkQyFGtiZmZlgYeA7kBzYICZNY9a7COgjXPuCOBF4J7ijFFEpLR7+mkYMwaGD4ehQ8OORkQiFXeLWTtglXPuG+fcn8DzQM/IBZxzC5xz24On/wUaFnOMIiKl1uLFPhk76SR/c3IRSSzFnZg1AL6LeL4umJaTIcCrRRqRiEiSWLfOl8Ro1MgXkU1JCTsiEYmWsIP/zexcoA1wQg7zhwJDARo3blyMkYmIlDzbt/t7YG7f7gf7779/2BGJSCzF3WK2HmgU8bxhMC0LMzsZuBHo4Zz7I9aGnHMTnHNtnHNtateuXSTBioiUBs7B4MHw0Ufw3HO+ZpmIJKbibjFbChxiZgfiE7KzgLMjFzCzVsCjQDfn3OZijk9EpNTYtg2+/BKmTIGpU+Guu+D008OOSkRyU6yJmXMu3cwuAeYBZYEnnXPLzexWYJlzbg5wL1AFmGZmAGudcz2KM04RkZLkl1/giy9gxYq9P198AWvX7l1m4EBfIkNEEluxjzFzzs0F5kZN+0fE7ycXd0wiIonOOfjhh+wJ2IoV8P33e5erWBEOOww6dIDmzf3PYYfBoYeC/19XRBJZwg7+FxFJRs7Bhg17W70iE7Cfftq7XJUqPunq1m1v8tW8OTRpAmXLhhe/iOwbJWYiIiHYs8d3NUZ2PWb8vnXr3uVq1PAJV+/eWVvAGjZUC5hIaaTETESkCKWnw7ffZu9+/PJLX7oiQ926Puk699y9CVjz5lCnjhIwkWSis2Wk/wAADrlJREFUxExEpBD8+Sd8/XX21q+vvvLzMjRs6BOuoUP3dj8edpjqiomIp8RMRCQfduzwyVZ0F+TXX8Pu3X4ZM0hLyz4G7LDDYL/9Qg1fRBKcEjMRkRgyaoBFl6D45hs/QB/8IPuDD/YJV+QYsEMPhdTUcOMXkZJJiZmIJLVYNcBWrIDvIu7qm5Lik63WrbOOATvkEKhQIbzYRaT0UWImIqVeRg2wWCUoYtUAO/74rCUoDjoIyunbUkSKgb5qRKTUiKwBFp2ERdYAq1rVJ10Z478ykjDVABORsCkxE5ESJ7oGWGQiFl0D7PDDs47/at4cGjRQCQoRSUxKzEQkYaWn+8H20d2POdUA+/vf93Y/qgaYiJRESsxEJHQFqQEW2QVZs2Z4sYuIFCYlZiJSbOKtAXbggdnHgDVrphpgIlL6KTETkUIXqwbYihX+1kTRNcCi7wOpGmAiksyUmIlIgf3yS+wSFLFqgLVpAwMH7u1+VA0wEZHslJiJSK4ia4BFJ2GRNcAqVcpaAyzj5y9/UQ0wEZF46etSRIDsNcAik7DoGmDNm0P37lmvgGzSBMqUCS9+EZHSQImZSJLZswfWrMne/RhdA6xmTZ9w9emTtQq+aoCJiBQdJWYipVRGDbDo7sfoGmAHHOCTrr//PWsXZO3aSsBERIqbEjOREi6yBlhkEhZdA6xRI59wRY4BUw0wEZHEosRMpISIrgGWkYTFqgEWPQZMNcBEREoGJWYiCSa/NcAix4CpBpiISMmmxEwkJBk1wKLHgEXWACtfPnsNsObNfQ2w8uXDi11ERIqGEjORIhRdAywyCYtVA+yEE7KWoFANMBGR5KKvfJFC4BysX5+9BMWKFfDzz3uXi6wBFlmCQjXA5P/bu/9gz+q6juPP1y4wyo/ERgJWSBhCSMwEWRjFwHLA+DEaCGaFImM//AED2USC6UjlYCZoTWkWy48CCeTXqAnI74FwaVyiSKCkIoGWXyawoECw7/4457t87+V7937v3nv3nN37fMycud/z+33u2R+v7/mc8zmSBAYzaUYGfYCNeg3RqlUvLDfoA+zIIyd2QbFkiV1QSJKmZjCTRhjuA2w4hN11V/N05MB22zWBa/j+L/sAkyStK4OZFrTJfYANwtdUfYAdcMALTZD2ASZJmmsGMy0Io/oAu/NOuOeeqfsAG1z92n335t4wSZLmm8FMG5VVq5orXpPv/5rcB9iuu8Iee0y8B2y33ZqnIyVJ6orBTHOuqrlJfvXq5mrU8DB52rosMzx+333T9wG2dCkcc8wLT0DaB5gkqa8MZmN4/PHmistchYn5Cil9WWf16vV7fjbfvGluHNz/NRh23tk+wCRJGxb/2xrDddfBEUd0s+9Fi5ph8eKJwzjTpltms83mZjtdLrPttvYBJknaeBjMxrDvvnDppes/iCxaZJcLkiQtJAazMSxZAocf3nUVkiRpY2cDkCRJUk8YzCRJknrCYCZJktQTBjNJkqSeMJhJkiT1hMFMkiSpJwxmkiRJPWEwkyRJ6gmDmSRJUk8YzCRJknrCVzL1QVUzrF7dDMOfZzJtXdeb723N5/ans7aXjU73ItI+rjuf23bdjXvdRYtGD4MX80439HU5XyisjYzBbBzXXAPHHz9/AaWq6yPsv+F/hIc/r+0f5bX9Xqf7nfdx3fnctn8GtSEbN+j1NVzOx3J9rm2my033b/1GxmA2jpe9DF73uhcHg1FhYapp67peF9vvW60L6C/kBqmvYdJ1J84b9UXx+edfPG3U0Ofluqzt2WfX/7Eu1C9Ro/6PmGnQGzcMHnccHH10Z4dqMBvH0qVw4YVdVyH102ybZiWNb6qQ3ZewujEE88026/QUG8wkSdpQDJr1Fi3quhLNE8+sJElSTxjMJEmSesJgJkmS1BMGM0mSpJ4wmEmSJPWEwUySJKknDGaSJEk9YTCTJEnqCYOZJElSTxjMJEmSesJgJkmS1BMGM0mSpJ4wmEmSJPVEqqrrGmYtySPAf8/zbl4BPDrP+9DMeV76x3PST56X/vGc9NP6OC+vqqptRs3YKILZ+pDk21W1d9d1aCLPS/94TvrJ89I/npN+6vq82JQpSZLUEwYzSZKknjCYje+vui5AI3le+sdz0k+el/7xnPRTp+fFe8wkSZJ6witmkiRJPWEwm0aSHZNcn+TOJN9JckLXNQmSvCTJPyb55/a8nNp1TWokWZzkn5J8vetaBEnuTXJHktuTfLvretRIsnWSi5PcneSuJG/suqaFLMlu7d+RwfBEkhM7qcWmzLVLsj2wfVXdlmQrYAXwS1V1Z8elLWhJAmxRVU8m2RS4GTihqpZ3XNqCl+QjwN7Aj1XVYV3Xs9AluRfYu6rsL6tHkpwL3FRVZybZDNi8qh7rui41Xy6BB4B9q2q++0h9Ea+YTaOqVlbVbe3nVcBdwCu7rUrVeLId3bQd/JbRsSQ7AIcCZ3Zdi9RXSV4G7A8sA6iqZw1lvfJW4D+6CGVgMJuRJDsBewK3dluJYE2T2e3Aw8DVVeV56d7ngZOA1V0XojUK+GaSFUl+s+tiBMDOwCPA2W2z/5lJtui6KK3xbuCCrnZuMBtTki2BS4ATq+qJrusRVNXzVfV6YAdgnySv7bqmhSzJYcDDVbWi61o0wZurai/gYODDSfbvuiCxCbAX8MWq2hN4CvhotyUJoG1Wfjvwla5qMJiNob2H6RLg/Kq6tOt6NFHbBHA98Itd17LA7Qe8vb2n6e+AX0hyXrclqaoeaH8+DFwG7NNtRQLuB+4fusp/MU1QU/cOBm6rqoe6KsBgNo32JvNlwF1VdUbX9aiRZJskW7efXwocCNzdbVULW1WdXFU7VNVONE0B11XV0R2XtaAl2aJ9aIm2qewg4F+7rUpV9SBwX5Ld2klvBXygrB9+hQ6bMaG5nKq12w94D3BHez8TwClV9Y0OaxJsD5zbPj2zCLioquyeQZpoW+Cy5vslmwBfrqoruy1JreOB89ums/8Eju24ngWv/fJyIPBbndZhdxmSJEn9YFOmJElSTxjMJEmSesJgJkmS1BMGM0mSpJ4wmEmSJPWEwUzSlJJ8MkkluWrEvIuT3LAea3lLW0sv3/CQ5KeT3JTkqbbOnaZYrpIct36rk7ShMJhJGsdBSZZ2XUTP/QmwNc3rXN4IrOy2HEkbIoOZpOn8L3AH8LGuC5lPSV4yy03sDlxdVddW1fKqemYu6ppvc3DckuaQwUzSdAr4FM17MH9mqoXaZs9HR0yf0HSX5N4kn03y0SQrkzye5PQ0DknynSSrklye5OUjdrUkydfbJsPvJfnAiH3+XJIbk/wwyfeT/PXg1UTt/Pe1de2T5IYkPwJ+dy3H9vok17bb+0GS85Ns287bKUkBuwC/3W73hqm2NWLbhya5OsnDSZ5IsjzJQUPzX9Nu8y2T1tsyyZNJTpiL405ycpJ7kjyd5KEkVybZbtzjkDQ3DGaSxvEV4LvM3VWzd9O8TPtY4DPAR4AzgD8EPg58ADgAOG3EusuAfwGOAL4BfDHJYYOZSfYDrgEeBI4ETgQOAc4esa0LgK+180e+0ivJNsANwObAr9K8SucA4Or2dToraZouHwS+3H7+0Di/hNbObQ3vAd4J3AJc0R4HVXUnsBx436T1jgI2Bc6b7XEneS9wCs05eBvwQeAeYIsZHIekOeC7MiVNq6pWJzkNWJbkE1X177Pc5NPAUVX1PHBlknfQBJ5dq+q/AJL8LHAMTUgbdkVVndJ+virJLsDv80Kw+jRwS1X98mCFJA8A1yZ5bVUNv8T7z6rqT6ep9Xfan2+rqifa7X2XJiy9s6ouAJYneQZYWVXLx/0lAFTVnw/VuQi4HtgDeD/wD+2sZcDnkxxXVU+2044FvlZV35/tcSd5P/DNqvrC0DKXzuQ4JM0Nr5hJGtd5wPeAk+dgWze0oWzgHuDeQSgbmrZNe1Vq2GWTxi8F3pBkcZLNaa5YXZRkk8EA3Az8H/CGSev+/Ri17kMTWp4YTKiqW4F7gTePsf5aJdkhybltiHqurfMg4NVDi13Y/jyqXWeXdt9nt+OzPe7bgUOSnNo2cy6e7XFJWjcGM0ljqarnaJodj07yqllu7rFJ489OMS3A5GD28IjxTYBXAC8HFgNfoAkkg+EZmma/HSet+9AYtW4/xXIPAT8+xvpTaq+QfRV4E/AJ4OeBpcAVwJqb8qtqFXARzVUyaJo1HwSubMdne9xn0TRlvgu4FXgoyR8Z0KT1z6ZMSTNxFk2z4e+NmPc0k0LUFDfvz9ZPjBh/DniUJswU8Ema+88m+59J4zXG/laO2CfAtsCKMdZfm58C9gQOrqpByCLJS0cseyZwc5JdgfcCfzN01fExZnHcVbUa+BzwuSQ7Ar9G88DH/cBfzvCYJM2CwUzS2KrqmSSfpbkpfwXNVZmB+4Gtkryyqh5opx00eRtz4HCaK0rD4yvakPJUkuXAblX1B3O0v1uBDybZqr1yRZo+3XaiaSqcjUEAW9O1Rns1cj+aBxzWqKpbkvwbTTj+SeCcoXlzdtxVdR/w6STHAq+ZzbYkzZzBTNJMfYmm2etNwI1D068EfgScleR0mqcNX9SVxRw4OMmn2n0fARwIvGNo/kk0N7yvBi4GVtEEmUOBj63Dgwtn0DyleFWSPwa2pLnR/g7gktkcCHA3TaA9PcnHga2AU4EHplh+GU1Htt+qqrsnzVvn407yJZr+6pYDj9M0qe7K6CujkuaR95hJmpGq+iFNs9fk6Y/SdPewA3A5cDRN9xJz7deBvdp9HAZ8uKq+OlTHzcD+wDbA39J0C3EScB/j3VM2QVU9QhNUnqbpZuIvgJuAA6vq2dkcSNsJ7RE0TbEX03QXchoTA++wy9ufZ43Y1myO+1vtumfTNIUeDvxGVV2+1rUkzblUjXOLhSSpa0k+RPMAxpLhp0QlbTxsypSknkvzQvRX0zQhn2MokzZeXjGTpJ5Lcg5Ns/CNwLuq6gfdViRpvhjMJEmSesKb/yVJknrCYCZJktQTBjNJkqSeMJhJkiT1hMFMkiSpJwxmkiRJPfH/+coE5zM2G4QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_layers_l, error_rates_1, label='No pre-training', color='blue')\n",
    "plt.plot(n_layers_l, error_rates_2, label='With pre-training', color='red')\n",
    "plt.title('Error rate according to number of layers', size=20)\n",
    "plt.xlabel('Number of layers', size=15)\n",
    "plt.ylabel('Error rate', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "fig.savefig('n_layers.jpg', dpi = 300, bbox_inches='tight', orientation = 'landscape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJBJYfco8FWE"
   },
   "source": [
    "### 2 Réseaux en fonction du nombre de neuronnes par couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "F4KbKMOk8FCh",
    "outputId": "5b2aa34c-f7c2-4017-edd7-2a8562bd0809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 \t : \t loss 0.23161 - accuracy 0.10596\n",
      "iteration 1 \t : \t loss 0.23081 - accuracy 0.11084\n",
      "iteration 2 \t : \t loss 0.23040 - accuracy 0.12012\n",
      "iteration 3 \t : \t loss 0.22994 - accuracy 0.12646\n",
      "iteration 4 \t : \t loss 0.22941 - accuracy 0.13452\n",
      "iteration 5 \t : \t loss 0.22875 - accuracy 0.14648\n",
      "iteration 6 \t : \t loss 0.22791 - accuracy 0.16138\n",
      "iteration 7 \t : \t loss 0.22678 - accuracy 0.18042\n",
      "iteration 8 \t : \t loss 0.22518 - accuracy 0.20801\n",
      "iteration 9 \t : \t loss 0.22278 - accuracy 0.24634\n",
      "iteration 10 \t : \t loss 0.21898 - accuracy 0.27979\n",
      "iteration 11 \t : \t loss 0.21278 - accuracy 0.31763\n",
      "iteration 12 \t : \t loss 0.20321 - accuracy 0.34839\n",
      "iteration 13 \t : \t loss 0.19081 - accuracy 0.39551\n",
      "iteration 14 \t : \t loss 0.17741 - accuracy 0.44312\n",
      "iteration 15 \t : \t loss 0.16364 - accuracy 0.49146\n",
      "iteration 16 \t : \t loss 0.14927 - accuracy 0.53760\n",
      "iteration 17 \t : \t loss 0.13529 - accuracy 0.57495\n",
      "iteration 18 \t : \t loss 0.12331 - accuracy 0.60498\n",
      "iteration 19 \t : \t loss 0.11370 - accuracy 0.63232\n",
      "iteration 20 \t : \t loss 0.10583 - accuracy 0.65601\n",
      "iteration 21 \t : \t loss 0.09913 - accuracy 0.68359\n",
      "iteration 22 \t : \t loss 0.09330 - accuracy 0.70459\n",
      "iteration 23 \t : \t loss 0.08818 - accuracy 0.72241\n",
      "iteration 24 \t : \t loss 0.08369 - accuracy 0.74023\n",
      "iteration 25 \t : \t loss 0.07972 - accuracy 0.75073\n",
      "iteration 26 \t : \t loss 0.07621 - accuracy 0.76245\n",
      "iteration 27 \t : \t loss 0.07307 - accuracy 0.77295\n",
      "iteration 28 \t : \t loss 0.07024 - accuracy 0.78296\n",
      "iteration 29 \t : \t loss 0.06769 - accuracy 0.79102\n",
      "iteration 30 \t : \t loss 0.06536 - accuracy 0.79761\n",
      "iteration 31 \t : \t loss 0.06322 - accuracy 0.80542\n",
      "iteration 32 \t : \t loss 0.06125 - accuracy 0.81299\n",
      "iteration 33 \t : \t loss 0.05942 - accuracy 0.81934\n",
      "iteration 34 \t : \t loss 0.05772 - accuracy 0.82568\n",
      "iteration 35 \t : \t loss 0.05613 - accuracy 0.82983\n",
      "iteration 36 \t : \t loss 0.05463 - accuracy 0.83472\n",
      "iteration 37 \t : \t loss 0.05322 - accuracy 0.83813\n",
      "iteration 38 \t : \t loss 0.05188 - accuracy 0.84302\n",
      "iteration 39 \t : \t loss 0.05062 - accuracy 0.84619\n",
      "iteration 40 \t : \t loss 0.04941 - accuracy 0.85205\n",
      "iteration 41 \t : \t loss 0.04826 - accuracy 0.85596\n",
      "iteration 42 \t : \t loss 0.04716 - accuracy 0.85962\n",
      "iteration 43 \t : \t loss 0.04609 - accuracy 0.86182\n",
      "iteration 44 \t : \t loss 0.04507 - accuracy 0.86572\n",
      "iteration 45 \t : \t loss 0.04407 - accuracy 0.87036\n",
      "iteration 46 \t : \t loss 0.04311 - accuracy 0.87451\n",
      "iteration 47 \t : \t loss 0.04217 - accuracy 0.87915\n",
      "iteration 48 \t : \t loss 0.04125 - accuracy 0.88330\n",
      "iteration 49 \t : \t loss 0.04036 - accuracy 0.88672\n",
      "iteration 50 \t : \t loss 0.03948 - accuracy 0.88892\n",
      "iteration 51 \t : \t loss 0.03862 - accuracy 0.89160\n",
      "iteration 52 \t : \t loss 0.03777 - accuracy 0.89282\n",
      "iteration 53 \t : \t loss 0.03694 - accuracy 0.89453\n",
      "iteration 54 \t : \t loss 0.03612 - accuracy 0.89844\n",
      "iteration 55 \t : \t loss 0.03532 - accuracy 0.90210\n",
      "iteration 56 \t : \t loss 0.03453 - accuracy 0.90332\n",
      "iteration 57 \t : \t loss 0.03375 - accuracy 0.90576\n",
      "iteration 58 \t : \t loss 0.03298 - accuracy 0.90894\n",
      "iteration 59 \t : \t loss 0.03222 - accuracy 0.91089\n",
      "iteration 60 \t : \t loss 0.03148 - accuracy 0.91382\n",
      "iteration 61 \t : \t loss 0.03075 - accuracy 0.91602\n",
      "iteration 62 \t : \t loss 0.03003 - accuracy 0.91821\n",
      "iteration 63 \t : \t loss 0.02932 - accuracy 0.91943\n",
      "iteration 64 \t : \t loss 0.02863 - accuracy 0.92236\n",
      "iteration 65 \t : \t loss 0.02795 - accuracy 0.92407\n",
      "iteration 66 \t : \t loss 0.02729 - accuracy 0.92578\n",
      "iteration 67 \t : \t loss 0.02664 - accuracy 0.92725\n",
      "iteration 68 \t : \t loss 0.02600 - accuracy 0.92847\n",
      "iteration 69 \t : \t loss 0.02537 - accuracy 0.93164\n",
      "iteration 70 \t : \t loss 0.02476 - accuracy 0.93359\n",
      "iteration 71 \t : \t loss 0.02417 - accuracy 0.93579\n",
      "iteration 72 \t : \t loss 0.02359 - accuracy 0.93774\n",
      "iteration 73 \t : \t loss 0.02302 - accuracy 0.93921\n",
      "iteration 74 \t : \t loss 0.02246 - accuracy 0.94141\n",
      "iteration 75 \t : \t loss 0.02191 - accuracy 0.94189\n",
      "iteration 76 \t : \t loss 0.02138 - accuracy 0.94336\n",
      "iteration 77 \t : \t loss 0.02086 - accuracy 0.94482\n",
      "iteration 78 \t : \t loss 0.02035 - accuracy 0.94653\n",
      "iteration 79 \t : \t loss 0.01986 - accuracy 0.94727\n",
      "iteration 80 \t : \t loss 0.01937 - accuracy 0.94897\n",
      "iteration 81 \t : \t loss 0.01890 - accuracy 0.95020\n",
      "iteration 82 \t : \t loss 0.01843 - accuracy 0.95215\n",
      "iteration 83 \t : \t loss 0.01798 - accuracy 0.95337\n",
      "iteration 84 \t : \t loss 0.01753 - accuracy 0.95508\n",
      "iteration 85 \t : \t loss 0.01710 - accuracy 0.95605\n",
      "iteration 86 \t : \t loss 0.01667 - accuracy 0.95825\n",
      "iteration 87 \t : \t loss 0.01626 - accuracy 0.95874\n",
      "iteration 88 \t : \t loss 0.01585 - accuracy 0.96069\n",
      "iteration 89 \t : \t loss 0.01545 - accuracy 0.96216\n",
      "iteration 90 \t : \t loss 0.01506 - accuracy 0.96436\n",
      "iteration 91 \t : \t loss 0.01468 - accuracy 0.96631\n",
      "iteration 92 \t : \t loss 0.01430 - accuracy 0.96802\n",
      "iteration 93 \t : \t loss 0.01394 - accuracy 0.96851\n",
      "iteration 94 \t : \t loss 0.01358 - accuracy 0.97021\n",
      "iteration 95 \t : \t loss 0.01323 - accuracy 0.97070\n",
      "iteration 96 \t : \t loss 0.01289 - accuracy 0.97168\n",
      "iteration 97 \t : \t loss 0.01255 - accuracy 0.97266\n",
      "iteration 98 \t : \t loss 0.01223 - accuracy 0.97363\n",
      "iteration 99 \t : \t loss 0.01191 - accuracy 0.97485\n",
      "iteration 100 \t : \t loss 0.01159 - accuracy 0.97559\n",
      "iteration 101 \t : \t loss 0.01129 - accuracy 0.97681\n",
      "iteration 102 \t : \t loss 0.01099 - accuracy 0.97803\n",
      "iteration 103 \t : \t loss 0.01070 - accuracy 0.97852\n",
      "iteration 104 \t : \t loss 0.01041 - accuracy 0.97900\n",
      "iteration 105 \t : \t loss 0.01014 - accuracy 0.97925\n",
      "iteration 106 \t : \t loss 0.00987 - accuracy 0.98071\n",
      "iteration 107 \t : \t loss 0.00960 - accuracy 0.98193\n",
      "iteration 108 \t : \t loss 0.00934 - accuracy 0.98315\n",
      "iteration 109 \t : \t loss 0.00909 - accuracy 0.98389\n",
      "iteration 110 \t : \t loss 0.00885 - accuracy 0.98462\n",
      "iteration 111 \t : \t loss 0.00861 - accuracy 0.98511\n",
      "iteration 112 \t : \t loss 0.00838 - accuracy 0.98682\n",
      "iteration 113 \t : \t loss 0.00815 - accuracy 0.98730\n",
      "iteration 114 \t : \t loss 0.00793 - accuracy 0.98804\n",
      "iteration 115 \t : \t loss 0.00772 - accuracy 0.98877\n",
      "iteration 116 \t : \t loss 0.00751 - accuracy 0.98999\n",
      "iteration 117 \t : \t loss 0.00731 - accuracy 0.99170\n",
      "iteration 118 \t : \t loss 0.00711 - accuracy 0.99243\n",
      "iteration 119 \t : \t loss 0.00692 - accuracy 0.99292\n",
      "iteration 120 \t : \t loss 0.00673 - accuracy 0.99292\n",
      "iteration 121 \t : \t loss 0.00655 - accuracy 0.99316\n",
      "iteration 122 \t : \t loss 0.00637 - accuracy 0.99341\n",
      "iteration 123 \t : \t loss 0.00620 - accuracy 0.99341\n",
      "iteration 124 \t : \t loss 0.00603 - accuracy 0.99390\n",
      "iteration 125 \t : \t loss 0.00587 - accuracy 0.99438\n",
      "iteration 126 \t : \t loss 0.00571 - accuracy 0.99463\n",
      "iteration 127 \t : \t loss 0.00556 - accuracy 0.99463\n",
      "iteration 128 \t : \t loss 0.00541 - accuracy 0.99463\n",
      "iteration 129 \t : \t loss 0.00527 - accuracy 0.99487\n",
      "iteration 130 \t : \t loss 0.00513 - accuracy 0.99561\n",
      "iteration 131 \t : \t loss 0.00499 - accuracy 0.99585\n",
      "iteration 132 \t : \t loss 0.00486 - accuracy 0.99585\n",
      "iteration 133 \t : \t loss 0.00473 - accuracy 0.99609\n",
      "iteration 134 \t : \t loss 0.00461 - accuracy 0.99658\n",
      "iteration 135 \t : \t loss 0.00449 - accuracy 0.99658\n",
      "iteration 136 \t : \t loss 0.00437 - accuracy 0.99683\n",
      "iteration 137 \t : \t loss 0.00426 - accuracy 0.99731\n",
      "iteration 138 \t : \t loss 0.00415 - accuracy 0.99731\n",
      "iteration 139 \t : \t loss 0.00405 - accuracy 0.99780\n",
      "iteration 140 \t : \t loss 0.00395 - accuracy 0.99780\n",
      "iteration 141 \t : \t loss 0.00385 - accuracy 0.99780\n",
      "iteration 142 \t : \t loss 0.00375 - accuracy 0.99780\n",
      "iteration 143 \t : \t loss 0.00366 - accuracy 0.99805\n",
      "iteration 144 \t : \t loss 0.00357 - accuracy 0.99805\n",
      "iteration 145 \t : \t loss 0.00348 - accuracy 0.99805\n",
      "iteration 146 \t : \t loss 0.00340 - accuracy 0.99829\n",
      "iteration 147 \t : \t loss 0.00332 - accuracy 0.99854\n",
      "iteration 148 \t : \t loss 0.00324 - accuracy 0.99878\n",
      "iteration 149 \t : \t loss 0.00316 - accuracy 0.99878\n",
      "iteration 150 \t : \t loss 0.00309 - accuracy 0.99927\n",
      "iteration 151 \t : \t loss 0.00302 - accuracy 0.99927\n",
      "iteration 152 \t : \t loss 0.00295 - accuracy 0.99927\n",
      "iteration 153 \t : \t loss 0.00288 - accuracy 0.99927\n",
      "iteration 154 \t : \t loss 0.00282 - accuracy 0.99927\n",
      "iteration 155 \t : \t loss 0.00276 - accuracy 0.99927\n",
      "iteration 156 \t : \t loss 0.00270 - accuracy 0.99927\n",
      "iteration 157 \t : \t loss 0.00264 - accuracy 0.99927\n",
      "iteration 158 \t : \t loss 0.00258 - accuracy 0.99927\n",
      "iteration 159 \t : \t loss 0.00253 - accuracy 0.99927\n",
      "iteration 160 \t : \t loss 0.00247 - accuracy 0.99927\n",
      "iteration 161 \t : \t loss 0.00242 - accuracy 0.99927\n",
      "iteration 162 \t : \t loss 0.00237 - accuracy 0.99927\n",
      "iteration 163 \t : \t loss 0.00232 - accuracy 0.99951\n",
      "iteration 164 \t : \t loss 0.00227 - accuracy 0.99951\n",
      "iteration 165 \t : \t loss 0.00223 - accuracy 0.99951\n",
      "iteration 166 \t : \t loss 0.00218 - accuracy 0.99951\n",
      "iteration 167 \t : \t loss 0.00214 - accuracy 0.99951\n",
      "iteration 168 \t : \t loss 0.00210 - accuracy 0.99951\n",
      "iteration 169 \t : \t loss 0.00206 - accuracy 0.99951\n",
      "iteration 170 \t : \t loss 0.00202 - accuracy 0.99951\n",
      "iteration 171 \t : \t loss 0.00198 - accuracy 0.99951\n",
      "iteration 172 \t : \t loss 0.00194 - accuracy 0.99951\n",
      "iteration 173 \t : \t loss 0.00191 - accuracy 0.99951\n",
      "iteration 174 \t : \t loss 0.00187 - accuracy 0.99951\n",
      "iteration 175 \t : \t loss 0.00184 - accuracy 0.99951\n",
      "iteration 176 \t : \t loss 0.00181 - accuracy 0.99951\n",
      "iteration 177 \t : \t loss 0.00177 - accuracy 0.99951\n",
      "iteration 178 \t : \t loss 0.00174 - accuracy 0.99951\n",
      "iteration 179 \t : \t loss 0.00171 - accuracy 0.99951\n",
      "iteration 180 \t : \t loss 0.00168 - accuracy 0.99951\n",
      "iteration 181 \t : \t loss 0.00165 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00163 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00160 - accuracy 0.99976\n",
      "iteration 184 \t : \t loss 0.00157 - accuracy 0.99976\n",
      "iteration 185 \t : \t loss 0.00155 - accuracy 0.99976\n",
      "iteration 186 \t : \t loss 0.00152 - accuracy 0.99976\n",
      "iteration 187 \t : \t loss 0.00150 - accuracy 0.99976\n",
      "iteration 188 \t : \t loss 0.00147 - accuracy 0.99976\n",
      "iteration 189 \t : \t loss 0.00145 - accuracy 0.99976\n",
      "iteration 190 \t : \t loss 0.00143 - accuracy 0.99976\n",
      "iteration 191 \t : \t loss 0.00141 - accuracy 0.99976\n",
      "iteration 192 \t : \t loss 0.00138 - accuracy 0.99976\n",
      "iteration 193 \t : \t loss 0.00136 - accuracy 0.99976\n",
      "iteration 194 \t : \t loss 0.00134 - accuracy 0.99976\n",
      "iteration 195 \t : \t loss 0.00132 - accuracy 0.99976\n",
      "iteration 196 \t : \t loss 0.00130 - accuracy 0.99976\n",
      "iteration 197 \t : \t loss 0.00129 - accuracy 0.99976\n",
      "iteration 198 \t : \t loss 0.00127 - accuracy 0.99976\n",
      "iteration 199 \t : \t loss 0.00125 - accuracy 0.99976\n",
      "iteration 0 \t : \t loss 0.14805 - accuracy 0.62964\n",
      "iteration 1 \t : \t loss 0.07715 - accuracy 0.84912\n",
      "iteration 2 \t : \t loss 0.05844 - accuracy 0.87183\n",
      "iteration 3 \t : \t loss 0.04980 - accuracy 0.88550\n",
      "iteration 4 \t : \t loss 0.04468 - accuracy 0.89062\n",
      "iteration 5 \t : \t loss 0.04121 - accuracy 0.89795\n",
      "iteration 6 \t : \t loss 0.03865 - accuracy 0.90088\n",
      "iteration 7 \t : \t loss 0.03664 - accuracy 0.90430\n",
      "iteration 8 \t : \t loss 0.03501 - accuracy 0.90625\n",
      "iteration 9 \t : \t loss 0.03362 - accuracy 0.90820\n",
      "iteration 10 \t : \t loss 0.03243 - accuracy 0.91162\n",
      "iteration 11 \t : \t loss 0.03138 - accuracy 0.91431\n",
      "iteration 12 \t : \t loss 0.03044 - accuracy 0.91553\n",
      "iteration 13 \t : \t loss 0.02959 - accuracy 0.91699\n",
      "iteration 14 \t : \t loss 0.02881 - accuracy 0.91919\n",
      "iteration 15 \t : \t loss 0.02809 - accuracy 0.92188\n",
      "iteration 16 \t : \t loss 0.02742 - accuracy 0.92358\n",
      "iteration 17 \t : \t loss 0.02679 - accuracy 0.92480\n",
      "iteration 18 \t : \t loss 0.02621 - accuracy 0.92627\n",
      "iteration 19 \t : \t loss 0.02565 - accuracy 0.92773\n",
      "iteration 20 \t : \t loss 0.02513 - accuracy 0.92920\n",
      "iteration 21 \t : \t loss 0.02462 - accuracy 0.93091\n",
      "iteration 22 \t : \t loss 0.02415 - accuracy 0.93164\n",
      "iteration 23 \t : \t loss 0.02369 - accuracy 0.93237\n",
      "iteration 24 \t : \t loss 0.02325 - accuracy 0.93335\n",
      "iteration 25 \t : \t loss 0.02283 - accuracy 0.93604\n",
      "iteration 26 \t : \t loss 0.02243 - accuracy 0.93701\n",
      "iteration 27 \t : \t loss 0.02203 - accuracy 0.93774\n",
      "iteration 28 \t : \t loss 0.02166 - accuracy 0.93848\n",
      "iteration 29 \t : \t loss 0.02129 - accuracy 0.93945\n",
      "iteration 30 \t : \t loss 0.02094 - accuracy 0.94067\n",
      "iteration 31 \t : \t loss 0.02060 - accuracy 0.94214\n",
      "iteration 32 \t : \t loss 0.02026 - accuracy 0.94214\n",
      "iteration 33 \t : \t loss 0.01994 - accuracy 0.94312\n",
      "iteration 34 \t : \t loss 0.01962 - accuracy 0.94434\n",
      "iteration 35 \t : \t loss 0.01932 - accuracy 0.94531\n",
      "iteration 36 \t : \t loss 0.01902 - accuracy 0.94702\n",
      "iteration 37 \t : \t loss 0.01873 - accuracy 0.94727\n",
      "iteration 38 \t : \t loss 0.01845 - accuracy 0.94849\n",
      "iteration 39 \t : \t loss 0.01817 - accuracy 0.94946\n",
      "iteration 40 \t : \t loss 0.01790 - accuracy 0.95068\n",
      "iteration 41 \t : \t loss 0.01764 - accuracy 0.95142\n",
      "iteration 42 \t : \t loss 0.01738 - accuracy 0.95288\n",
      "iteration 43 \t : \t loss 0.01713 - accuracy 0.95410\n",
      "iteration 44 \t : \t loss 0.01688 - accuracy 0.95557\n",
      "iteration 45 \t : \t loss 0.01664 - accuracy 0.95679\n",
      "iteration 46 \t : \t loss 0.01641 - accuracy 0.95801\n",
      "iteration 47 \t : \t loss 0.01618 - accuracy 0.95874\n",
      "iteration 48 \t : \t loss 0.01595 - accuracy 0.95923\n",
      "iteration 49 \t : \t loss 0.01573 - accuracy 0.95947\n",
      "iteration 50 \t : \t loss 0.01552 - accuracy 0.95996\n",
      "iteration 51 \t : \t loss 0.01531 - accuracy 0.96021\n",
      "iteration 52 \t : \t loss 0.01510 - accuracy 0.96069\n",
      "iteration 53 \t : \t loss 0.01490 - accuracy 0.96118\n",
      "iteration 54 \t : \t loss 0.01470 - accuracy 0.96143\n",
      "iteration 55 \t : \t loss 0.01450 - accuracy 0.96143\n",
      "iteration 56 \t : \t loss 0.01431 - accuracy 0.96216\n",
      "iteration 57 \t : \t loss 0.01413 - accuracy 0.96289\n",
      "iteration 58 \t : \t loss 0.01394 - accuracy 0.96289\n",
      "iteration 59 \t : \t loss 0.01376 - accuracy 0.96411\n",
      "iteration 60 \t : \t loss 0.01359 - accuracy 0.96411\n",
      "iteration 61 \t : \t loss 0.01341 - accuracy 0.96460\n",
      "iteration 62 \t : \t loss 0.01324 - accuracy 0.96558\n",
      "iteration 63 \t : \t loss 0.01307 - accuracy 0.96582\n",
      "iteration 64 \t : \t loss 0.01291 - accuracy 0.96655\n",
      "iteration 65 \t : \t loss 0.01275 - accuracy 0.96680\n",
      "iteration 66 \t : \t loss 0.01259 - accuracy 0.96729\n",
      "iteration 67 \t : \t loss 0.01243 - accuracy 0.96729\n",
      "iteration 68 \t : \t loss 0.01228 - accuracy 0.96802\n",
      "iteration 69 \t : \t loss 0.01213 - accuracy 0.96899\n",
      "iteration 70 \t : \t loss 0.01198 - accuracy 0.96973\n",
      "iteration 71 \t : \t loss 0.01183 - accuracy 0.97021\n",
      "iteration 72 \t : \t loss 0.01169 - accuracy 0.97070\n",
      "iteration 73 \t : \t loss 0.01154 - accuracy 0.97070\n",
      "iteration 74 \t : \t loss 0.01140 - accuracy 0.97192\n",
      "iteration 75 \t : \t loss 0.01127 - accuracy 0.97217\n",
      "iteration 76 \t : \t loss 0.01113 - accuracy 0.97241\n",
      "iteration 77 \t : \t loss 0.01100 - accuracy 0.97266\n",
      "iteration 78 \t : \t loss 0.01087 - accuracy 0.97339\n",
      "iteration 79 \t : \t loss 0.01074 - accuracy 0.97437\n",
      "iteration 80 \t : \t loss 0.01061 - accuracy 0.97485\n",
      "iteration 81 \t : \t loss 0.01048 - accuracy 0.97510\n",
      "iteration 82 \t : \t loss 0.01036 - accuracy 0.97534\n",
      "iteration 83 \t : \t loss 0.01024 - accuracy 0.97559\n",
      "iteration 84 \t : \t loss 0.01012 - accuracy 0.97559\n",
      "iteration 85 \t : \t loss 0.01000 - accuracy 0.97607\n",
      "iteration 86 \t : \t loss 0.00988 - accuracy 0.97681\n",
      "iteration 87 \t : \t loss 0.00977 - accuracy 0.97705\n",
      "iteration 88 \t : \t loss 0.00965 - accuracy 0.97729\n",
      "iteration 89 \t : \t loss 0.00954 - accuracy 0.97754\n",
      "iteration 90 \t : \t loss 0.00943 - accuracy 0.97827\n",
      "iteration 91 \t : \t loss 0.00932 - accuracy 0.97876\n",
      "iteration 92 \t : \t loss 0.00921 - accuracy 0.97925\n",
      "iteration 93 \t : \t loss 0.00911 - accuracy 0.97949\n",
      "iteration 94 \t : \t loss 0.00900 - accuracy 0.97998\n",
      "iteration 95 \t : \t loss 0.00890 - accuracy 0.98022\n",
      "iteration 96 \t : \t loss 0.00880 - accuracy 0.98047\n",
      "iteration 97 \t : \t loss 0.00870 - accuracy 0.98047\n",
      "iteration 98 \t : \t loss 0.00860 - accuracy 0.98120\n",
      "iteration 99 \t : \t loss 0.00850 - accuracy 0.98193\n",
      "iteration 100 \t : \t loss 0.00841 - accuracy 0.98218\n",
      "iteration 101 \t : \t loss 0.00831 - accuracy 0.98218\n",
      "iteration 102 \t : \t loss 0.00822 - accuracy 0.98218\n",
      "iteration 103 \t : \t loss 0.00812 - accuracy 0.98218\n",
      "iteration 104 \t : \t loss 0.00803 - accuracy 0.98218\n",
      "iteration 105 \t : \t loss 0.00794 - accuracy 0.98242\n",
      "iteration 106 \t : \t loss 0.00786 - accuracy 0.98340\n",
      "iteration 107 \t : \t loss 0.00777 - accuracy 0.98340\n",
      "iteration 108 \t : \t loss 0.00768 - accuracy 0.98364\n",
      "iteration 109 \t : \t loss 0.00760 - accuracy 0.98462\n",
      "iteration 110 \t : \t loss 0.00751 - accuracy 0.98486\n",
      "iteration 111 \t : \t loss 0.00743 - accuracy 0.98486\n",
      "iteration 112 \t : \t loss 0.00735 - accuracy 0.98511\n",
      "iteration 113 \t : \t loss 0.00727 - accuracy 0.98535\n",
      "iteration 114 \t : \t loss 0.00719 - accuracy 0.98560\n",
      "iteration 115 \t : \t loss 0.00711 - accuracy 0.98560\n",
      "iteration 116 \t : \t loss 0.00703 - accuracy 0.98584\n",
      "iteration 117 \t : \t loss 0.00696 - accuracy 0.98657\n",
      "iteration 118 \t : \t loss 0.00688 - accuracy 0.98657\n",
      "iteration 119 \t : \t loss 0.00681 - accuracy 0.98682\n",
      "iteration 120 \t : \t loss 0.00673 - accuracy 0.98682\n",
      "iteration 121 \t : \t loss 0.00666 - accuracy 0.98706\n",
      "iteration 122 \t : \t loss 0.00659 - accuracy 0.98730\n",
      "iteration 123 \t : \t loss 0.00652 - accuracy 0.98804\n",
      "iteration 124 \t : \t loss 0.00645 - accuracy 0.98804\n",
      "iteration 125 \t : \t loss 0.00638 - accuracy 0.98804\n",
      "iteration 126 \t : \t loss 0.00631 - accuracy 0.98853\n",
      "iteration 127 \t : \t loss 0.00624 - accuracy 0.98877\n",
      "iteration 128 \t : \t loss 0.00618 - accuracy 0.98901\n",
      "iteration 129 \t : \t loss 0.00611 - accuracy 0.98901\n",
      "iteration 130 \t : \t loss 0.00605 - accuracy 0.98926\n",
      "iteration 131 \t : \t loss 0.00598 - accuracy 0.98950\n",
      "iteration 132 \t : \t loss 0.00592 - accuracy 0.99023\n",
      "iteration 133 \t : \t loss 0.00586 - accuracy 0.99023\n",
      "iteration 134 \t : \t loss 0.00580 - accuracy 0.99072\n",
      "iteration 135 \t : \t loss 0.00574 - accuracy 0.99097\n",
      "iteration 136 \t : \t loss 0.00568 - accuracy 0.99121\n",
      "iteration 137 \t : \t loss 0.00562 - accuracy 0.99146\n",
      "iteration 138 \t : \t loss 0.00556 - accuracy 0.99146\n",
      "iteration 139 \t : \t loss 0.00550 - accuracy 0.99170\n",
      "iteration 140 \t : \t loss 0.00544 - accuracy 0.99170\n",
      "iteration 141 \t : \t loss 0.00539 - accuracy 0.99170\n",
      "iteration 142 \t : \t loss 0.00533 - accuracy 0.99219\n",
      "iteration 143 \t : \t loss 0.00528 - accuracy 0.99219\n",
      "iteration 144 \t : \t loss 0.00522 - accuracy 0.99219\n",
      "iteration 145 \t : \t loss 0.00517 - accuracy 0.99243\n",
      "iteration 146 \t : \t loss 0.00512 - accuracy 0.99243\n",
      "iteration 147 \t : \t loss 0.00506 - accuracy 0.99243\n",
      "iteration 148 \t : \t loss 0.00501 - accuracy 0.99268\n",
      "iteration 149 \t : \t loss 0.00496 - accuracy 0.99292\n",
      "iteration 150 \t : \t loss 0.00491 - accuracy 0.99292\n",
      "iteration 151 \t : \t loss 0.00486 - accuracy 0.99316\n",
      "iteration 152 \t : \t loss 0.00481 - accuracy 0.99316\n",
      "iteration 153 \t : \t loss 0.00476 - accuracy 0.99341\n",
      "iteration 154 \t : \t loss 0.00472 - accuracy 0.99365\n",
      "iteration 155 \t : \t loss 0.00467 - accuracy 0.99390\n",
      "iteration 156 \t : \t loss 0.00462 - accuracy 0.99390\n",
      "iteration 157 \t : \t loss 0.00458 - accuracy 0.99390\n",
      "iteration 158 \t : \t loss 0.00453 - accuracy 0.99414\n",
      "iteration 159 \t : \t loss 0.00449 - accuracy 0.99438\n",
      "iteration 160 \t : \t loss 0.00444 - accuracy 0.99438\n",
      "iteration 161 \t : \t loss 0.00440 - accuracy 0.99438\n",
      "iteration 162 \t : \t loss 0.00435 - accuracy 0.99438\n",
      "iteration 163 \t : \t loss 0.00431 - accuracy 0.99438\n",
      "iteration 164 \t : \t loss 0.00427 - accuracy 0.99463\n",
      "iteration 165 \t : \t loss 0.00423 - accuracy 0.99463\n",
      "iteration 166 \t : \t loss 0.00419 - accuracy 0.99463\n",
      "iteration 167 \t : \t loss 0.00415 - accuracy 0.99487\n",
      "iteration 168 \t : \t loss 0.00410 - accuracy 0.99512\n",
      "iteration 169 \t : \t loss 0.00406 - accuracy 0.99512\n",
      "iteration 170 \t : \t loss 0.00403 - accuracy 0.99512\n",
      "iteration 171 \t : \t loss 0.00399 - accuracy 0.99512\n",
      "iteration 172 \t : \t loss 0.00395 - accuracy 0.99512\n",
      "iteration 173 \t : \t loss 0.00391 - accuracy 0.99536\n",
      "iteration 174 \t : \t loss 0.00387 - accuracy 0.99536\n",
      "iteration 175 \t : \t loss 0.00384 - accuracy 0.99585\n",
      "iteration 176 \t : \t loss 0.00380 - accuracy 0.99609\n",
      "iteration 177 \t : \t loss 0.00376 - accuracy 0.99609\n",
      "iteration 178 \t : \t loss 0.00373 - accuracy 0.99609\n",
      "iteration 179 \t : \t loss 0.00369 - accuracy 0.99634\n",
      "iteration 180 \t : \t loss 0.00366 - accuracy 0.99634\n",
      "iteration 181 \t : \t loss 0.00362 - accuracy 0.99683\n",
      "iteration 182 \t : \t loss 0.00359 - accuracy 0.99683\n",
      "iteration 183 \t : \t loss 0.00355 - accuracy 0.99683\n",
      "iteration 184 \t : \t loss 0.00352 - accuracy 0.99683\n",
      "iteration 185 \t : \t loss 0.00349 - accuracy 0.99683\n",
      "iteration 186 \t : \t loss 0.00346 - accuracy 0.99683\n",
      "iteration 187 \t : \t loss 0.00342 - accuracy 0.99731\n",
      "iteration 188 \t : \t loss 0.00339 - accuracy 0.99756\n",
      "iteration 189 \t : \t loss 0.00336 - accuracy 0.99756\n",
      "iteration 190 \t : \t loss 0.00333 - accuracy 0.99756\n",
      "iteration 191 \t : \t loss 0.00330 - accuracy 0.99756\n",
      "iteration 192 \t : \t loss 0.00327 - accuracy 0.99756\n",
      "iteration 193 \t : \t loss 0.00324 - accuracy 0.99756\n",
      "iteration 194 \t : \t loss 0.00321 - accuracy 0.99756\n",
      "iteration 195 \t : \t loss 0.00318 - accuracy 0.99756\n",
      "iteration 196 \t : \t loss 0.00315 - accuracy 0.99756\n",
      "iteration 197 \t : \t loss 0.00312 - accuracy 0.99756\n",
      "iteration 198 \t : \t loss 0.00310 - accuracy 0.99756\n",
      "iteration 199 \t : \t loss 0.00307 - accuracy 0.99756\n",
      "iteration 0 \t : \t loss 0.23408 - accuracy 0.11963\n",
      "iteration 1 \t : \t loss 0.22429 - accuracy 0.16504\n",
      "iteration 2 \t : \t loss 0.21102 - accuracy 0.26636\n",
      "iteration 3 \t : \t loss 0.18952 - accuracy 0.43066\n",
      "iteration 4 \t : \t loss 0.15944 - accuracy 0.55713\n",
      "iteration 5 \t : \t loss 0.12944 - accuracy 0.65234\n",
      "iteration 6 \t : \t loss 0.10636 - accuracy 0.71436\n",
      "iteration 7 \t : \t loss 0.09006 - accuracy 0.76172\n",
      "iteration 8 \t : \t loss 0.07847 - accuracy 0.79175\n",
      "iteration 9 \t : \t loss 0.06997 - accuracy 0.81299\n",
      "iteration 10 \t : \t loss 0.06353 - accuracy 0.82861\n",
      "iteration 11 \t : \t loss 0.05851 - accuracy 0.84253\n",
      "iteration 12 \t : \t loss 0.05448 - accuracy 0.85498\n",
      "iteration 13 \t : \t loss 0.05119 - accuracy 0.86304\n",
      "iteration 14 \t : \t loss 0.04842 - accuracy 0.87012\n",
      "iteration 15 \t : \t loss 0.04607 - accuracy 0.87451\n",
      "iteration 16 \t : \t loss 0.04404 - accuracy 0.87964\n",
      "iteration 17 \t : \t loss 0.04227 - accuracy 0.88550\n",
      "iteration 18 \t : \t loss 0.04070 - accuracy 0.88745\n",
      "iteration 19 \t : \t loss 0.03930 - accuracy 0.88916\n",
      "iteration 20 \t : \t loss 0.03804 - accuracy 0.88916\n",
      "iteration 21 \t : \t loss 0.03690 - accuracy 0.89136\n",
      "iteration 22 \t : \t loss 0.03585 - accuracy 0.89380\n",
      "iteration 23 \t : \t loss 0.03488 - accuracy 0.89795\n",
      "iteration 24 \t : \t loss 0.03398 - accuracy 0.90112\n",
      "iteration 25 \t : \t loss 0.03313 - accuracy 0.90356\n",
      "iteration 26 \t : \t loss 0.03234 - accuracy 0.90527\n",
      "iteration 27 \t : \t loss 0.03159 - accuracy 0.90674\n",
      "iteration 28 \t : \t loss 0.03087 - accuracy 0.90918\n",
      "iteration 29 \t : \t loss 0.03019 - accuracy 0.91211\n",
      "iteration 30 \t : \t loss 0.02954 - accuracy 0.91406\n",
      "iteration 31 \t : \t loss 0.02891 - accuracy 0.91699\n",
      "iteration 32 \t : \t loss 0.02831 - accuracy 0.91846\n",
      "iteration 33 \t : \t loss 0.02773 - accuracy 0.92017\n",
      "iteration 34 \t : \t loss 0.02717 - accuracy 0.92236\n",
      "iteration 35 \t : \t loss 0.02663 - accuracy 0.92407\n",
      "iteration 36 \t : \t loss 0.02610 - accuracy 0.92603\n",
      "iteration 37 \t : \t loss 0.02559 - accuracy 0.92676\n",
      "iteration 38 \t : \t loss 0.02509 - accuracy 0.92822\n",
      "iteration 39 \t : \t loss 0.02461 - accuracy 0.93091\n",
      "iteration 40 \t : \t loss 0.02414 - accuracy 0.93213\n",
      "iteration 41 \t : \t loss 0.02368 - accuracy 0.93359\n",
      "iteration 42 \t : \t loss 0.02323 - accuracy 0.93506\n",
      "iteration 43 \t : \t loss 0.02279 - accuracy 0.93677\n",
      "iteration 44 \t : \t loss 0.02236 - accuracy 0.93872\n",
      "iteration 45 \t : \t loss 0.02194 - accuracy 0.94092\n",
      "iteration 46 \t : \t loss 0.02152 - accuracy 0.94238\n",
      "iteration 47 \t : \t loss 0.02112 - accuracy 0.94312\n",
      "iteration 48 \t : \t loss 0.02072 - accuracy 0.94531\n",
      "iteration 49 \t : \t loss 0.02033 - accuracy 0.94653\n",
      "iteration 50 \t : \t loss 0.01995 - accuracy 0.94775\n",
      "iteration 51 \t : \t loss 0.01958 - accuracy 0.94897\n",
      "iteration 52 \t : \t loss 0.01921 - accuracy 0.94995\n",
      "iteration 53 \t : \t loss 0.01884 - accuracy 0.95093\n",
      "iteration 54 \t : \t loss 0.01849 - accuracy 0.95239\n",
      "iteration 55 \t : \t loss 0.01814 - accuracy 0.95361\n",
      "iteration 56 \t : \t loss 0.01779 - accuracy 0.95508\n",
      "iteration 57 \t : \t loss 0.01745 - accuracy 0.95581\n",
      "iteration 58 \t : \t loss 0.01711 - accuracy 0.95728\n",
      "iteration 59 \t : \t loss 0.01678 - accuracy 0.95825\n",
      "iteration 60 \t : \t loss 0.01646 - accuracy 0.95923\n",
      "iteration 61 \t : \t loss 0.01614 - accuracy 0.96094\n",
      "iteration 62 \t : \t loss 0.01582 - accuracy 0.96216\n",
      "iteration 63 \t : \t loss 0.01551 - accuracy 0.96265\n",
      "iteration 64 \t : \t loss 0.01520 - accuracy 0.96362\n",
      "iteration 65 \t : \t loss 0.01490 - accuracy 0.96484\n",
      "iteration 66 \t : \t loss 0.01460 - accuracy 0.96680\n",
      "iteration 67 \t : \t loss 0.01431 - accuracy 0.96924\n",
      "iteration 68 \t : \t loss 0.01402 - accuracy 0.97021\n",
      "iteration 69 \t : \t loss 0.01373 - accuracy 0.97119\n",
      "iteration 70 \t : \t loss 0.01345 - accuracy 0.97241\n",
      "iteration 71 \t : \t loss 0.01318 - accuracy 0.97363\n",
      "iteration 72 \t : \t loss 0.01290 - accuracy 0.97437\n",
      "iteration 73 \t : \t loss 0.01263 - accuracy 0.97510\n",
      "iteration 74 \t : \t loss 0.01237 - accuracy 0.97583\n",
      "iteration 75 \t : \t loss 0.01211 - accuracy 0.97681\n",
      "iteration 76 \t : \t loss 0.01185 - accuracy 0.97803\n",
      "iteration 77 \t : \t loss 0.01160 - accuracy 0.97827\n",
      "iteration 78 \t : \t loss 0.01135 - accuracy 0.97925\n",
      "iteration 79 \t : \t loss 0.01110 - accuracy 0.97998\n",
      "iteration 80 \t : \t loss 0.01086 - accuracy 0.98096\n",
      "iteration 81 \t : \t loss 0.01062 - accuracy 0.98193\n",
      "iteration 82 \t : \t loss 0.01039 - accuracy 0.98242\n",
      "iteration 83 \t : \t loss 0.01016 - accuracy 0.98315\n",
      "iteration 84 \t : \t loss 0.00993 - accuracy 0.98340\n",
      "iteration 85 \t : \t loss 0.00971 - accuracy 0.98389\n",
      "iteration 86 \t : \t loss 0.00949 - accuracy 0.98462\n",
      "iteration 87 \t : \t loss 0.00927 - accuracy 0.98486\n",
      "iteration 88 \t : \t loss 0.00906 - accuracy 0.98486\n",
      "iteration 89 \t : \t loss 0.00885 - accuracy 0.98511\n",
      "iteration 90 \t : \t loss 0.00865 - accuracy 0.98511\n",
      "iteration 91 \t : \t loss 0.00845 - accuracy 0.98535\n",
      "iteration 92 \t : \t loss 0.00825 - accuracy 0.98584\n",
      "iteration 93 \t : \t loss 0.00805 - accuracy 0.98657\n",
      "iteration 94 \t : \t loss 0.00786 - accuracy 0.98682\n",
      "iteration 95 \t : \t loss 0.00768 - accuracy 0.98755\n",
      "iteration 96 \t : \t loss 0.00749 - accuracy 0.98853\n",
      "iteration 97 \t : \t loss 0.00731 - accuracy 0.98901\n",
      "iteration 98 \t : \t loss 0.00714 - accuracy 0.98926\n",
      "iteration 99 \t : \t loss 0.00697 - accuracy 0.98926\n",
      "iteration 100 \t : \t loss 0.00680 - accuracy 0.98926\n",
      "iteration 101 \t : \t loss 0.00663 - accuracy 0.98926\n",
      "iteration 102 \t : \t loss 0.00647 - accuracy 0.98975\n",
      "iteration 103 \t : \t loss 0.00631 - accuracy 0.98999\n",
      "iteration 104 \t : \t loss 0.00616 - accuracy 0.99048\n",
      "iteration 105 \t : \t loss 0.00601 - accuracy 0.99048\n",
      "iteration 106 \t : \t loss 0.00586 - accuracy 0.99097\n",
      "iteration 107 \t : \t loss 0.00572 - accuracy 0.99097\n",
      "iteration 108 \t : \t loss 0.00558 - accuracy 0.99121\n",
      "iteration 109 \t : \t loss 0.00544 - accuracy 0.99146\n",
      "iteration 110 \t : \t loss 0.00531 - accuracy 0.99146\n",
      "iteration 111 \t : \t loss 0.00518 - accuracy 0.99219\n",
      "iteration 112 \t : \t loss 0.00505 - accuracy 0.99268\n",
      "iteration 113 \t : \t loss 0.00492 - accuracy 0.99341\n",
      "iteration 114 \t : \t loss 0.00480 - accuracy 0.99463\n",
      "iteration 115 \t : \t loss 0.00469 - accuracy 0.99536\n",
      "iteration 116 \t : \t loss 0.00457 - accuracy 0.99536\n",
      "iteration 117 \t : \t loss 0.00446 - accuracy 0.99561\n",
      "iteration 118 \t : \t loss 0.00435 - accuracy 0.99634\n",
      "iteration 119 \t : \t loss 0.00425 - accuracy 0.99658\n",
      "iteration 120 \t : \t loss 0.00414 - accuracy 0.99658\n",
      "iteration 121 \t : \t loss 0.00404 - accuracy 0.99707\n",
      "iteration 122 \t : \t loss 0.00395 - accuracy 0.99707\n",
      "iteration 123 \t : \t loss 0.00385 - accuracy 0.99707\n",
      "iteration 124 \t : \t loss 0.00376 - accuracy 0.99707\n",
      "iteration 125 \t : \t loss 0.00367 - accuracy 0.99731\n",
      "iteration 126 \t : \t loss 0.00359 - accuracy 0.99756\n",
      "iteration 127 \t : \t loss 0.00350 - accuracy 0.99756\n",
      "iteration 128 \t : \t loss 0.00342 - accuracy 0.99756\n",
      "iteration 129 \t : \t loss 0.00334 - accuracy 0.99780\n",
      "iteration 130 \t : \t loss 0.00327 - accuracy 0.99805\n",
      "iteration 131 \t : \t loss 0.00319 - accuracy 0.99854\n",
      "iteration 132 \t : \t loss 0.00312 - accuracy 0.99854\n",
      "iteration 133 \t : \t loss 0.00305 - accuracy 0.99854\n",
      "iteration 134 \t : \t loss 0.00299 - accuracy 0.99854\n",
      "iteration 135 \t : \t loss 0.00292 - accuracy 0.99854\n",
      "iteration 136 \t : \t loss 0.00286 - accuracy 0.99878\n",
      "iteration 137 \t : \t loss 0.00279 - accuracy 0.99878\n",
      "iteration 138 \t : \t loss 0.00273 - accuracy 0.99878\n",
      "iteration 139 \t : \t loss 0.00268 - accuracy 0.99878\n",
      "iteration 140 \t : \t loss 0.00262 - accuracy 0.99878\n",
      "iteration 141 \t : \t loss 0.00256 - accuracy 0.99878\n",
      "iteration 142 \t : \t loss 0.00251 - accuracy 0.99878\n",
      "iteration 143 \t : \t loss 0.00246 - accuracy 0.99878\n",
      "iteration 144 \t : \t loss 0.00241 - accuracy 0.99902\n",
      "iteration 145 \t : \t loss 0.00236 - accuracy 0.99927\n",
      "iteration 146 \t : \t loss 0.00231 - accuracy 0.99927\n",
      "iteration 147 \t : \t loss 0.00227 - accuracy 0.99927\n",
      "iteration 148 \t : \t loss 0.00222 - accuracy 0.99927\n",
      "iteration 149 \t : \t loss 0.00218 - accuracy 0.99927\n",
      "iteration 150 \t : \t loss 0.00213 - accuracy 0.99927\n",
      "iteration 151 \t : \t loss 0.00209 - accuracy 0.99927\n",
      "iteration 152 \t : \t loss 0.00205 - accuracy 0.99927\n",
      "iteration 153 \t : \t loss 0.00201 - accuracy 0.99927\n",
      "iteration 154 \t : \t loss 0.00198 - accuracy 0.99927\n",
      "iteration 155 \t : \t loss 0.00194 - accuracy 0.99927\n",
      "iteration 156 \t : \t loss 0.00190 - accuracy 0.99927\n",
      "iteration 157 \t : \t loss 0.00187 - accuracy 0.99927\n",
      "iteration 158 \t : \t loss 0.00183 - accuracy 0.99927\n",
      "iteration 159 \t : \t loss 0.00180 - accuracy 0.99927\n",
      "iteration 160 \t : \t loss 0.00177 - accuracy 0.99927\n",
      "iteration 161 \t : \t loss 0.00174 - accuracy 0.99951\n",
      "iteration 162 \t : \t loss 0.00171 - accuracy 0.99951\n",
      "iteration 163 \t : \t loss 0.00168 - accuracy 0.99976\n",
      "iteration 164 \t : \t loss 0.00165 - accuracy 0.99976\n",
      "iteration 165 \t : \t loss 0.00162 - accuracy 0.99976\n",
      "iteration 166 \t : \t loss 0.00159 - accuracy 0.99976\n",
      "iteration 167 \t : \t loss 0.00156 - accuracy 0.99976\n",
      "iteration 168 \t : \t loss 0.00154 - accuracy 0.99976\n",
      "iteration 169 \t : \t loss 0.00151 - accuracy 0.99976\n",
      "iteration 170 \t : \t loss 0.00149 - accuracy 0.99976\n",
      "iteration 171 \t : \t loss 0.00146 - accuracy 0.99976\n",
      "iteration 172 \t : \t loss 0.00144 - accuracy 0.99976\n",
      "iteration 173 \t : \t loss 0.00142 - accuracy 0.99976\n",
      "iteration 174 \t : \t loss 0.00139 - accuracy 0.99976\n",
      "iteration 175 \t : \t loss 0.00137 - accuracy 0.99976\n",
      "iteration 176 \t : \t loss 0.00135 - accuracy 0.99976\n",
      "iteration 177 \t : \t loss 0.00133 - accuracy 0.99976\n",
      "iteration 178 \t : \t loss 0.00131 - accuracy 0.99976\n",
      "iteration 179 \t : \t loss 0.00129 - accuracy 0.99976\n",
      "iteration 180 \t : \t loss 0.00127 - accuracy 0.99976\n",
      "iteration 181 \t : \t loss 0.00125 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00123 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00118 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00112 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00110 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00107 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00106 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00104 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00103 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00102 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00100 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00099 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.11437 - accuracy 0.68042\n",
      "iteration 1 \t : \t loss 0.05354 - accuracy 0.88062\n",
      "iteration 2 \t : \t loss 0.04226 - accuracy 0.90137\n",
      "iteration 3 \t : \t loss 0.03688 - accuracy 0.91040\n",
      "iteration 4 \t : \t loss 0.03351 - accuracy 0.91528\n",
      "iteration 5 \t : \t loss 0.03110 - accuracy 0.92139\n",
      "iteration 6 \t : \t loss 0.02923 - accuracy 0.92432\n",
      "iteration 7 \t : \t loss 0.02771 - accuracy 0.92896\n",
      "iteration 8 \t : \t loss 0.02643 - accuracy 0.93359\n",
      "iteration 9 \t : \t loss 0.02532 - accuracy 0.93579\n",
      "iteration 10 \t : \t loss 0.02434 - accuracy 0.93750\n",
      "iteration 11 \t : \t loss 0.02345 - accuracy 0.93872\n",
      "iteration 12 \t : \t loss 0.02265 - accuracy 0.94043\n",
      "iteration 13 \t : \t loss 0.02192 - accuracy 0.94263\n",
      "iteration 14 \t : \t loss 0.02124 - accuracy 0.94458\n",
      "iteration 15 \t : \t loss 0.02060 - accuracy 0.94629\n",
      "iteration 16 \t : \t loss 0.02001 - accuracy 0.94727\n",
      "iteration 17 \t : \t loss 0.01945 - accuracy 0.94946\n",
      "iteration 18 \t : \t loss 0.01892 - accuracy 0.95117\n",
      "iteration 19 \t : \t loss 0.01842 - accuracy 0.95215\n",
      "iteration 20 \t : \t loss 0.01795 - accuracy 0.95410\n",
      "iteration 21 \t : \t loss 0.01749 - accuracy 0.95508\n",
      "iteration 22 \t : \t loss 0.01706 - accuracy 0.95679\n",
      "iteration 23 \t : \t loss 0.01665 - accuracy 0.95776\n",
      "iteration 24 \t : \t loss 0.01625 - accuracy 0.95898\n",
      "iteration 25 \t : \t loss 0.01586 - accuracy 0.96021\n",
      "iteration 26 \t : \t loss 0.01550 - accuracy 0.96118\n",
      "iteration 27 \t : \t loss 0.01514 - accuracy 0.96216\n",
      "iteration 28 \t : \t loss 0.01480 - accuracy 0.96338\n",
      "iteration 29 \t : \t loss 0.01447 - accuracy 0.96533\n",
      "iteration 30 \t : \t loss 0.01415 - accuracy 0.96582\n",
      "iteration 31 \t : \t loss 0.01384 - accuracy 0.96606\n",
      "iteration 32 \t : \t loss 0.01354 - accuracy 0.96777\n",
      "iteration 33 \t : \t loss 0.01324 - accuracy 0.96826\n",
      "iteration 34 \t : \t loss 0.01296 - accuracy 0.96875\n",
      "iteration 35 \t : \t loss 0.01269 - accuracy 0.97021\n",
      "iteration 36 \t : \t loss 0.01242 - accuracy 0.97144\n",
      "iteration 37 \t : \t loss 0.01216 - accuracy 0.97241\n",
      "iteration 38 \t : \t loss 0.01191 - accuracy 0.97290\n",
      "iteration 39 \t : \t loss 0.01167 - accuracy 0.97388\n",
      "iteration 40 \t : \t loss 0.01143 - accuracy 0.97510\n",
      "iteration 41 \t : \t loss 0.01120 - accuracy 0.97534\n",
      "iteration 42 \t : \t loss 0.01097 - accuracy 0.97632\n",
      "iteration 43 \t : \t loss 0.01075 - accuracy 0.97681\n",
      "iteration 44 \t : \t loss 0.01054 - accuracy 0.97705\n",
      "iteration 45 \t : \t loss 0.01033 - accuracy 0.97827\n",
      "iteration 46 \t : \t loss 0.01013 - accuracy 0.97876\n",
      "iteration 47 \t : \t loss 0.00993 - accuracy 0.97949\n",
      "iteration 48 \t : \t loss 0.00974 - accuracy 0.97949\n",
      "iteration 49 \t : \t loss 0.00955 - accuracy 0.97974\n",
      "iteration 50 \t : \t loss 0.00936 - accuracy 0.97998\n",
      "iteration 51 \t : \t loss 0.00918 - accuracy 0.98071\n",
      "iteration 52 \t : \t loss 0.00901 - accuracy 0.98096\n",
      "iteration 53 \t : \t loss 0.00884 - accuracy 0.98120\n",
      "iteration 54 \t : \t loss 0.00867 - accuracy 0.98193\n",
      "iteration 55 \t : \t loss 0.00851 - accuracy 0.98267\n",
      "iteration 56 \t : \t loss 0.00835 - accuracy 0.98340\n",
      "iteration 57 \t : \t loss 0.00819 - accuracy 0.98389\n",
      "iteration 58 \t : \t loss 0.00804 - accuracy 0.98462\n",
      "iteration 59 \t : \t loss 0.00789 - accuracy 0.98535\n",
      "iteration 60 \t : \t loss 0.00774 - accuracy 0.98584\n",
      "iteration 61 \t : \t loss 0.00760 - accuracy 0.98584\n",
      "iteration 62 \t : \t loss 0.00746 - accuracy 0.98657\n",
      "iteration 63 \t : \t loss 0.00733 - accuracy 0.98682\n",
      "iteration 64 \t : \t loss 0.00719 - accuracy 0.98730\n",
      "iteration 65 \t : \t loss 0.00706 - accuracy 0.98828\n",
      "iteration 66 \t : \t loss 0.00694 - accuracy 0.98877\n",
      "iteration 67 \t : \t loss 0.00681 - accuracy 0.98877\n",
      "iteration 68 \t : \t loss 0.00669 - accuracy 0.98926\n",
      "iteration 69 \t : \t loss 0.00657 - accuracy 0.98975\n",
      "iteration 70 \t : \t loss 0.00646 - accuracy 0.98975\n",
      "iteration 71 \t : \t loss 0.00634 - accuracy 0.99048\n",
      "iteration 72 \t : \t loss 0.00623 - accuracy 0.99048\n",
      "iteration 73 \t : \t loss 0.00612 - accuracy 0.99048\n",
      "iteration 74 \t : \t loss 0.00601 - accuracy 0.99072\n",
      "iteration 75 \t : \t loss 0.00591 - accuracy 0.99146\n",
      "iteration 76 \t : \t loss 0.00581 - accuracy 0.99170\n",
      "iteration 77 \t : \t loss 0.00571 - accuracy 0.99219\n",
      "iteration 78 \t : \t loss 0.00561 - accuracy 0.99243\n",
      "iteration 79 \t : \t loss 0.00551 - accuracy 0.99243\n",
      "iteration 80 \t : \t loss 0.00542 - accuracy 0.99268\n",
      "iteration 81 \t : \t loss 0.00533 - accuracy 0.99268\n",
      "iteration 82 \t : \t loss 0.00524 - accuracy 0.99316\n",
      "iteration 83 \t : \t loss 0.00515 - accuracy 0.99316\n",
      "iteration 84 \t : \t loss 0.00506 - accuracy 0.99341\n",
      "iteration 85 \t : \t loss 0.00498 - accuracy 0.99341\n",
      "iteration 86 \t : \t loss 0.00490 - accuracy 0.99365\n",
      "iteration 87 \t : \t loss 0.00482 - accuracy 0.99365\n",
      "iteration 88 \t : \t loss 0.00474 - accuracy 0.99365\n",
      "iteration 89 \t : \t loss 0.00466 - accuracy 0.99390\n",
      "iteration 90 \t : \t loss 0.00458 - accuracy 0.99414\n",
      "iteration 91 \t : \t loss 0.00451 - accuracy 0.99414\n",
      "iteration 92 \t : \t loss 0.00443 - accuracy 0.99414\n",
      "iteration 93 \t : \t loss 0.00436 - accuracy 0.99438\n",
      "iteration 94 \t : \t loss 0.00429 - accuracy 0.99487\n",
      "iteration 95 \t : \t loss 0.00422 - accuracy 0.99487\n",
      "iteration 96 \t : \t loss 0.00416 - accuracy 0.99512\n",
      "iteration 97 \t : \t loss 0.00409 - accuracy 0.99512\n",
      "iteration 98 \t : \t loss 0.00403 - accuracy 0.99512\n",
      "iteration 99 \t : \t loss 0.00396 - accuracy 0.99536\n",
      "iteration 100 \t : \t loss 0.00390 - accuracy 0.99561\n",
      "iteration 101 \t : \t loss 0.00384 - accuracy 0.99561\n",
      "iteration 102 \t : \t loss 0.00378 - accuracy 0.99561\n",
      "iteration 103 \t : \t loss 0.00372 - accuracy 0.99585\n",
      "iteration 104 \t : \t loss 0.00367 - accuracy 0.99585\n",
      "iteration 105 \t : \t loss 0.00361 - accuracy 0.99609\n",
      "iteration 106 \t : \t loss 0.00356 - accuracy 0.99609\n",
      "iteration 107 \t : \t loss 0.00350 - accuracy 0.99658\n",
      "iteration 108 \t : \t loss 0.00345 - accuracy 0.99658\n",
      "iteration 109 \t : \t loss 0.00340 - accuracy 0.99658\n",
      "iteration 110 \t : \t loss 0.00335 - accuracy 0.99658\n",
      "iteration 111 \t : \t loss 0.00330 - accuracy 0.99658\n",
      "iteration 112 \t : \t loss 0.00325 - accuracy 0.99658\n",
      "iteration 113 \t : \t loss 0.00320 - accuracy 0.99658\n",
      "iteration 114 \t : \t loss 0.00316 - accuracy 0.99683\n",
      "iteration 115 \t : \t loss 0.00311 - accuracy 0.99683\n",
      "iteration 116 \t : \t loss 0.00307 - accuracy 0.99683\n",
      "iteration 117 \t : \t loss 0.00302 - accuracy 0.99683\n",
      "iteration 118 \t : \t loss 0.00298 - accuracy 0.99683\n",
      "iteration 119 \t : \t loss 0.00294 - accuracy 0.99707\n",
      "iteration 120 \t : \t loss 0.00290 - accuracy 0.99756\n",
      "iteration 121 \t : \t loss 0.00286 - accuracy 0.99756\n",
      "iteration 122 \t : \t loss 0.00282 - accuracy 0.99756\n",
      "iteration 123 \t : \t loss 0.00278 - accuracy 0.99756\n",
      "iteration 124 \t : \t loss 0.00274 - accuracy 0.99756\n",
      "iteration 125 \t : \t loss 0.00270 - accuracy 0.99780\n",
      "iteration 126 \t : \t loss 0.00267 - accuracy 0.99780\n",
      "iteration 127 \t : \t loss 0.00263 - accuracy 0.99780\n",
      "iteration 128 \t : \t loss 0.00260 - accuracy 0.99829\n",
      "iteration 129 \t : \t loss 0.00256 - accuracy 0.99829\n",
      "iteration 130 \t : \t loss 0.00253 - accuracy 0.99854\n",
      "iteration 131 \t : \t loss 0.00250 - accuracy 0.99854\n",
      "iteration 132 \t : \t loss 0.00246 - accuracy 0.99854\n",
      "iteration 133 \t : \t loss 0.00243 - accuracy 0.99854\n",
      "iteration 134 \t : \t loss 0.00240 - accuracy 0.99854\n",
      "iteration 135 \t : \t loss 0.00237 - accuracy 0.99854\n",
      "iteration 136 \t : \t loss 0.00234 - accuracy 0.99854\n",
      "iteration 137 \t : \t loss 0.00231 - accuracy 0.99878\n",
      "iteration 138 \t : \t loss 0.00228 - accuracy 0.99878\n",
      "iteration 139 \t : \t loss 0.00225 - accuracy 0.99878\n",
      "iteration 140 \t : \t loss 0.00223 - accuracy 0.99878\n",
      "iteration 141 \t : \t loss 0.00220 - accuracy 0.99902\n",
      "iteration 142 \t : \t loss 0.00217 - accuracy 0.99902\n",
      "iteration 143 \t : \t loss 0.00215 - accuracy 0.99902\n",
      "iteration 144 \t : \t loss 0.00212 - accuracy 0.99902\n",
      "iteration 145 \t : \t loss 0.00210 - accuracy 0.99902\n",
      "iteration 146 \t : \t loss 0.00207 - accuracy 0.99902\n",
      "iteration 147 \t : \t loss 0.00205 - accuracy 0.99902\n",
      "iteration 148 \t : \t loss 0.00202 - accuracy 0.99902\n",
      "iteration 149 \t : \t loss 0.00200 - accuracy 0.99902\n",
      "iteration 150 \t : \t loss 0.00198 - accuracy 0.99902\n",
      "iteration 151 \t : \t loss 0.00196 - accuracy 0.99927\n",
      "iteration 152 \t : \t loss 0.00193 - accuracy 0.99927\n",
      "iteration 153 \t : \t loss 0.00191 - accuracy 0.99951\n",
      "iteration 154 \t : \t loss 0.00189 - accuracy 0.99951\n",
      "iteration 155 \t : \t loss 0.00187 - accuracy 0.99951\n",
      "iteration 156 \t : \t loss 0.00185 - accuracy 0.99951\n",
      "iteration 157 \t : \t loss 0.00183 - accuracy 0.99976\n",
      "iteration 158 \t : \t loss 0.00181 - accuracy 0.99976\n",
      "iteration 159 \t : \t loss 0.00179 - accuracy 0.99976\n",
      "iteration 160 \t : \t loss 0.00177 - accuracy 0.99976\n",
      "iteration 161 \t : \t loss 0.00175 - accuracy 0.99976\n",
      "iteration 162 \t : \t loss 0.00173 - accuracy 0.99976\n",
      "iteration 163 \t : \t loss 0.00172 - accuracy 0.99976\n",
      "iteration 164 \t : \t loss 0.00170 - accuracy 0.99976\n",
      "iteration 165 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00166 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00165 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00163 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00161 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00160 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00157 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00155 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00154 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00152 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00151 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00149 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00148 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00146 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00145 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00142 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00140 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00139 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00136 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00134 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00130 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00128 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00123 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.25417 - accuracy 0.13525\n",
      "iteration 1 \t : \t loss 0.20175 - accuracy 0.29199\n",
      "iteration 2 \t : \t loss 0.15309 - accuracy 0.53418\n",
      "iteration 3 \t : \t loss 0.11249 - accuracy 0.67700\n",
      "iteration 4 \t : \t loss 0.08779 - accuracy 0.74487\n",
      "iteration 5 \t : \t loss 0.07336 - accuracy 0.78540\n",
      "iteration 6 \t : \t loss 0.06420 - accuracy 0.81079\n",
      "iteration 7 \t : \t loss 0.05782 - accuracy 0.83130\n",
      "iteration 8 \t : \t loss 0.05306 - accuracy 0.84717\n",
      "iteration 9 \t : \t loss 0.04933 - accuracy 0.85840\n",
      "iteration 10 \t : \t loss 0.04632 - accuracy 0.86792\n",
      "iteration 11 \t : \t loss 0.04381 - accuracy 0.87622\n",
      "iteration 12 \t : \t loss 0.04170 - accuracy 0.88208\n",
      "iteration 13 \t : \t loss 0.03988 - accuracy 0.88892\n",
      "iteration 14 \t : \t loss 0.03828 - accuracy 0.89209\n",
      "iteration 15 \t : \t loss 0.03687 - accuracy 0.89380\n",
      "iteration 16 \t : \t loss 0.03560 - accuracy 0.89819\n",
      "iteration 17 \t : \t loss 0.03445 - accuracy 0.89990\n",
      "iteration 18 \t : \t loss 0.03339 - accuracy 0.90161\n",
      "iteration 19 \t : \t loss 0.03241 - accuracy 0.90479\n",
      "iteration 20 \t : \t loss 0.03149 - accuracy 0.90796\n",
      "iteration 21 \t : \t loss 0.03064 - accuracy 0.91113\n",
      "iteration 22 \t : \t loss 0.02983 - accuracy 0.91357\n",
      "iteration 23 \t : \t loss 0.02907 - accuracy 0.91650\n",
      "iteration 24 \t : \t loss 0.02834 - accuracy 0.91919\n",
      "iteration 25 \t : \t loss 0.02765 - accuracy 0.92090\n",
      "iteration 26 \t : \t loss 0.02699 - accuracy 0.92310\n",
      "iteration 27 \t : \t loss 0.02635 - accuracy 0.92627\n",
      "iteration 28 \t : \t loss 0.02575 - accuracy 0.92896\n",
      "iteration 29 \t : \t loss 0.02516 - accuracy 0.93042\n",
      "iteration 30 \t : \t loss 0.02460 - accuracy 0.93213\n",
      "iteration 31 \t : \t loss 0.02405 - accuracy 0.93335\n",
      "iteration 32 \t : \t loss 0.02352 - accuracy 0.93506\n",
      "iteration 33 \t : \t loss 0.02301 - accuracy 0.93579\n",
      "iteration 34 \t : \t loss 0.02252 - accuracy 0.93896\n",
      "iteration 35 \t : \t loss 0.02204 - accuracy 0.94019\n",
      "iteration 36 \t : \t loss 0.02157 - accuracy 0.94092\n",
      "iteration 37 \t : \t loss 0.02112 - accuracy 0.94165\n",
      "iteration 38 \t : \t loss 0.02067 - accuracy 0.94312\n",
      "iteration 39 \t : \t loss 0.02024 - accuracy 0.94434\n",
      "iteration 40 \t : \t loss 0.01982 - accuracy 0.94604\n",
      "iteration 41 \t : \t loss 0.01941 - accuracy 0.94751\n",
      "iteration 42 \t : \t loss 0.01900 - accuracy 0.94873\n",
      "iteration 43 \t : \t loss 0.01861 - accuracy 0.94946\n",
      "iteration 44 \t : \t loss 0.01822 - accuracy 0.95117\n",
      "iteration 45 \t : \t loss 0.01784 - accuracy 0.95288\n",
      "iteration 46 \t : \t loss 0.01747 - accuracy 0.95435\n",
      "iteration 47 \t : \t loss 0.01711 - accuracy 0.95581\n",
      "iteration 48 \t : \t loss 0.01675 - accuracy 0.95752\n",
      "iteration 49 \t : \t loss 0.01640 - accuracy 0.95898\n",
      "iteration 50 \t : \t loss 0.01606 - accuracy 0.96045\n",
      "iteration 51 \t : \t loss 0.01572 - accuracy 0.96094\n",
      "iteration 52 \t : \t loss 0.01539 - accuracy 0.96240\n",
      "iteration 53 \t : \t loss 0.01506 - accuracy 0.96387\n",
      "iteration 54 \t : \t loss 0.01474 - accuracy 0.96582\n",
      "iteration 55 \t : \t loss 0.01443 - accuracy 0.96631\n",
      "iteration 56 \t : \t loss 0.01412 - accuracy 0.96704\n",
      "iteration 57 \t : \t loss 0.01381 - accuracy 0.96802\n",
      "iteration 58 \t : \t loss 0.01351 - accuracy 0.96973\n",
      "iteration 59 \t : \t loss 0.01322 - accuracy 0.97021\n",
      "iteration 60 \t : \t loss 0.01293 - accuracy 0.97119\n",
      "iteration 61 \t : \t loss 0.01264 - accuracy 0.97217\n",
      "iteration 62 \t : \t loss 0.01236 - accuracy 0.97241\n",
      "iteration 63 \t : \t loss 0.01208 - accuracy 0.97290\n",
      "iteration 64 \t : \t loss 0.01181 - accuracy 0.97461\n",
      "iteration 65 \t : \t loss 0.01154 - accuracy 0.97534\n",
      "iteration 66 \t : \t loss 0.01128 - accuracy 0.97632\n",
      "iteration 67 \t : \t loss 0.01102 - accuracy 0.97729\n",
      "iteration 68 \t : \t loss 0.01077 - accuracy 0.97876\n",
      "iteration 69 \t : \t loss 0.01052 - accuracy 0.97876\n",
      "iteration 70 \t : \t loss 0.01027 - accuracy 0.97974\n",
      "iteration 71 \t : \t loss 0.01003 - accuracy 0.98071\n",
      "iteration 72 \t : \t loss 0.00979 - accuracy 0.98120\n",
      "iteration 73 \t : \t loss 0.00955 - accuracy 0.98193\n",
      "iteration 74 \t : \t loss 0.00932 - accuracy 0.98267\n",
      "iteration 75 \t : \t loss 0.00909 - accuracy 0.98291\n",
      "iteration 76 \t : \t loss 0.00887 - accuracy 0.98340\n",
      "iteration 77 \t : \t loss 0.00865 - accuracy 0.98364\n",
      "iteration 78 \t : \t loss 0.00844 - accuracy 0.98413\n",
      "iteration 79 \t : \t loss 0.00823 - accuracy 0.98438\n",
      "iteration 80 \t : \t loss 0.00802 - accuracy 0.98462\n",
      "iteration 81 \t : \t loss 0.00782 - accuracy 0.98511\n",
      "iteration 82 \t : \t loss 0.00762 - accuracy 0.98560\n",
      "iteration 83 \t : \t loss 0.00743 - accuracy 0.98584\n",
      "iteration 84 \t : \t loss 0.00724 - accuracy 0.98706\n",
      "iteration 85 \t : \t loss 0.00705 - accuracy 0.98804\n",
      "iteration 86 \t : \t loss 0.00687 - accuracy 0.98901\n",
      "iteration 87 \t : \t loss 0.00669 - accuracy 0.98926\n",
      "iteration 88 \t : \t loss 0.00652 - accuracy 0.98950\n",
      "iteration 89 \t : \t loss 0.00635 - accuracy 0.99023\n",
      "iteration 90 \t : \t loss 0.00618 - accuracy 0.99048\n",
      "iteration 91 \t : \t loss 0.00602 - accuracy 0.99097\n",
      "iteration 92 \t : \t loss 0.00586 - accuracy 0.99194\n",
      "iteration 93 \t : \t loss 0.00571 - accuracy 0.99243\n",
      "iteration 94 \t : \t loss 0.00556 - accuracy 0.99341\n",
      "iteration 95 \t : \t loss 0.00542 - accuracy 0.99341\n",
      "iteration 96 \t : \t loss 0.00528 - accuracy 0.99341\n",
      "iteration 97 \t : \t loss 0.00514 - accuracy 0.99438\n",
      "iteration 98 \t : \t loss 0.00501 - accuracy 0.99487\n",
      "iteration 99 \t : \t loss 0.00488 - accuracy 0.99536\n",
      "iteration 100 \t : \t loss 0.00476 - accuracy 0.99585\n",
      "iteration 101 \t : \t loss 0.00463 - accuracy 0.99634\n",
      "iteration 102 \t : \t loss 0.00452 - accuracy 0.99658\n",
      "iteration 103 \t : \t loss 0.00440 - accuracy 0.99707\n",
      "iteration 104 \t : \t loss 0.00429 - accuracy 0.99731\n",
      "iteration 105 \t : \t loss 0.00418 - accuracy 0.99731\n",
      "iteration 106 \t : \t loss 0.00408 - accuracy 0.99731\n",
      "iteration 107 \t : \t loss 0.00398 - accuracy 0.99780\n",
      "iteration 108 \t : \t loss 0.00388 - accuracy 0.99780\n",
      "iteration 109 \t : \t loss 0.00378 - accuracy 0.99805\n",
      "iteration 110 \t : \t loss 0.00369 - accuracy 0.99805\n",
      "iteration 111 \t : \t loss 0.00360 - accuracy 0.99829\n",
      "iteration 112 \t : \t loss 0.00351 - accuracy 0.99854\n",
      "iteration 113 \t : \t loss 0.00343 - accuracy 0.99902\n",
      "iteration 114 \t : \t loss 0.00334 - accuracy 0.99902\n",
      "iteration 115 \t : \t loss 0.00326 - accuracy 0.99927\n",
      "iteration 116 \t : \t loss 0.00318 - accuracy 0.99927\n",
      "iteration 117 \t : \t loss 0.00311 - accuracy 0.99927\n",
      "iteration 118 \t : \t loss 0.00303 - accuracy 0.99927\n",
      "iteration 119 \t : \t loss 0.00296 - accuracy 0.99927\n",
      "iteration 120 \t : \t loss 0.00289 - accuracy 0.99927\n",
      "iteration 121 \t : \t loss 0.00282 - accuracy 0.99927\n",
      "iteration 122 \t : \t loss 0.00276 - accuracy 0.99927\n",
      "iteration 123 \t : \t loss 0.00270 - accuracy 0.99927\n",
      "iteration 124 \t : \t loss 0.00263 - accuracy 0.99927\n",
      "iteration 125 \t : \t loss 0.00257 - accuracy 0.99927\n",
      "iteration 126 \t : \t loss 0.00251 - accuracy 0.99927\n",
      "iteration 127 \t : \t loss 0.00246 - accuracy 0.99927\n",
      "iteration 128 \t : \t loss 0.00240 - accuracy 0.99927\n",
      "iteration 129 \t : \t loss 0.00235 - accuracy 0.99927\n",
      "iteration 130 \t : \t loss 0.00230 - accuracy 0.99927\n",
      "iteration 131 \t : \t loss 0.00225 - accuracy 0.99927\n",
      "iteration 132 \t : \t loss 0.00220 - accuracy 0.99951\n",
      "iteration 133 \t : \t loss 0.00215 - accuracy 0.99976\n",
      "iteration 134 \t : \t loss 0.00210 - accuracy 0.99976\n",
      "iteration 135 \t : \t loss 0.00206 - accuracy 0.99976\n",
      "iteration 136 \t : \t loss 0.00202 - accuracy 0.99976\n",
      "iteration 137 \t : \t loss 0.00197 - accuracy 0.99976\n",
      "iteration 138 \t : \t loss 0.00193 - accuracy 0.99976\n",
      "iteration 139 \t : \t loss 0.00189 - accuracy 0.99976\n",
      "iteration 140 \t : \t loss 0.00185 - accuracy 0.99976\n",
      "iteration 141 \t : \t loss 0.00182 - accuracy 0.99976\n",
      "iteration 142 \t : \t loss 0.00178 - accuracy 0.99976\n",
      "iteration 143 \t : \t loss 0.00175 - accuracy 0.99976\n",
      "iteration 144 \t : \t loss 0.00171 - accuracy 0.99976\n",
      "iteration 145 \t : \t loss 0.00168 - accuracy 0.99976\n",
      "iteration 146 \t : \t loss 0.00165 - accuracy 1.00000\n",
      "iteration 147 \t : \t loss 0.00162 - accuracy 1.00000\n",
      "iteration 148 \t : \t loss 0.00159 - accuracy 1.00000\n",
      "iteration 149 \t : \t loss 0.00156 - accuracy 1.00000\n",
      "iteration 150 \t : \t loss 0.00153 - accuracy 1.00000\n",
      "iteration 151 \t : \t loss 0.00150 - accuracy 1.00000\n",
      "iteration 152 \t : \t loss 0.00148 - accuracy 1.00000\n",
      "iteration 153 \t : \t loss 0.00145 - accuracy 1.00000\n",
      "iteration 154 \t : \t loss 0.00143 - accuracy 1.00000\n",
      "iteration 155 \t : \t loss 0.00140 - accuracy 1.00000\n",
      "iteration 156 \t : \t loss 0.00138 - accuracy 1.00000\n",
      "iteration 157 \t : \t loss 0.00136 - accuracy 1.00000\n",
      "iteration 158 \t : \t loss 0.00134 - accuracy 1.00000\n",
      "iteration 159 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 160 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 161 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 162 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 163 \t : \t loss 0.00123 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00118 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00114 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00111 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00108 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00106 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00105 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00104 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00102 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00101 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00099 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00097 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00095 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00093 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00092 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00091 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00087 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00086 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00084 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00082 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00079 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00079 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.10384 - accuracy 0.70923\n",
      "iteration 1 \t : \t loss 0.04685 - accuracy 0.88135\n",
      "iteration 2 \t : \t loss 0.03697 - accuracy 0.90698\n",
      "iteration 3 \t : \t loss 0.03206 - accuracy 0.91431\n",
      "iteration 4 \t : \t loss 0.02888 - accuracy 0.92261\n",
      "iteration 5 \t : \t loss 0.02657 - accuracy 0.92993\n",
      "iteration 6 \t : \t loss 0.02476 - accuracy 0.93408\n",
      "iteration 7 \t : \t loss 0.02328 - accuracy 0.93994\n",
      "iteration 8 \t : \t loss 0.02203 - accuracy 0.94214\n",
      "iteration 9 \t : \t loss 0.02094 - accuracy 0.94507\n",
      "iteration 10 \t : \t loss 0.01999 - accuracy 0.94995\n",
      "iteration 11 \t : \t loss 0.01913 - accuracy 0.95117\n",
      "iteration 12 \t : \t loss 0.01836 - accuracy 0.95435\n",
      "iteration 13 \t : \t loss 0.01765 - accuracy 0.95605\n",
      "iteration 14 \t : \t loss 0.01700 - accuracy 0.95654\n",
      "iteration 15 \t : \t loss 0.01640 - accuracy 0.95776\n",
      "iteration 16 \t : \t loss 0.01584 - accuracy 0.95972\n",
      "iteration 17 \t : \t loss 0.01531 - accuracy 0.96045\n",
      "iteration 18 \t : \t loss 0.01481 - accuracy 0.96143\n",
      "iteration 19 \t : \t loss 0.01435 - accuracy 0.96240\n",
      "iteration 20 \t : \t loss 0.01391 - accuracy 0.96265\n",
      "iteration 21 \t : \t loss 0.01349 - accuracy 0.96338\n",
      "iteration 22 \t : \t loss 0.01309 - accuracy 0.96436\n",
      "iteration 23 \t : \t loss 0.01271 - accuracy 0.96704\n",
      "iteration 24 \t : \t loss 0.01234 - accuracy 0.96777\n",
      "iteration 25 \t : \t loss 0.01200 - accuracy 0.96924\n",
      "iteration 26 \t : \t loss 0.01166 - accuracy 0.97095\n",
      "iteration 27 \t : \t loss 0.01134 - accuracy 0.97144\n",
      "iteration 28 \t : \t loss 0.01103 - accuracy 0.97314\n",
      "iteration 29 \t : \t loss 0.01074 - accuracy 0.97363\n",
      "iteration 30 \t : \t loss 0.01045 - accuracy 0.97559\n",
      "iteration 31 \t : \t loss 0.01018 - accuracy 0.97607\n",
      "iteration 32 \t : \t loss 0.00991 - accuracy 0.97729\n",
      "iteration 33 \t : \t loss 0.00965 - accuracy 0.97876\n",
      "iteration 34 \t : \t loss 0.00940 - accuracy 0.97998\n",
      "iteration 35 \t : \t loss 0.00916 - accuracy 0.98145\n",
      "iteration 36 \t : \t loss 0.00893 - accuracy 0.98218\n",
      "iteration 37 \t : \t loss 0.00871 - accuracy 0.98267\n",
      "iteration 38 \t : \t loss 0.00849 - accuracy 0.98340\n",
      "iteration 39 \t : \t loss 0.00828 - accuracy 0.98413\n",
      "iteration 40 \t : \t loss 0.00808 - accuracy 0.98486\n",
      "iteration 41 \t : \t loss 0.00788 - accuracy 0.98511\n",
      "iteration 42 \t : \t loss 0.00769 - accuracy 0.98584\n",
      "iteration 43 \t : \t loss 0.00750 - accuracy 0.98608\n",
      "iteration 44 \t : \t loss 0.00732 - accuracy 0.98657\n",
      "iteration 45 \t : \t loss 0.00715 - accuracy 0.98730\n",
      "iteration 46 \t : \t loss 0.00698 - accuracy 0.98755\n",
      "iteration 47 \t : \t loss 0.00681 - accuracy 0.98853\n",
      "iteration 48 \t : \t loss 0.00665 - accuracy 0.98926\n",
      "iteration 49 \t : \t loss 0.00650 - accuracy 0.98950\n",
      "iteration 50 \t : \t loss 0.00635 - accuracy 0.98975\n",
      "iteration 51 \t : \t loss 0.00620 - accuracy 0.99048\n",
      "iteration 52 \t : \t loss 0.00606 - accuracy 0.99072\n",
      "iteration 53 \t : \t loss 0.00592 - accuracy 0.99121\n",
      "iteration 54 \t : \t loss 0.00579 - accuracy 0.99170\n",
      "iteration 55 \t : \t loss 0.00566 - accuracy 0.99243\n",
      "iteration 56 \t : \t loss 0.00553 - accuracy 0.99268\n",
      "iteration 57 \t : \t loss 0.00541 - accuracy 0.99316\n",
      "iteration 58 \t : \t loss 0.00529 - accuracy 0.99316\n",
      "iteration 59 \t : \t loss 0.00517 - accuracy 0.99316\n",
      "iteration 60 \t : \t loss 0.00506 - accuracy 0.99414\n",
      "iteration 61 \t : \t loss 0.00495 - accuracy 0.99438\n",
      "iteration 62 \t : \t loss 0.00484 - accuracy 0.99463\n",
      "iteration 63 \t : \t loss 0.00474 - accuracy 0.99487\n",
      "iteration 64 \t : \t loss 0.00464 - accuracy 0.99536\n",
      "iteration 65 \t : \t loss 0.00454 - accuracy 0.99561\n",
      "iteration 66 \t : \t loss 0.00445 - accuracy 0.99561\n",
      "iteration 67 \t : \t loss 0.00435 - accuracy 0.99561\n",
      "iteration 68 \t : \t loss 0.00426 - accuracy 0.99561\n",
      "iteration 69 \t : \t loss 0.00418 - accuracy 0.99585\n",
      "iteration 70 \t : \t loss 0.00409 - accuracy 0.99585\n",
      "iteration 71 \t : \t loss 0.00401 - accuracy 0.99609\n",
      "iteration 72 \t : \t loss 0.00393 - accuracy 0.99634\n",
      "iteration 73 \t : \t loss 0.00385 - accuracy 0.99658\n",
      "iteration 74 \t : \t loss 0.00377 - accuracy 0.99658\n",
      "iteration 75 \t : \t loss 0.00370 - accuracy 0.99683\n",
      "iteration 76 \t : \t loss 0.00363 - accuracy 0.99683\n",
      "iteration 77 \t : \t loss 0.00356 - accuracy 0.99731\n",
      "iteration 78 \t : \t loss 0.00349 - accuracy 0.99731\n",
      "iteration 79 \t : \t loss 0.00342 - accuracy 0.99731\n",
      "iteration 80 \t : \t loss 0.00336 - accuracy 0.99731\n",
      "iteration 81 \t : \t loss 0.00330 - accuracy 0.99756\n",
      "iteration 82 \t : \t loss 0.00323 - accuracy 0.99756\n",
      "iteration 83 \t : \t loss 0.00317 - accuracy 0.99756\n",
      "iteration 84 \t : \t loss 0.00312 - accuracy 0.99780\n",
      "iteration 85 \t : \t loss 0.00306 - accuracy 0.99780\n",
      "iteration 86 \t : \t loss 0.00301 - accuracy 0.99780\n",
      "iteration 87 \t : \t loss 0.00295 - accuracy 0.99780\n",
      "iteration 88 \t : \t loss 0.00290 - accuracy 0.99805\n",
      "iteration 89 \t : \t loss 0.00285 - accuracy 0.99805\n",
      "iteration 90 \t : \t loss 0.00280 - accuracy 0.99805\n",
      "iteration 91 \t : \t loss 0.00275 - accuracy 0.99829\n",
      "iteration 92 \t : \t loss 0.00270 - accuracy 0.99829\n",
      "iteration 93 \t : \t loss 0.00266 - accuracy 0.99854\n",
      "iteration 94 \t : \t loss 0.00261 - accuracy 0.99878\n",
      "iteration 95 \t : \t loss 0.00257 - accuracy 0.99878\n",
      "iteration 96 \t : \t loss 0.00253 - accuracy 0.99902\n",
      "iteration 97 \t : \t loss 0.00249 - accuracy 0.99927\n",
      "iteration 98 \t : \t loss 0.00245 - accuracy 0.99927\n",
      "iteration 99 \t : \t loss 0.00241 - accuracy 0.99927\n",
      "iteration 100 \t : \t loss 0.00237 - accuracy 0.99927\n",
      "iteration 101 \t : \t loss 0.00233 - accuracy 0.99927\n",
      "iteration 102 \t : \t loss 0.00230 - accuracy 0.99927\n",
      "iteration 103 \t : \t loss 0.00226 - accuracy 0.99951\n",
      "iteration 104 \t : \t loss 0.00223 - accuracy 0.99976\n",
      "iteration 105 \t : \t loss 0.00219 - accuracy 0.99976\n",
      "iteration 106 \t : \t loss 0.00216 - accuracy 0.99976\n",
      "iteration 107 \t : \t loss 0.00213 - accuracy 0.99976\n",
      "iteration 108 \t : \t loss 0.00210 - accuracy 0.99976\n",
      "iteration 109 \t : \t loss 0.00207 - accuracy 0.99976\n",
      "iteration 110 \t : \t loss 0.00204 - accuracy 0.99976\n",
      "iteration 111 \t : \t loss 0.00201 - accuracy 1.00000\n",
      "iteration 112 \t : \t loss 0.00198 - accuracy 1.00000\n",
      "iteration 113 \t : \t loss 0.00195 - accuracy 1.00000\n",
      "iteration 114 \t : \t loss 0.00192 - accuracy 1.00000\n",
      "iteration 115 \t : \t loss 0.00190 - accuracy 1.00000\n",
      "iteration 116 \t : \t loss 0.00187 - accuracy 1.00000\n",
      "iteration 117 \t : \t loss 0.00185 - accuracy 1.00000\n",
      "iteration 118 \t : \t loss 0.00182 - accuracy 1.00000\n",
      "iteration 119 \t : \t loss 0.00180 - accuracy 1.00000\n",
      "iteration 120 \t : \t loss 0.00177 - accuracy 1.00000\n",
      "iteration 121 \t : \t loss 0.00175 - accuracy 1.00000\n",
      "iteration 122 \t : \t loss 0.00173 - accuracy 1.00000\n",
      "iteration 123 \t : \t loss 0.00171 - accuracy 1.00000\n",
      "iteration 124 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 125 \t : \t loss 0.00166 - accuracy 1.00000\n",
      "iteration 126 \t : \t loss 0.00164 - accuracy 1.00000\n",
      "iteration 127 \t : \t loss 0.00162 - accuracy 1.00000\n",
      "iteration 128 \t : \t loss 0.00160 - accuracy 1.00000\n",
      "iteration 129 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 130 \t : \t loss 0.00156 - accuracy 1.00000\n",
      "iteration 131 \t : \t loss 0.00154 - accuracy 1.00000\n",
      "iteration 132 \t : \t loss 0.00153 - accuracy 1.00000\n",
      "iteration 133 \t : \t loss 0.00151 - accuracy 1.00000\n",
      "iteration 134 \t : \t loss 0.00149 - accuracy 1.00000\n",
      "iteration 135 \t : \t loss 0.00147 - accuracy 1.00000\n",
      "iteration 136 \t : \t loss 0.00146 - accuracy 1.00000\n",
      "iteration 137 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 138 \t : \t loss 0.00142 - accuracy 1.00000\n",
      "iteration 139 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 140 \t : \t loss 0.00139 - accuracy 1.00000\n",
      "iteration 141 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 142 \t : \t loss 0.00136 - accuracy 1.00000\n",
      "iteration 143 \t : \t loss 0.00134 - accuracy 1.00000\n",
      "iteration 144 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 145 \t : \t loss 0.00132 - accuracy 1.00000\n",
      "iteration 146 \t : \t loss 0.00130 - accuracy 1.00000\n",
      "iteration 147 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 148 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 149 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 150 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 151 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 152 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 153 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 154 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 155 \t : \t loss 0.00119 - accuracy 1.00000\n",
      "iteration 156 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 157 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 158 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 159 \t : \t loss 0.00114 - accuracy 1.00000\n",
      "iteration 160 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 161 \t : \t loss 0.00112 - accuracy 1.00000\n",
      "iteration 162 \t : \t loss 0.00111 - accuracy 1.00000\n",
      "iteration 163 \t : \t loss 0.00110 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00108 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00107 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00106 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00105 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00104 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00103 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00102 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00101 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00100 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00099 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00097 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00096 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00095 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00093 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00093 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00092 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00091 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00089 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00087 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00086 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00084 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00082 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00082 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.30964 - accuracy 0.14307\n",
      "iteration 1 \t : \t loss 0.17796 - accuracy 0.40063\n",
      "iteration 2 \t : \t loss 0.12186 - accuracy 0.62036\n",
      "iteration 3 \t : \t loss 0.08915 - accuracy 0.73560\n",
      "iteration 4 \t : \t loss 0.07154 - accuracy 0.79028\n",
      "iteration 5 \t : \t loss 0.06093 - accuracy 0.82568\n",
      "iteration 6 \t : \t loss 0.05384 - accuracy 0.84937\n",
      "iteration 7 \t : \t loss 0.04881 - accuracy 0.86206\n",
      "iteration 8 \t : \t loss 0.04506 - accuracy 0.87354\n",
      "iteration 9 \t : \t loss 0.04215 - accuracy 0.88330\n",
      "iteration 10 \t : \t loss 0.03981 - accuracy 0.88867\n",
      "iteration 11 \t : \t loss 0.03787 - accuracy 0.89502\n",
      "iteration 12 \t : \t loss 0.03621 - accuracy 0.89941\n",
      "iteration 13 \t : \t loss 0.03476 - accuracy 0.90381\n",
      "iteration 14 \t : \t loss 0.03348 - accuracy 0.90552\n",
      "iteration 15 \t : \t loss 0.03232 - accuracy 0.90845\n",
      "iteration 16 \t : \t loss 0.03126 - accuracy 0.90967\n",
      "iteration 17 \t : \t loss 0.03028 - accuracy 0.91479\n",
      "iteration 18 \t : \t loss 0.02938 - accuracy 0.91821\n",
      "iteration 19 \t : \t loss 0.02853 - accuracy 0.92163\n",
      "iteration 20 \t : \t loss 0.02773 - accuracy 0.92334\n",
      "iteration 21 \t : \t loss 0.02697 - accuracy 0.92456\n",
      "iteration 22 \t : \t loss 0.02624 - accuracy 0.92651\n",
      "iteration 23 \t : \t loss 0.02555 - accuracy 0.92822\n",
      "iteration 24 \t : \t loss 0.02489 - accuracy 0.93018\n",
      "iteration 25 \t : \t loss 0.02426 - accuracy 0.93213\n",
      "iteration 26 \t : \t loss 0.02364 - accuracy 0.93335\n",
      "iteration 27 \t : \t loss 0.02305 - accuracy 0.93433\n",
      "iteration 28 \t : \t loss 0.02248 - accuracy 0.93530\n",
      "iteration 29 \t : \t loss 0.02192 - accuracy 0.93701\n",
      "iteration 30 \t : \t loss 0.02138 - accuracy 0.93872\n",
      "iteration 31 \t : \t loss 0.02086 - accuracy 0.94116\n",
      "iteration 32 \t : \t loss 0.02035 - accuracy 0.94189\n",
      "iteration 33 \t : \t loss 0.01985 - accuracy 0.94287\n",
      "iteration 34 \t : \t loss 0.01936 - accuracy 0.94507\n",
      "iteration 35 \t : \t loss 0.01889 - accuracy 0.94629\n",
      "iteration 36 \t : \t loss 0.01842 - accuracy 0.94751\n",
      "iteration 37 \t : \t loss 0.01797 - accuracy 0.94775\n",
      "iteration 38 \t : \t loss 0.01753 - accuracy 0.95020\n",
      "iteration 39 \t : \t loss 0.01709 - accuracy 0.95117\n",
      "iteration 40 \t : \t loss 0.01667 - accuracy 0.95312\n",
      "iteration 41 \t : \t loss 0.01625 - accuracy 0.95435\n",
      "iteration 42 \t : \t loss 0.01584 - accuracy 0.95752\n",
      "iteration 43 \t : \t loss 0.01544 - accuracy 0.95801\n",
      "iteration 44 \t : \t loss 0.01505 - accuracy 0.95923\n",
      "iteration 45 \t : \t loss 0.01466 - accuracy 0.96021\n",
      "iteration 46 \t : \t loss 0.01429 - accuracy 0.96143\n",
      "iteration 47 \t : \t loss 0.01392 - accuracy 0.96289\n",
      "iteration 48 \t : \t loss 0.01355 - accuracy 0.96411\n",
      "iteration 49 \t : \t loss 0.01320 - accuracy 0.96460\n",
      "iteration 50 \t : \t loss 0.01285 - accuracy 0.96704\n",
      "iteration 51 \t : \t loss 0.01251 - accuracy 0.96777\n",
      "iteration 52 \t : \t loss 0.01218 - accuracy 0.96875\n",
      "iteration 53 \t : \t loss 0.01186 - accuracy 0.97095\n",
      "iteration 54 \t : \t loss 0.01154 - accuracy 0.97168\n",
      "iteration 55 \t : \t loss 0.01123 - accuracy 0.97314\n",
      "iteration 56 \t : \t loss 0.01092 - accuracy 0.97437\n",
      "iteration 57 \t : \t loss 0.01063 - accuracy 0.97583\n",
      "iteration 58 \t : \t loss 0.01033 - accuracy 0.97705\n",
      "iteration 59 \t : \t loss 0.01005 - accuracy 0.97729\n",
      "iteration 60 \t : \t loss 0.00977 - accuracy 0.97876\n",
      "iteration 61 \t : \t loss 0.00950 - accuracy 0.97974\n",
      "iteration 62 \t : \t loss 0.00924 - accuracy 0.97998\n",
      "iteration 63 \t : \t loss 0.00898 - accuracy 0.98047\n",
      "iteration 64 \t : \t loss 0.00873 - accuracy 0.98193\n",
      "iteration 65 \t : \t loss 0.00849 - accuracy 0.98291\n",
      "iteration 66 \t : \t loss 0.00825 - accuracy 0.98389\n",
      "iteration 67 \t : \t loss 0.00802 - accuracy 0.98511\n",
      "iteration 68 \t : \t loss 0.00779 - accuracy 0.98608\n",
      "iteration 69 \t : \t loss 0.00757 - accuracy 0.98706\n",
      "iteration 70 \t : \t loss 0.00735 - accuracy 0.98755\n",
      "iteration 71 \t : \t loss 0.00715 - accuracy 0.98853\n",
      "iteration 72 \t : \t loss 0.00694 - accuracy 0.98853\n",
      "iteration 73 \t : \t loss 0.00674 - accuracy 0.98926\n",
      "iteration 74 \t : \t loss 0.00655 - accuracy 0.98975\n",
      "iteration 75 \t : \t loss 0.00636 - accuracy 0.99023\n",
      "iteration 76 \t : \t loss 0.00618 - accuracy 0.99146\n",
      "iteration 77 \t : \t loss 0.00600 - accuracy 0.99219\n",
      "iteration 78 \t : \t loss 0.00583 - accuracy 0.99292\n",
      "iteration 79 \t : \t loss 0.00566 - accuracy 0.99316\n",
      "iteration 80 \t : \t loss 0.00550 - accuracy 0.99390\n",
      "iteration 81 \t : \t loss 0.00534 - accuracy 0.99390\n",
      "iteration 82 \t : \t loss 0.00519 - accuracy 0.99414\n",
      "iteration 83 \t : \t loss 0.00504 - accuracy 0.99438\n",
      "iteration 84 \t : \t loss 0.00489 - accuracy 0.99438\n",
      "iteration 85 \t : \t loss 0.00475 - accuracy 0.99438\n",
      "iteration 86 \t : \t loss 0.00461 - accuracy 0.99487\n",
      "iteration 87 \t : \t loss 0.00448 - accuracy 0.99536\n",
      "iteration 88 \t : \t loss 0.00435 - accuracy 0.99561\n",
      "iteration 89 \t : \t loss 0.00423 - accuracy 0.99609\n",
      "iteration 90 \t : \t loss 0.00411 - accuracy 0.99634\n",
      "iteration 91 \t : \t loss 0.00399 - accuracy 0.99707\n",
      "iteration 92 \t : \t loss 0.00388 - accuracy 0.99707\n",
      "iteration 93 \t : \t loss 0.00377 - accuracy 0.99756\n",
      "iteration 94 \t : \t loss 0.00366 - accuracy 0.99805\n",
      "iteration 95 \t : \t loss 0.00356 - accuracy 0.99805\n",
      "iteration 96 \t : \t loss 0.00346 - accuracy 0.99805\n",
      "iteration 97 \t : \t loss 0.00337 - accuracy 0.99829\n",
      "iteration 98 \t : \t loss 0.00328 - accuracy 0.99829\n",
      "iteration 99 \t : \t loss 0.00319 - accuracy 0.99829\n",
      "iteration 100 \t : \t loss 0.00311 - accuracy 0.99854\n",
      "iteration 101 \t : \t loss 0.00302 - accuracy 0.99878\n",
      "iteration 102 \t : \t loss 0.00295 - accuracy 0.99878\n",
      "iteration 103 \t : \t loss 0.00287 - accuracy 0.99878\n",
      "iteration 104 \t : \t loss 0.00280 - accuracy 0.99878\n",
      "iteration 105 \t : \t loss 0.00273 - accuracy 0.99927\n",
      "iteration 106 \t : \t loss 0.00266 - accuracy 0.99951\n",
      "iteration 107 \t : \t loss 0.00259 - accuracy 0.99951\n",
      "iteration 108 \t : \t loss 0.00253 - accuracy 0.99951\n",
      "iteration 109 \t : \t loss 0.00247 - accuracy 0.99951\n",
      "iteration 110 \t : \t loss 0.00241 - accuracy 0.99976\n",
      "iteration 111 \t : \t loss 0.00235 - accuracy 0.99976\n",
      "iteration 112 \t : \t loss 0.00230 - accuracy 0.99976\n",
      "iteration 113 \t : \t loss 0.00225 - accuracy 0.99976\n",
      "iteration 114 \t : \t loss 0.00220 - accuracy 0.99976\n",
      "iteration 115 \t : \t loss 0.00215 - accuracy 0.99976\n",
      "iteration 116 \t : \t loss 0.00210 - accuracy 1.00000\n",
      "iteration 117 \t : \t loss 0.00205 - accuracy 1.00000\n",
      "iteration 118 \t : \t loss 0.00201 - accuracy 1.00000\n",
      "iteration 119 \t : \t loss 0.00197 - accuracy 1.00000\n",
      "iteration 120 \t : \t loss 0.00193 - accuracy 1.00000\n",
      "iteration 121 \t : \t loss 0.00189 - accuracy 1.00000\n",
      "iteration 122 \t : \t loss 0.00185 - accuracy 1.00000\n",
      "iteration 123 \t : \t loss 0.00181 - accuracy 1.00000\n",
      "iteration 124 \t : \t loss 0.00177 - accuracy 1.00000\n",
      "iteration 125 \t : \t loss 0.00174 - accuracy 1.00000\n",
      "iteration 126 \t : \t loss 0.00171 - accuracy 1.00000\n",
      "iteration 127 \t : \t loss 0.00167 - accuracy 1.00000\n",
      "iteration 128 \t : \t loss 0.00164 - accuracy 1.00000\n",
      "iteration 129 \t : \t loss 0.00161 - accuracy 1.00000\n",
      "iteration 130 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 131 \t : \t loss 0.00155 - accuracy 1.00000\n",
      "iteration 132 \t : \t loss 0.00152 - accuracy 1.00000\n",
      "iteration 133 \t : \t loss 0.00150 - accuracy 1.00000\n",
      "iteration 134 \t : \t loss 0.00147 - accuracy 1.00000\n",
      "iteration 135 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 136 \t : \t loss 0.00142 - accuracy 1.00000\n",
      "iteration 137 \t : \t loss 0.00139 - accuracy 1.00000\n",
      "iteration 138 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 139 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 140 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 141 \t : \t loss 0.00130 - accuracy 1.00000\n",
      "iteration 142 \t : \t loss 0.00128 - accuracy 1.00000\n",
      "iteration 143 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 144 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 145 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 146 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 147 \t : \t loss 0.00119 - accuracy 1.00000\n",
      "iteration 148 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 149 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 150 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 151 \t : \t loss 0.00112 - accuracy 1.00000\n",
      "iteration 152 \t : \t loss 0.00110 - accuracy 1.00000\n",
      "iteration 153 \t : \t loss 0.00108 - accuracy 1.00000\n",
      "iteration 154 \t : \t loss 0.00107 - accuracy 1.00000\n",
      "iteration 155 \t : \t loss 0.00105 - accuracy 1.00000\n",
      "iteration 156 \t : \t loss 0.00104 - accuracy 1.00000\n",
      "iteration 157 \t : \t loss 0.00102 - accuracy 1.00000\n",
      "iteration 158 \t : \t loss 0.00101 - accuracy 1.00000\n",
      "iteration 159 \t : \t loss 0.00100 - accuracy 1.00000\n",
      "iteration 160 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 161 \t : \t loss 0.00097 - accuracy 1.00000\n",
      "iteration 162 \t : \t loss 0.00096 - accuracy 1.00000\n",
      "iteration 163 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00093 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00092 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00091 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00087 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00086 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00084 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00082 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00079 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00075 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00074 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00073 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00072 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00071 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00071 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00070 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00069 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00068 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00068 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00067 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00066 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00066 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00065 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00064 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00064 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00063 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00062 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.10537 - accuracy 0.69922\n",
      "iteration 1 \t : \t loss 0.04693 - accuracy 0.87964\n",
      "iteration 2 \t : \t loss 0.03695 - accuracy 0.90161\n",
      "iteration 3 \t : \t loss 0.03199 - accuracy 0.91479\n",
      "iteration 4 \t : \t loss 0.02879 - accuracy 0.92334\n",
      "iteration 5 \t : \t loss 0.02645 - accuracy 0.92847\n",
      "iteration 6 \t : \t loss 0.02461 - accuracy 0.93408\n",
      "iteration 7 \t : \t loss 0.02310 - accuracy 0.93677\n",
      "iteration 8 \t : \t loss 0.02180 - accuracy 0.94141\n",
      "iteration 9 \t : \t loss 0.02067 - accuracy 0.94385\n",
      "iteration 10 \t : \t loss 0.01967 - accuracy 0.94678\n",
      "iteration 11 \t : \t loss 0.01877 - accuracy 0.94922\n",
      "iteration 12 \t : \t loss 0.01795 - accuracy 0.95117\n",
      "iteration 13 \t : \t loss 0.01720 - accuracy 0.95386\n",
      "iteration 14 \t : \t loss 0.01650 - accuracy 0.95630\n",
      "iteration 15 \t : \t loss 0.01586 - accuracy 0.95825\n",
      "iteration 16 \t : \t loss 0.01525 - accuracy 0.96045\n",
      "iteration 17 \t : \t loss 0.01469 - accuracy 0.96265\n",
      "iteration 18 \t : \t loss 0.01416 - accuracy 0.96338\n",
      "iteration 19 \t : \t loss 0.01366 - accuracy 0.96509\n",
      "iteration 20 \t : \t loss 0.01319 - accuracy 0.96631\n",
      "iteration 21 \t : \t loss 0.01274 - accuracy 0.96802\n",
      "iteration 22 \t : \t loss 0.01231 - accuracy 0.96851\n",
      "iteration 23 \t : \t loss 0.01191 - accuracy 0.96899\n",
      "iteration 24 \t : \t loss 0.01152 - accuracy 0.97119\n",
      "iteration 25 \t : \t loss 0.01115 - accuracy 0.97461\n",
      "iteration 26 \t : \t loss 0.01080 - accuracy 0.97510\n",
      "iteration 27 \t : \t loss 0.01047 - accuracy 0.97632\n",
      "iteration 28 \t : \t loss 0.01015 - accuracy 0.97705\n",
      "iteration 29 \t : \t loss 0.00984 - accuracy 0.97852\n",
      "iteration 30 \t : \t loss 0.00954 - accuracy 0.97974\n",
      "iteration 31 \t : \t loss 0.00926 - accuracy 0.98047\n",
      "iteration 32 \t : \t loss 0.00899 - accuracy 0.98120\n",
      "iteration 33 \t : \t loss 0.00873 - accuracy 0.98193\n",
      "iteration 34 \t : \t loss 0.00847 - accuracy 0.98267\n",
      "iteration 35 \t : \t loss 0.00823 - accuracy 0.98340\n",
      "iteration 36 \t : \t loss 0.00800 - accuracy 0.98438\n",
      "iteration 37 \t : \t loss 0.00777 - accuracy 0.98633\n",
      "iteration 38 \t : \t loss 0.00756 - accuracy 0.98657\n",
      "iteration 39 \t : \t loss 0.00735 - accuracy 0.98779\n",
      "iteration 40 \t : \t loss 0.00715 - accuracy 0.98828\n",
      "iteration 41 \t : \t loss 0.00695 - accuracy 0.98926\n",
      "iteration 42 \t : \t loss 0.00676 - accuracy 0.98975\n",
      "iteration 43 \t : \t loss 0.00658 - accuracy 0.99048\n",
      "iteration 44 \t : \t loss 0.00641 - accuracy 0.99048\n",
      "iteration 45 \t : \t loss 0.00624 - accuracy 0.99097\n",
      "iteration 46 \t : \t loss 0.00608 - accuracy 0.99146\n",
      "iteration 47 \t : \t loss 0.00592 - accuracy 0.99146\n",
      "iteration 48 \t : \t loss 0.00577 - accuracy 0.99219\n",
      "iteration 49 \t : \t loss 0.00562 - accuracy 0.99243\n",
      "iteration 50 \t : \t loss 0.00548 - accuracy 0.99268\n",
      "iteration 51 \t : \t loss 0.00534 - accuracy 0.99292\n",
      "iteration 52 \t : \t loss 0.00521 - accuracy 0.99365\n",
      "iteration 53 \t : \t loss 0.00508 - accuracy 0.99463\n",
      "iteration 54 \t : \t loss 0.00496 - accuracy 0.99487\n",
      "iteration 55 \t : \t loss 0.00484 - accuracy 0.99561\n",
      "iteration 56 \t : \t loss 0.00473 - accuracy 0.99585\n",
      "iteration 57 \t : \t loss 0.00461 - accuracy 0.99609\n",
      "iteration 58 \t : \t loss 0.00451 - accuracy 0.99683\n",
      "iteration 59 \t : \t loss 0.00440 - accuracy 0.99707\n",
      "iteration 60 \t : \t loss 0.00430 - accuracy 0.99731\n",
      "iteration 61 \t : \t loss 0.00420 - accuracy 0.99731\n",
      "iteration 62 \t : \t loss 0.00411 - accuracy 0.99756\n",
      "iteration 63 \t : \t loss 0.00402 - accuracy 0.99756\n",
      "iteration 64 \t : \t loss 0.00393 - accuracy 0.99780\n",
      "iteration 65 \t : \t loss 0.00384 - accuracy 0.99805\n",
      "iteration 66 \t : \t loss 0.00376 - accuracy 0.99829\n",
      "iteration 67 \t : \t loss 0.00368 - accuracy 0.99854\n",
      "iteration 68 \t : \t loss 0.00360 - accuracy 0.99878\n",
      "iteration 69 \t : \t loss 0.00353 - accuracy 0.99878\n",
      "iteration 70 \t : \t loss 0.00345 - accuracy 0.99878\n",
      "iteration 71 \t : \t loss 0.00338 - accuracy 0.99902\n",
      "iteration 72 \t : \t loss 0.00331 - accuracy 0.99902\n",
      "iteration 73 \t : \t loss 0.00325 - accuracy 0.99902\n",
      "iteration 74 \t : \t loss 0.00318 - accuracy 0.99902\n",
      "iteration 75 \t : \t loss 0.00312 - accuracy 0.99902\n",
      "iteration 76 \t : \t loss 0.00306 - accuracy 0.99927\n",
      "iteration 77 \t : \t loss 0.00300 - accuracy 0.99927\n",
      "iteration 78 \t : \t loss 0.00295 - accuracy 0.99927\n",
      "iteration 79 \t : \t loss 0.00289 - accuracy 0.99927\n",
      "iteration 80 \t : \t loss 0.00284 - accuracy 0.99927\n",
      "iteration 81 \t : \t loss 0.00278 - accuracy 0.99951\n",
      "iteration 82 \t : \t loss 0.00273 - accuracy 0.99951\n",
      "iteration 83 \t : \t loss 0.00268 - accuracy 0.99951\n",
      "iteration 84 \t : \t loss 0.00264 - accuracy 0.99951\n",
      "iteration 85 \t : \t loss 0.00259 - accuracy 0.99951\n",
      "iteration 86 \t : \t loss 0.00254 - accuracy 0.99951\n",
      "iteration 87 \t : \t loss 0.00250 - accuracy 0.99951\n",
      "iteration 88 \t : \t loss 0.00246 - accuracy 0.99951\n",
      "iteration 89 \t : \t loss 0.00242 - accuracy 0.99951\n",
      "iteration 90 \t : \t loss 0.00238 - accuracy 0.99951\n",
      "iteration 91 \t : \t loss 0.00234 - accuracy 0.99951\n",
      "iteration 92 \t : \t loss 0.00230 - accuracy 0.99951\n",
      "iteration 93 \t : \t loss 0.00226 - accuracy 0.99951\n",
      "iteration 94 \t : \t loss 0.00222 - accuracy 0.99951\n",
      "iteration 95 \t : \t loss 0.00219 - accuracy 0.99951\n",
      "iteration 96 \t : \t loss 0.00216 - accuracy 0.99951\n",
      "iteration 97 \t : \t loss 0.00212 - accuracy 0.99951\n",
      "iteration 98 \t : \t loss 0.00209 - accuracy 0.99951\n",
      "iteration 99 \t : \t loss 0.00206 - accuracy 0.99951\n",
      "iteration 100 \t : \t loss 0.00203 - accuracy 0.99951\n",
      "iteration 101 \t : \t loss 0.00200 - accuracy 0.99951\n",
      "iteration 102 \t : \t loss 0.00197 - accuracy 0.99951\n",
      "iteration 103 \t : \t loss 0.00194 - accuracy 0.99951\n",
      "iteration 104 \t : \t loss 0.00191 - accuracy 0.99951\n",
      "iteration 105 \t : \t loss 0.00188 - accuracy 0.99951\n",
      "iteration 106 \t : \t loss 0.00186 - accuracy 0.99951\n",
      "iteration 107 \t : \t loss 0.00183 - accuracy 0.99976\n",
      "iteration 108 \t : \t loss 0.00180 - accuracy 0.99976\n",
      "iteration 109 \t : \t loss 0.00178 - accuracy 0.99976\n",
      "iteration 110 \t : \t loss 0.00175 - accuracy 1.00000\n",
      "iteration 111 \t : \t loss 0.00173 - accuracy 1.00000\n",
      "iteration 112 \t : \t loss 0.00171 - accuracy 1.00000\n",
      "iteration 113 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 114 \t : \t loss 0.00166 - accuracy 1.00000\n",
      "iteration 115 \t : \t loss 0.00164 - accuracy 1.00000\n",
      "iteration 116 \t : \t loss 0.00162 - accuracy 1.00000\n",
      "iteration 117 \t : \t loss 0.00160 - accuracy 1.00000\n",
      "iteration 118 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 119 \t : \t loss 0.00156 - accuracy 1.00000\n",
      "iteration 120 \t : \t loss 0.00154 - accuracy 1.00000\n",
      "iteration 121 \t : \t loss 0.00152 - accuracy 1.00000\n",
      "iteration 122 \t : \t loss 0.00150 - accuracy 1.00000\n",
      "iteration 123 \t : \t loss 0.00148 - accuracy 1.00000\n",
      "iteration 124 \t : \t loss 0.00147 - accuracy 1.00000\n",
      "iteration 125 \t : \t loss 0.00145 - accuracy 1.00000\n",
      "iteration 126 \t : \t loss 0.00143 - accuracy 1.00000\n",
      "iteration 127 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 128 \t : \t loss 0.00140 - accuracy 1.00000\n",
      "iteration 129 \t : \t loss 0.00138 - accuracy 1.00000\n",
      "iteration 130 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 131 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 132 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 133 \t : \t loss 0.00132 - accuracy 1.00000\n",
      "iteration 134 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 135 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 136 \t : \t loss 0.00128 - accuracy 1.00000\n",
      "iteration 137 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 138 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 139 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 140 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 141 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 142 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 143 \t : \t loss 0.00119 - accuracy 1.00000\n",
      "iteration 144 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 145 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 146 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 147 \t : \t loss 0.00114 - accuracy 1.00000\n",
      "iteration 148 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 149 \t : \t loss 0.00112 - accuracy 1.00000\n",
      "iteration 150 \t : \t loss 0.00111 - accuracy 1.00000\n",
      "iteration 151 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 152 \t : \t loss 0.00108 - accuracy 1.00000\n",
      "iteration 153 \t : \t loss 0.00107 - accuracy 1.00000\n",
      "iteration 154 \t : \t loss 0.00106 - accuracy 1.00000\n",
      "iteration 155 \t : \t loss 0.00105 - accuracy 1.00000\n",
      "iteration 156 \t : \t loss 0.00104 - accuracy 1.00000\n",
      "iteration 157 \t : \t loss 0.00103 - accuracy 1.00000\n",
      "iteration 158 \t : \t loss 0.00102 - accuracy 1.00000\n",
      "iteration 159 \t : \t loss 0.00101 - accuracy 1.00000\n",
      "iteration 160 \t : \t loss 0.00101 - accuracy 1.00000\n",
      "iteration 161 \t : \t loss 0.00100 - accuracy 1.00000\n",
      "iteration 162 \t : \t loss 0.00099 - accuracy 1.00000\n",
      "iteration 163 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00097 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00096 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00095 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00093 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00092 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00091 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00089 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00087 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00086 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00084 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00082 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00079 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00075 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00074 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00074 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00073 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.41675 - accuracy 0.21143\n",
      "iteration 1 \t : \t loss 0.14936 - accuracy 0.53247\n",
      "iteration 2 \t : \t loss 0.09501 - accuracy 0.70142\n",
      "iteration 3 \t : \t loss 0.07031 - accuracy 0.77930\n",
      "iteration 4 \t : \t loss 0.05808 - accuracy 0.82007\n",
      "iteration 5 \t : \t loss 0.05098 - accuracy 0.84595\n",
      "iteration 6 \t : \t loss 0.04627 - accuracy 0.85913\n",
      "iteration 7 \t : \t loss 0.04285 - accuracy 0.86865\n",
      "iteration 8 \t : \t loss 0.04020 - accuracy 0.87964\n",
      "iteration 9 \t : \t loss 0.03805 - accuracy 0.88672\n",
      "iteration 10 \t : \t loss 0.03625 - accuracy 0.89233\n",
      "iteration 11 \t : \t loss 0.03469 - accuracy 0.89648\n",
      "iteration 12 \t : \t loss 0.03331 - accuracy 0.90063\n",
      "iteration 13 \t : \t loss 0.03208 - accuracy 0.90601\n",
      "iteration 14 \t : \t loss 0.03096 - accuracy 0.90991\n",
      "iteration 15 \t : \t loss 0.02993 - accuracy 0.91211\n",
      "iteration 16 \t : \t loss 0.02897 - accuracy 0.91406\n",
      "iteration 17 \t : \t loss 0.02807 - accuracy 0.91626\n",
      "iteration 18 \t : \t loss 0.02723 - accuracy 0.91919\n",
      "iteration 19 \t : \t loss 0.02643 - accuracy 0.92334\n",
      "iteration 20 \t : \t loss 0.02567 - accuracy 0.92603\n",
      "iteration 21 \t : \t loss 0.02494 - accuracy 0.92920\n",
      "iteration 22 \t : \t loss 0.02425 - accuracy 0.93115\n",
      "iteration 23 \t : \t loss 0.02359 - accuracy 0.93433\n",
      "iteration 24 \t : \t loss 0.02295 - accuracy 0.93628\n",
      "iteration 25 \t : \t loss 0.02233 - accuracy 0.93970\n",
      "iteration 26 \t : \t loss 0.02174 - accuracy 0.94312\n",
      "iteration 27 \t : \t loss 0.02116 - accuracy 0.94604\n",
      "iteration 28 \t : \t loss 0.02060 - accuracy 0.94751\n",
      "iteration 29 \t : \t loss 0.02006 - accuracy 0.94800\n",
      "iteration 30 \t : \t loss 0.01954 - accuracy 0.95044\n",
      "iteration 31 \t : \t loss 0.01903 - accuracy 0.95312\n",
      "iteration 32 \t : \t loss 0.01853 - accuracy 0.95386\n",
      "iteration 33 \t : \t loss 0.01805 - accuracy 0.95508\n",
      "iteration 34 \t : \t loss 0.01758 - accuracy 0.95654\n",
      "iteration 35 \t : \t loss 0.01712 - accuracy 0.95825\n",
      "iteration 36 \t : \t loss 0.01667 - accuracy 0.95923\n",
      "iteration 37 \t : \t loss 0.01623 - accuracy 0.96045\n",
      "iteration 38 \t : \t loss 0.01580 - accuracy 0.96118\n",
      "iteration 39 \t : \t loss 0.01539 - accuracy 0.96216\n",
      "iteration 40 \t : \t loss 0.01498 - accuracy 0.96338\n",
      "iteration 41 \t : \t loss 0.01458 - accuracy 0.96436\n",
      "iteration 42 \t : \t loss 0.01419 - accuracy 0.96582\n",
      "iteration 43 \t : \t loss 0.01380 - accuracy 0.96729\n",
      "iteration 44 \t : \t loss 0.01343 - accuracy 0.96826\n",
      "iteration 45 \t : \t loss 0.01307 - accuracy 0.97021\n",
      "iteration 46 \t : \t loss 0.01271 - accuracy 0.97119\n",
      "iteration 47 \t : \t loss 0.01236 - accuracy 0.97217\n",
      "iteration 48 \t : \t loss 0.01202 - accuracy 0.97363\n",
      "iteration 49 \t : \t loss 0.01168 - accuracy 0.97510\n",
      "iteration 50 \t : \t loss 0.01136 - accuracy 0.97632\n",
      "iteration 51 \t : \t loss 0.01104 - accuracy 0.97729\n",
      "iteration 52 \t : \t loss 0.01073 - accuracy 0.97852\n",
      "iteration 53 \t : \t loss 0.01043 - accuracy 0.97876\n",
      "iteration 54 \t : \t loss 0.01013 - accuracy 0.97974\n",
      "iteration 55 \t : \t loss 0.00985 - accuracy 0.98096\n",
      "iteration 56 \t : \t loss 0.00957 - accuracy 0.98193\n",
      "iteration 57 \t : \t loss 0.00930 - accuracy 0.98218\n",
      "iteration 58 \t : \t loss 0.00903 - accuracy 0.98267\n",
      "iteration 59 \t : \t loss 0.00877 - accuracy 0.98340\n",
      "iteration 60 \t : \t loss 0.00852 - accuracy 0.98438\n",
      "iteration 61 \t : \t loss 0.00828 - accuracy 0.98511\n",
      "iteration 62 \t : \t loss 0.00805 - accuracy 0.98608\n",
      "iteration 63 \t : \t loss 0.00782 - accuracy 0.98706\n",
      "iteration 64 \t : \t loss 0.00760 - accuracy 0.98779\n",
      "iteration 65 \t : \t loss 0.00738 - accuracy 0.98828\n",
      "iteration 66 \t : \t loss 0.00717 - accuracy 0.98901\n",
      "iteration 67 \t : \t loss 0.00697 - accuracy 0.98926\n",
      "iteration 68 \t : \t loss 0.00677 - accuracy 0.98975\n",
      "iteration 69 \t : \t loss 0.00658 - accuracy 0.99121\n",
      "iteration 70 \t : \t loss 0.00640 - accuracy 0.99146\n",
      "iteration 71 \t : \t loss 0.00622 - accuracy 0.99194\n",
      "iteration 72 \t : \t loss 0.00604 - accuracy 0.99268\n",
      "iteration 73 \t : \t loss 0.00588 - accuracy 0.99292\n",
      "iteration 74 \t : \t loss 0.00571 - accuracy 0.99341\n",
      "iteration 75 \t : \t loss 0.00555 - accuracy 0.99365\n",
      "iteration 76 \t : \t loss 0.00540 - accuracy 0.99414\n",
      "iteration 77 \t : \t loss 0.00525 - accuracy 0.99463\n",
      "iteration 78 \t : \t loss 0.00511 - accuracy 0.99536\n",
      "iteration 79 \t : \t loss 0.00497 - accuracy 0.99536\n",
      "iteration 80 \t : \t loss 0.00483 - accuracy 0.99585\n",
      "iteration 81 \t : \t loss 0.00470 - accuracy 0.99585\n",
      "iteration 82 \t : \t loss 0.00457 - accuracy 0.99585\n",
      "iteration 83 \t : \t loss 0.00444 - accuracy 0.99585\n",
      "iteration 84 \t : \t loss 0.00432 - accuracy 0.99585\n",
      "iteration 85 \t : \t loss 0.00420 - accuracy 0.99609\n",
      "iteration 86 \t : \t loss 0.00409 - accuracy 0.99658\n",
      "iteration 87 \t : \t loss 0.00398 - accuracy 0.99658\n",
      "iteration 88 \t : \t loss 0.00387 - accuracy 0.99658\n",
      "iteration 89 \t : \t loss 0.00377 - accuracy 0.99658\n",
      "iteration 90 \t : \t loss 0.00366 - accuracy 0.99658\n",
      "iteration 91 \t : \t loss 0.00356 - accuracy 0.99683\n",
      "iteration 92 \t : \t loss 0.00347 - accuracy 0.99707\n",
      "iteration 93 \t : \t loss 0.00337 - accuracy 0.99731\n",
      "iteration 94 \t : \t loss 0.00328 - accuracy 0.99731\n",
      "iteration 95 \t : \t loss 0.00319 - accuracy 0.99731\n",
      "iteration 96 \t : \t loss 0.00311 - accuracy 0.99731\n",
      "iteration 97 \t : \t loss 0.00302 - accuracy 0.99756\n",
      "iteration 98 \t : \t loss 0.00294 - accuracy 0.99756\n",
      "iteration 99 \t : \t loss 0.00286 - accuracy 0.99829\n",
      "iteration 100 \t : \t loss 0.00279 - accuracy 0.99829\n",
      "iteration 101 \t : \t loss 0.00271 - accuracy 0.99854\n",
      "iteration 102 \t : \t loss 0.00264 - accuracy 0.99854\n",
      "iteration 103 \t : \t loss 0.00257 - accuracy 0.99902\n",
      "iteration 104 \t : \t loss 0.00251 - accuracy 0.99902\n",
      "iteration 105 \t : \t loss 0.00244 - accuracy 0.99902\n",
      "iteration 106 \t : \t loss 0.00238 - accuracy 0.99927\n",
      "iteration 107 \t : \t loss 0.00232 - accuracy 0.99927\n",
      "iteration 108 \t : \t loss 0.00226 - accuracy 0.99927\n",
      "iteration 109 \t : \t loss 0.00221 - accuracy 0.99927\n",
      "iteration 110 \t : \t loss 0.00215 - accuracy 0.99951\n",
      "iteration 111 \t : \t loss 0.00210 - accuracy 0.99976\n",
      "iteration 112 \t : \t loss 0.00205 - accuracy 0.99976\n",
      "iteration 113 \t : \t loss 0.00200 - accuracy 0.99976\n",
      "iteration 114 \t : \t loss 0.00196 - accuracy 0.99976\n",
      "iteration 115 \t : \t loss 0.00191 - accuracy 1.00000\n",
      "iteration 116 \t : \t loss 0.00187 - accuracy 1.00000\n",
      "iteration 117 \t : \t loss 0.00183 - accuracy 1.00000\n",
      "iteration 118 \t : \t loss 0.00179 - accuracy 1.00000\n",
      "iteration 119 \t : \t loss 0.00175 - accuracy 1.00000\n",
      "iteration 120 \t : \t loss 0.00172 - accuracy 1.00000\n",
      "iteration 121 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 122 \t : \t loss 0.00165 - accuracy 1.00000\n",
      "iteration 123 \t : \t loss 0.00161 - accuracy 1.00000\n",
      "iteration 124 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 125 \t : \t loss 0.00155 - accuracy 1.00000\n",
      "iteration 126 \t : \t loss 0.00152 - accuracy 1.00000\n",
      "iteration 127 \t : \t loss 0.00149 - accuracy 1.00000\n",
      "iteration 128 \t : \t loss 0.00146 - accuracy 1.00000\n",
      "iteration 129 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 130 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 131 \t : \t loss 0.00138 - accuracy 1.00000\n",
      "iteration 132 \t : \t loss 0.00136 - accuracy 1.00000\n",
      "iteration 133 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 134 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 135 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 136 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 137 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 138 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 139 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 140 \t : \t loss 0.00118 - accuracy 1.00000\n",
      "iteration 141 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 142 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 143 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 144 \t : \t loss 0.00111 - accuracy 1.00000\n",
      "iteration 145 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 146 \t : \t loss 0.00108 - accuracy 1.00000\n",
      "iteration 147 \t : \t loss 0.00106 - accuracy 1.00000\n",
      "iteration 148 \t : \t loss 0.00104 - accuracy 1.00000\n",
      "iteration 149 \t : \t loss 0.00103 - accuracy 1.00000\n",
      "iteration 150 \t : \t loss 0.00101 - accuracy 1.00000\n",
      "iteration 151 \t : \t loss 0.00100 - accuracy 1.00000\n",
      "iteration 152 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 153 \t : \t loss 0.00097 - accuracy 1.00000\n",
      "iteration 154 \t : \t loss 0.00096 - accuracy 1.00000\n",
      "iteration 155 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 156 \t : \t loss 0.00093 - accuracy 1.00000\n",
      "iteration 157 \t : \t loss 0.00092 - accuracy 1.00000\n",
      "iteration 158 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 159 \t : \t loss 0.00089 - accuracy 1.00000\n",
      "iteration 161 \t : \t loss 0.00087 - accuracy 1.00000\n",
      "iteration 162 \t : \t loss 0.00086 - accuracy 1.00000\n",
      "iteration 163 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00082 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00079 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00075 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00074 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00073 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00072 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00071 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00070 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00070 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00069 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00068 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00067 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00067 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00066 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00065 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00064 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00064 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00063 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00062 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00062 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00061 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00060 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00060 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00059 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00059 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00058 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00057 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00057 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00056 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.09663 - accuracy 0.71802\n",
      "iteration 1 \t : \t loss 0.04492 - accuracy 0.88062\n",
      "iteration 2 \t : \t loss 0.03549 - accuracy 0.90088\n",
      "iteration 3 \t : \t loss 0.03062 - accuracy 0.91504\n",
      "iteration 4 \t : \t loss 0.02743 - accuracy 0.92529\n",
      "iteration 5 \t : \t loss 0.02507 - accuracy 0.93237\n",
      "iteration 6 \t : \t loss 0.02321 - accuracy 0.93750\n",
      "iteration 7 \t : \t loss 0.02168 - accuracy 0.94141\n",
      "iteration 8 \t : \t loss 0.02037 - accuracy 0.94482\n",
      "iteration 9 \t : \t loss 0.01924 - accuracy 0.94897\n",
      "iteration 10 \t : \t loss 0.01824 - accuracy 0.95312\n",
      "iteration 11 \t : \t loss 0.01734 - accuracy 0.95581\n",
      "iteration 12 \t : \t loss 0.01652 - accuracy 0.95825\n",
      "iteration 13 \t : \t loss 0.01578 - accuracy 0.96045\n",
      "iteration 14 \t : \t loss 0.01509 - accuracy 0.96216\n",
      "iteration 15 \t : \t loss 0.01446 - accuracy 0.96436\n",
      "iteration 16 \t : \t loss 0.01387 - accuracy 0.96558\n",
      "iteration 17 \t : \t loss 0.01332 - accuracy 0.96753\n",
      "iteration 18 \t : \t loss 0.01280 - accuracy 0.96997\n",
      "iteration 19 \t : \t loss 0.01232 - accuracy 0.97144\n",
      "iteration 20 \t : \t loss 0.01186 - accuracy 0.97266\n",
      "iteration 21 \t : \t loss 0.01143 - accuracy 0.97412\n",
      "iteration 22 \t : \t loss 0.01102 - accuracy 0.97559\n",
      "iteration 23 \t : \t loss 0.01063 - accuracy 0.97632\n",
      "iteration 24 \t : \t loss 0.01027 - accuracy 0.97803\n",
      "iteration 25 \t : \t loss 0.00992 - accuracy 0.97900\n",
      "iteration 26 \t : \t loss 0.00958 - accuracy 0.97925\n",
      "iteration 27 \t : \t loss 0.00927 - accuracy 0.98096\n",
      "iteration 28 \t : \t loss 0.00896 - accuracy 0.98145\n",
      "iteration 29 \t : \t loss 0.00867 - accuracy 0.98218\n",
      "iteration 30 \t : \t loss 0.00840 - accuracy 0.98267\n",
      "iteration 31 \t : \t loss 0.00813 - accuracy 0.98291\n",
      "iteration 32 \t : \t loss 0.00788 - accuracy 0.98413\n",
      "iteration 33 \t : \t loss 0.00764 - accuracy 0.98486\n",
      "iteration 34 \t : \t loss 0.00740 - accuracy 0.98560\n",
      "iteration 35 \t : \t loss 0.00718 - accuracy 0.98657\n",
      "iteration 36 \t : \t loss 0.00696 - accuracy 0.98779\n",
      "iteration 37 \t : \t loss 0.00676 - accuracy 0.98828\n",
      "iteration 38 \t : \t loss 0.00656 - accuracy 0.98901\n",
      "iteration 39 \t : \t loss 0.00637 - accuracy 0.98950\n",
      "iteration 40 \t : \t loss 0.00619 - accuracy 0.99048\n",
      "iteration 41 \t : \t loss 0.00601 - accuracy 0.99097\n",
      "iteration 42 \t : \t loss 0.00584 - accuracy 0.99170\n",
      "iteration 43 \t : \t loss 0.00568 - accuracy 0.99243\n",
      "iteration 44 \t : \t loss 0.00552 - accuracy 0.99292\n",
      "iteration 45 \t : \t loss 0.00537 - accuracy 0.99316\n",
      "iteration 46 \t : \t loss 0.00523 - accuracy 0.99365\n",
      "iteration 47 \t : \t loss 0.00509 - accuracy 0.99414\n",
      "iteration 48 \t : \t loss 0.00495 - accuracy 0.99512\n",
      "iteration 49 \t : \t loss 0.00482 - accuracy 0.99536\n",
      "iteration 50 \t : \t loss 0.00470 - accuracy 0.99536\n",
      "iteration 51 \t : \t loss 0.00458 - accuracy 0.99536\n",
      "iteration 52 \t : \t loss 0.00446 - accuracy 0.99609\n",
      "iteration 53 \t : \t loss 0.00435 - accuracy 0.99609\n",
      "iteration 54 \t : \t loss 0.00424 - accuracy 0.99634\n",
      "iteration 55 \t : \t loss 0.00413 - accuracy 0.99707\n",
      "iteration 56 \t : \t loss 0.00403 - accuracy 0.99780\n",
      "iteration 57 \t : \t loss 0.00394 - accuracy 0.99805\n",
      "iteration 58 \t : \t loss 0.00384 - accuracy 0.99805\n",
      "iteration 59 \t : \t loss 0.00375 - accuracy 0.99805\n",
      "iteration 60 \t : \t loss 0.00366 - accuracy 0.99829\n",
      "iteration 61 \t : \t loss 0.00358 - accuracy 0.99878\n",
      "iteration 62 \t : \t loss 0.00350 - accuracy 0.99878\n",
      "iteration 63 \t : \t loss 0.00342 - accuracy 0.99902\n",
      "iteration 64 \t : \t loss 0.00334 - accuracy 0.99927\n",
      "iteration 65 \t : \t loss 0.00327 - accuracy 0.99927\n",
      "iteration 66 \t : \t loss 0.00319 - accuracy 0.99927\n",
      "iteration 67 \t : \t loss 0.00313 - accuracy 0.99927\n",
      "iteration 68 \t : \t loss 0.00306 - accuracy 0.99927\n",
      "iteration 69 \t : \t loss 0.00299 - accuracy 0.99927\n",
      "iteration 70 \t : \t loss 0.00293 - accuracy 0.99927\n",
      "iteration 71 \t : \t loss 0.00287 - accuracy 0.99927\n",
      "iteration 72 \t : \t loss 0.00281 - accuracy 0.99927\n",
      "iteration 73 \t : \t loss 0.00276 - accuracy 0.99927\n",
      "iteration 74 \t : \t loss 0.00270 - accuracy 0.99951\n",
      "iteration 75 \t : \t loss 0.00265 - accuracy 0.99951\n",
      "iteration 76 \t : \t loss 0.00260 - accuracy 0.99951\n",
      "iteration 77 \t : \t loss 0.00255 - accuracy 0.99976\n",
      "iteration 78 \t : \t loss 0.00250 - accuracy 1.00000\n",
      "iteration 79 \t : \t loss 0.00245 - accuracy 1.00000\n",
      "iteration 80 \t : \t loss 0.00241 - accuracy 1.00000\n",
      "iteration 81 \t : \t loss 0.00236 - accuracy 1.00000\n",
      "iteration 82 \t : \t loss 0.00232 - accuracy 1.00000\n",
      "iteration 83 \t : \t loss 0.00228 - accuracy 1.00000\n",
      "iteration 84 \t : \t loss 0.00224 - accuracy 1.00000\n",
      "iteration 85 \t : \t loss 0.00220 - accuracy 1.00000\n",
      "iteration 86 \t : \t loss 0.00216 - accuracy 1.00000\n",
      "iteration 87 \t : \t loss 0.00213 - accuracy 1.00000\n",
      "iteration 88 \t : \t loss 0.00209 - accuracy 1.00000\n",
      "iteration 89 \t : \t loss 0.00205 - accuracy 1.00000\n",
      "iteration 90 \t : \t loss 0.00202 - accuracy 1.00000\n",
      "iteration 91 \t : \t loss 0.00199 - accuracy 1.00000\n",
      "iteration 92 \t : \t loss 0.00196 - accuracy 1.00000\n",
      "iteration 93 \t : \t loss 0.00192 - accuracy 1.00000\n",
      "iteration 94 \t : \t loss 0.00189 - accuracy 1.00000\n",
      "iteration 95 \t : \t loss 0.00186 - accuracy 1.00000\n",
      "iteration 96 \t : \t loss 0.00184 - accuracy 1.00000\n",
      "iteration 97 \t : \t loss 0.00181 - accuracy 1.00000\n",
      "iteration 98 \t : \t loss 0.00178 - accuracy 1.00000\n",
      "iteration 99 \t : \t loss 0.00175 - accuracy 1.00000\n",
      "iteration 100 \t : \t loss 0.00173 - accuracy 1.00000\n",
      "iteration 101 \t : \t loss 0.00170 - accuracy 1.00000\n",
      "iteration 102 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 103 \t : \t loss 0.00165 - accuracy 1.00000\n",
      "iteration 104 \t : \t loss 0.00163 - accuracy 1.00000\n",
      "iteration 105 \t : \t loss 0.00161 - accuracy 1.00000\n",
      "iteration 106 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 107 \t : \t loss 0.00156 - accuracy 1.00000\n",
      "iteration 108 \t : \t loss 0.00154 - accuracy 1.00000\n",
      "iteration 109 \t : \t loss 0.00152 - accuracy 1.00000\n",
      "iteration 110 \t : \t loss 0.00150 - accuracy 1.00000\n",
      "iteration 111 \t : \t loss 0.00148 - accuracy 1.00000\n",
      "iteration 112 \t : \t loss 0.00146 - accuracy 1.00000\n",
      "iteration 113 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 114 \t : \t loss 0.00142 - accuracy 1.00000\n",
      "iteration 115 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 116 \t : \t loss 0.00139 - accuracy 1.00000\n",
      "iteration 117 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 118 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 119 \t : \t loss 0.00134 - accuracy 1.00000\n",
      "iteration 120 \t : \t loss 0.00132 - accuracy 1.00000\n",
      "iteration 121 \t : \t loss 0.00130 - accuracy 1.00000\n",
      "iteration 122 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 123 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 124 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 125 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 126 \t : \t loss 0.00123 - accuracy 1.00000\n",
      "iteration 127 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 128 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 129 \t : \t loss 0.00119 - accuracy 1.00000\n",
      "iteration 130 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 131 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 132 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 133 \t : \t loss 0.00114 - accuracy 1.00000\n",
      "iteration 134 \t : \t loss 0.00112 - accuracy 1.00000\n",
      "iteration 135 \t : \t loss 0.00111 - accuracy 1.00000\n",
      "iteration 136 \t : \t loss 0.00110 - accuracy 1.00000\n",
      "iteration 137 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 138 \t : \t loss 0.00108 - accuracy 1.00000\n",
      "iteration 139 \t : \t loss 0.00107 - accuracy 1.00000\n",
      "iteration 140 \t : \t loss 0.00105 - accuracy 1.00000\n",
      "iteration 141 \t : \t loss 0.00104 - accuracy 1.00000\n",
      "iteration 142 \t : \t loss 0.00103 - accuracy 1.00000\n",
      "iteration 143 \t : \t loss 0.00102 - accuracy 1.00000\n",
      "iteration 144 \t : \t loss 0.00101 - accuracy 1.00000\n",
      "iteration 145 \t : \t loss 0.00100 - accuracy 1.00000\n",
      "iteration 146 \t : \t loss 0.00099 - accuracy 1.00000\n",
      "iteration 147 \t : \t loss 0.00098 - accuracy 1.00000\n",
      "iteration 148 \t : \t loss 0.00097 - accuracy 1.00000\n",
      "iteration 149 \t : \t loss 0.00096 - accuracy 1.00000\n",
      "iteration 150 \t : \t loss 0.00095 - accuracy 1.00000\n",
      "iteration 151 \t : \t loss 0.00095 - accuracy 1.00000\n",
      "iteration 152 \t : \t loss 0.00094 - accuracy 1.00000\n",
      "iteration 153 \t : \t loss 0.00093 - accuracy 1.00000\n",
      "iteration 154 \t : \t loss 0.00092 - accuracy 1.00000\n",
      "iteration 155 \t : \t loss 0.00091 - accuracy 1.00000\n",
      "iteration 156 \t : \t loss 0.00090 - accuracy 1.00000\n",
      "iteration 157 \t : \t loss 0.00089 - accuracy 1.00000\n",
      "iteration 158 \t : \t loss 0.00089 - accuracy 1.00000\n",
      "iteration 159 \t : \t loss 0.00088 - accuracy 1.00000\n",
      "iteration 160 \t : \t loss 0.00087 - accuracy 1.00000\n",
      "iteration 161 \t : \t loss 0.00086 - accuracy 1.00000\n",
      "iteration 162 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 163 \t : \t loss 0.00085 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00084 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00083 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00082 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00081 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00080 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00079 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00078 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00077 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00076 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00075 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00075 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00074 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00074 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00073 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00072 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00072 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00071 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00071 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00070 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00070 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00069 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00069 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00068 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00068 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00067 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00067 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00066 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00066 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00065 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00065 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00064 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00064 - accuracy 1.00000\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_samples = 60000\n",
    "q_l = [100, 300, 500, 700, 1000]\n",
    "n_layers = 2\n",
    "\n",
    "error_rates_1, error_rates_2 = [], []\n",
    "\n",
    "for q in q_l:\n",
    "    # Initialization\n",
    "    DNN_trained = init_DBN(p, q, n_layers)\n",
    "    DNN_pre_trained = copy.deepcopy(DNN_trained)\n",
    "    \n",
    "    # Training\n",
    "    DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train=False, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "    DNN_pre_trained = principal_mnist(n_samples, DNN_pre_trained, pre_train=True, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    error_rates_1.append(test_DNN(DNN_trained, test_image, test_label))\n",
    "    error_rates_2.append(test_DNN(DNN_pre_trained, test_image, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "WBB8jtkN7uJ2",
    "outputId": "e8693549-5a8c-4278-d601-db2233d9e032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09509999999999996, 0.09130000000000005, 0.0988, 0.10019999999999996, 0.0968]\n",
      "[0.07040000000000002, 0.05700000000000005, 0.050000000000000044, 0.04579999999999995, 0.04849999999999999]\n"
     ]
    }
   ],
   "source": [
    "print(error_rates_1)\n",
    "print(error_rates_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "id": "Q3V_cqsc9oag",
    "outputId": "05a39d3b-f9fa-4d4f-f2eb-f156a05ef665"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(q_l, error_rates_1, label='No pre-training', color='blue')\n",
    "plt.plot(q_l, error_rates_2, label='With pre-training', color='red')\n",
    "plt.title('Error rate according to number of neurons per layer', size=20)\n",
    "plt.xlabel('Number of neurons', size=15)\n",
    "plt.ylabel('Error rate', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "fig.savefig('n_neurons.jpg', dpi = 300, bbox_inches='tight', orientation = 'landscape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nkzcr8im8dvW"
   },
   "source": [
    "### 3 Réseaux en fonction du nombre de données train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jOQtNDv88dfw",
    "outputId": "29f25253-9697-491f-87e7-3fb8075dd113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 \t : \t loss 0.06043 - accuracy 0.77979\n",
      "iteration 1 \t : \t loss 0.06793 - accuracy 0.77905\n",
      "iteration 2 \t : \t loss 0.06777 - accuracy 0.78198\n",
      "iteration 3 \t : \t loss 0.06761 - accuracy 0.78540\n",
      "iteration 4 \t : \t loss 0.06741 - accuracy 0.79028\n",
      "iteration 5 \t : \t loss 0.06716 - accuracy 0.79590\n",
      "iteration 6 \t : \t loss 0.06682 - accuracy 0.80127\n",
      "iteration 7 \t : \t loss 0.06635 - accuracy 0.80273\n",
      "iteration 8 \t : \t loss 0.06570 - accuracy 0.80542\n",
      "iteration 9 \t : \t loss 0.06480 - accuracy 0.81396\n",
      "iteration 10 \t : \t loss 0.06359 - accuracy 0.81836\n",
      "iteration 11 \t : \t loss 0.06203 - accuracy 0.82520\n",
      "iteration 12 \t : \t loss 0.06010 - accuracy 0.83008\n",
      "iteration 13 \t : \t loss 0.05791 - accuracy 0.83618\n",
      "iteration 14 \t : \t loss 0.05557 - accuracy 0.84131\n",
      "iteration 15 \t : \t loss 0.05323 - accuracy 0.84668\n",
      "iteration 16 \t : \t loss 0.05108 - accuracy 0.84985\n",
      "iteration 17 \t : \t loss 0.04928 - accuracy 0.85449\n",
      "iteration 18 \t : \t loss 0.04785 - accuracy 0.85913\n",
      "iteration 19 \t : \t loss 0.04665 - accuracy 0.86279\n",
      "iteration 20 \t : \t loss 0.04556 - accuracy 0.86670\n",
      "iteration 21 \t : \t loss 0.04452 - accuracy 0.87085\n",
      "iteration 22 \t : \t loss 0.04348 - accuracy 0.87402\n",
      "iteration 23 \t : \t loss 0.04242 - accuracy 0.87720\n",
      "iteration 24 \t : \t loss 0.04135 - accuracy 0.88354\n",
      "iteration 25 \t : \t loss 0.04025 - accuracy 0.88745\n",
      "iteration 26 \t : \t loss 0.03913 - accuracy 0.89111\n",
      "iteration 27 \t : \t loss 0.03800 - accuracy 0.89355\n",
      "iteration 28 \t : \t loss 0.03688 - accuracy 0.89844\n",
      "iteration 29 \t : \t loss 0.03576 - accuracy 0.90234\n",
      "iteration 30 \t : \t loss 0.03467 - accuracy 0.90527\n",
      "iteration 31 \t : \t loss 0.03360 - accuracy 0.90991\n",
      "iteration 32 \t : \t loss 0.03255 - accuracy 0.91162\n",
      "iteration 33 \t : \t loss 0.03153 - accuracy 0.91528\n",
      "iteration 34 \t : \t loss 0.03054 - accuracy 0.91772\n",
      "iteration 35 \t : \t loss 0.02957 - accuracy 0.92285\n",
      "iteration 36 \t : \t loss 0.02862 - accuracy 0.92651\n",
      "iteration 37 \t : \t loss 0.02771 - accuracy 0.92847\n",
      "iteration 38 \t : \t loss 0.02681 - accuracy 0.92993\n",
      "iteration 39 \t : \t loss 0.02594 - accuracy 0.93115\n",
      "iteration 40 \t : \t loss 0.02509 - accuracy 0.93384\n",
      "iteration 41 \t : \t loss 0.02427 - accuracy 0.93604\n",
      "iteration 42 \t : \t loss 0.02347 - accuracy 0.93677\n",
      "iteration 43 \t : \t loss 0.02270 - accuracy 0.93848\n",
      "iteration 44 \t : \t loss 0.02196 - accuracy 0.94189\n",
      "iteration 45 \t : \t loss 0.02125 - accuracy 0.94482\n",
      "iteration 46 \t : \t loss 0.02057 - accuracy 0.94653\n",
      "iteration 47 \t : \t loss 0.01992 - accuracy 0.94849\n",
      "iteration 48 \t : \t loss 0.01930 - accuracy 0.94995\n",
      "iteration 49 \t : \t loss 0.01871 - accuracy 0.95020\n",
      "iteration 50 \t : \t loss 0.01815 - accuracy 0.95117\n",
      "iteration 51 \t : \t loss 0.01762 - accuracy 0.95239\n",
      "iteration 52 \t : \t loss 0.01712 - accuracy 0.95361\n",
      "iteration 53 \t : \t loss 0.01664 - accuracy 0.95483\n",
      "iteration 54 \t : \t loss 0.01619 - accuracy 0.95557\n",
      "iteration 55 \t : \t loss 0.01576 - accuracy 0.95679\n",
      "iteration 56 \t : \t loss 0.01535 - accuracy 0.95850\n",
      "iteration 57 \t : \t loss 0.01496 - accuracy 0.95972\n",
      "iteration 58 \t : \t loss 0.01458 - accuracy 0.96045\n",
      "iteration 59 \t : \t loss 0.01423 - accuracy 0.96118\n",
      "iteration 60 \t : \t loss 0.01389 - accuracy 0.96191\n",
      "iteration 61 \t : \t loss 0.01356 - accuracy 0.96313\n",
      "iteration 62 \t : \t loss 0.01325 - accuracy 0.96387\n",
      "iteration 63 \t : \t loss 0.01296 - accuracy 0.96436\n",
      "iteration 64 \t : \t loss 0.01267 - accuracy 0.96533\n",
      "iteration 65 \t : \t loss 0.01240 - accuracy 0.96533\n",
      "iteration 66 \t : \t loss 0.01213 - accuracy 0.96631\n",
      "iteration 67 \t : \t loss 0.01188 - accuracy 0.96655\n",
      "iteration 68 \t : \t loss 0.01163 - accuracy 0.96729\n",
      "iteration 69 \t : \t loss 0.01139 - accuracy 0.96777\n",
      "iteration 70 \t : \t loss 0.01116 - accuracy 0.96826\n",
      "iteration 71 \t : \t loss 0.01094 - accuracy 0.96875\n",
      "iteration 72 \t : \t loss 0.01073 - accuracy 0.97021\n",
      "iteration 73 \t : \t loss 0.01052 - accuracy 0.97070\n",
      "iteration 74 \t : \t loss 0.01032 - accuracy 0.97095\n",
      "iteration 75 \t : \t loss 0.01012 - accuracy 0.97144\n",
      "iteration 76 \t : \t loss 0.00994 - accuracy 0.97095\n",
      "iteration 77 \t : \t loss 0.00975 - accuracy 0.97192\n",
      "iteration 78 \t : \t loss 0.00957 - accuracy 0.97241\n",
      "iteration 79 \t : \t loss 0.00940 - accuracy 0.97290\n",
      "iteration 80 \t : \t loss 0.00923 - accuracy 0.97339\n",
      "iteration 81 \t : \t loss 0.00907 - accuracy 0.97339\n",
      "iteration 82 \t : \t loss 0.00891 - accuracy 0.97437\n",
      "iteration 83 \t : \t loss 0.00875 - accuracy 0.97461\n",
      "iteration 84 \t : \t loss 0.00860 - accuracy 0.97559\n",
      "iteration 85 \t : \t loss 0.00845 - accuracy 0.97559\n",
      "iteration 86 \t : \t loss 0.00831 - accuracy 0.97583\n",
      "iteration 87 \t : \t loss 0.00816 - accuracy 0.97607\n",
      "iteration 88 \t : \t loss 0.00803 - accuracy 0.97681\n",
      "iteration 89 \t : \t loss 0.00789 - accuracy 0.97852\n",
      "iteration 90 \t : \t loss 0.00776 - accuracy 0.97949\n",
      "iteration 91 \t : \t loss 0.00763 - accuracy 0.97949\n",
      "iteration 92 \t : \t loss 0.00750 - accuracy 0.97998\n",
      "iteration 93 \t : \t loss 0.00738 - accuracy 0.98096\n",
      "iteration 94 \t : \t loss 0.00726 - accuracy 0.98193\n",
      "iteration 95 \t : \t loss 0.00714 - accuracy 0.98218\n",
      "iteration 96 \t : \t loss 0.00703 - accuracy 0.98242\n",
      "iteration 97 \t : \t loss 0.00691 - accuracy 0.98315\n",
      "iteration 98 \t : \t loss 0.00680 - accuracy 0.98364\n",
      "iteration 99 \t : \t loss 0.00669 - accuracy 0.98389\n",
      "iteration 100 \t : \t loss 0.00659 - accuracy 0.98413\n",
      "iteration 101 \t : \t loss 0.00648 - accuracy 0.98438\n",
      "iteration 102 \t : \t loss 0.00638 - accuracy 0.98535\n",
      "iteration 103 \t : \t loss 0.00628 - accuracy 0.98633\n",
      "iteration 104 \t : \t loss 0.00618 - accuracy 0.98706\n",
      "iteration 105 \t : \t loss 0.00608 - accuracy 0.98755\n",
      "iteration 106 \t : \t loss 0.00599 - accuracy 0.98755\n",
      "iteration 107 \t : \t loss 0.00589 - accuracy 0.98755\n",
      "iteration 108 \t : \t loss 0.00580 - accuracy 0.98779\n",
      "iteration 109 \t : \t loss 0.00571 - accuracy 0.98853\n",
      "iteration 110 \t : \t loss 0.00562 - accuracy 0.98877\n",
      "iteration 111 \t : \t loss 0.00554 - accuracy 0.98877\n",
      "iteration 112 \t : \t loss 0.00545 - accuracy 0.98877\n",
      "iteration 113 \t : \t loss 0.00537 - accuracy 0.98901\n",
      "iteration 114 \t : \t loss 0.00528 - accuracy 0.98950\n",
      "iteration 115 \t : \t loss 0.00520 - accuracy 0.98975\n",
      "iteration 116 \t : \t loss 0.00512 - accuracy 0.98975\n",
      "iteration 117 \t : \t loss 0.00504 - accuracy 0.99023\n",
      "iteration 118 \t : \t loss 0.00496 - accuracy 0.99072\n",
      "iteration 119 \t : \t loss 0.00489 - accuracy 0.99121\n",
      "iteration 120 \t : \t loss 0.00481 - accuracy 0.99121\n",
      "iteration 121 \t : \t loss 0.00474 - accuracy 0.99146\n",
      "iteration 122 \t : \t loss 0.00467 - accuracy 0.99146\n",
      "iteration 123 \t : \t loss 0.00460 - accuracy 0.99194\n",
      "iteration 124 \t : \t loss 0.00453 - accuracy 0.99194\n",
      "iteration 125 \t : \t loss 0.00446 - accuracy 0.99219\n",
      "iteration 126 \t : \t loss 0.00439 - accuracy 0.99268\n",
      "iteration 127 \t : \t loss 0.00432 - accuracy 0.99268\n",
      "iteration 128 \t : \t loss 0.00426 - accuracy 0.99316\n",
      "iteration 129 \t : \t loss 0.00419 - accuracy 0.99365\n",
      "iteration 130 \t : \t loss 0.00413 - accuracy 0.99390\n",
      "iteration 131 \t : \t loss 0.00406 - accuracy 0.99414\n",
      "iteration 132 \t : \t loss 0.00400 - accuracy 0.99414\n",
      "iteration 133 \t : \t loss 0.00394 - accuracy 0.99414\n",
      "iteration 134 \t : \t loss 0.00388 - accuracy 0.99438\n",
      "iteration 135 \t : \t loss 0.00382 - accuracy 0.99438\n",
      "iteration 136 \t : \t loss 0.00376 - accuracy 0.99463\n",
      "iteration 137 \t : \t loss 0.00371 - accuracy 0.99463\n",
      "iteration 138 \t : \t loss 0.00365 - accuracy 0.99463\n",
      "iteration 139 \t : \t loss 0.00359 - accuracy 0.99487\n",
      "iteration 140 \t : \t loss 0.00354 - accuracy 0.99536\n",
      "iteration 141 \t : \t loss 0.00348 - accuracy 0.99536\n",
      "iteration 142 \t : \t loss 0.00343 - accuracy 0.99536\n",
      "iteration 143 \t : \t loss 0.00338 - accuracy 0.99536\n",
      "iteration 144 \t : \t loss 0.00333 - accuracy 0.99536\n",
      "iteration 145 \t : \t loss 0.00328 - accuracy 0.99561\n",
      "iteration 146 \t : \t loss 0.00323 - accuracy 0.99561\n",
      "iteration 147 \t : \t loss 0.00318 - accuracy 0.99561\n",
      "iteration 148 \t : \t loss 0.00313 - accuracy 0.99585\n",
      "iteration 149 \t : \t loss 0.00308 - accuracy 0.99585\n",
      "iteration 150 \t : \t loss 0.00303 - accuracy 0.99585\n",
      "iteration 151 \t : \t loss 0.00299 - accuracy 0.99634\n",
      "iteration 152 \t : \t loss 0.00294 - accuracy 0.99634\n",
      "iteration 153 \t : \t loss 0.00290 - accuracy 0.99634\n",
      "iteration 154 \t : \t loss 0.00285 - accuracy 0.99634\n",
      "iteration 155 \t : \t loss 0.00281 - accuracy 0.99634\n",
      "iteration 156 \t : \t loss 0.00277 - accuracy 0.99658\n",
      "iteration 157 \t : \t loss 0.00272 - accuracy 0.99658\n",
      "iteration 158 \t : \t loss 0.00268 - accuracy 0.99658\n",
      "iteration 159 \t : \t loss 0.00264 - accuracy 0.99683\n",
      "iteration 160 \t : \t loss 0.00260 - accuracy 0.99707\n",
      "iteration 161 \t : \t loss 0.00256 - accuracy 0.99780\n",
      "iteration 162 \t : \t loss 0.00252 - accuracy 0.99780\n",
      "iteration 163 \t : \t loss 0.00248 - accuracy 0.99805\n",
      "iteration 164 \t : \t loss 0.00245 - accuracy 0.99805\n",
      "iteration 165 \t : \t loss 0.00241 - accuracy 0.99805\n",
      "iteration 166 \t : \t loss 0.00237 - accuracy 0.99805\n",
      "iteration 167 \t : \t loss 0.00234 - accuracy 0.99805\n",
      "iteration 168 \t : \t loss 0.00230 - accuracy 0.99805\n",
      "iteration 169 \t : \t loss 0.00227 - accuracy 0.99805\n",
      "iteration 170 \t : \t loss 0.00223 - accuracy 0.99805\n",
      "iteration 171 \t : \t loss 0.00220 - accuracy 0.99805\n",
      "iteration 172 \t : \t loss 0.00217 - accuracy 0.99805\n",
      "iteration 173 \t : \t loss 0.00213 - accuracy 0.99829\n",
      "iteration 174 \t : \t loss 0.00210 - accuracy 0.99829\n",
      "iteration 175 \t : \t loss 0.00207 - accuracy 0.99854\n",
      "iteration 176 \t : \t loss 0.00204 - accuracy 0.99878\n",
      "iteration 177 \t : \t loss 0.00201 - accuracy 0.99878\n",
      "iteration 178 \t : \t loss 0.00198 - accuracy 0.99878\n",
      "iteration 179 \t : \t loss 0.00195 - accuracy 0.99878\n",
      "iteration 180 \t : \t loss 0.00192 - accuracy 0.99902\n",
      "iteration 181 \t : \t loss 0.00189 - accuracy 0.99902\n",
      "iteration 182 \t : \t loss 0.00186 - accuracy 0.99902\n",
      "iteration 183 \t : \t loss 0.00183 - accuracy 0.99902\n",
      "iteration 184 \t : \t loss 0.00181 - accuracy 0.99902\n",
      "iteration 185 \t : \t loss 0.00178 - accuracy 0.99902\n",
      "iteration 186 \t : \t loss 0.00175 - accuracy 0.99902\n",
      "iteration 187 \t : \t loss 0.00173 - accuracy 0.99902\n",
      "iteration 188 \t : \t loss 0.00170 - accuracy 0.99902\n",
      "iteration 189 \t : \t loss 0.00168 - accuracy 0.99902\n",
      "iteration 190 \t : \t loss 0.00165 - accuracy 0.99902\n",
      "iteration 191 \t : \t loss 0.00163 - accuracy 0.99902\n",
      "iteration 192 \t : \t loss 0.00161 - accuracy 0.99902\n",
      "iteration 193 \t : \t loss 0.00158 - accuracy 0.99902\n",
      "iteration 194 \t : \t loss 0.00156 - accuracy 0.99902\n",
      "iteration 195 \t : \t loss 0.00154 - accuracy 0.99902\n",
      "iteration 196 \t : \t loss 0.00152 - accuracy 0.99927\n",
      "iteration 197 \t : \t loss 0.00150 - accuracy 0.99927\n",
      "iteration 198 \t : \t loss 0.00148 - accuracy 0.99927\n",
      "iteration 199 \t : \t loss 0.00145 - accuracy 0.99927\n",
      "iteration 0 \t : \t loss 0.05322 - accuracy 0.83252\n",
      "iteration 1 \t : \t loss 0.04126 - accuracy 0.88428\n",
      "iteration 2 \t : \t loss 0.03215 - accuracy 0.92334\n",
      "iteration 3 \t : \t loss 0.02688 - accuracy 0.94067\n",
      "iteration 4 \t : \t loss 0.02344 - accuracy 0.94727\n",
      "iteration 5 \t : \t loss 0.02101 - accuracy 0.95386\n",
      "iteration 6 \t : \t loss 0.01919 - accuracy 0.95679\n",
      "iteration 7 \t : \t loss 0.01779 - accuracy 0.95801\n",
      "iteration 8 \t : \t loss 0.01666 - accuracy 0.96045\n",
      "iteration 9 \t : \t loss 0.01573 - accuracy 0.96167\n",
      "iteration 10 \t : \t loss 0.01495 - accuracy 0.96313\n",
      "iteration 11 \t : \t loss 0.01429 - accuracy 0.96484\n",
      "iteration 12 \t : \t loss 0.01371 - accuracy 0.96631\n",
      "iteration 13 \t : \t loss 0.01321 - accuracy 0.96802\n",
      "iteration 14 \t : \t loss 0.01276 - accuracy 0.96899\n",
      "iteration 15 \t : \t loss 0.01236 - accuracy 0.97046\n",
      "iteration 16 \t : \t loss 0.01200 - accuracy 0.97046\n",
      "iteration 17 \t : \t loss 0.01167 - accuracy 0.97095\n",
      "iteration 18 \t : \t loss 0.01137 - accuracy 0.97168\n",
      "iteration 19 \t : \t loss 0.01109 - accuracy 0.97192\n",
      "iteration 20 \t : \t loss 0.01083 - accuracy 0.97217\n",
      "iteration 21 \t : \t loss 0.01059 - accuracy 0.97314\n",
      "iteration 22 \t : \t loss 0.01037 - accuracy 0.97339\n",
      "iteration 23 \t : \t loss 0.01016 - accuracy 0.97339\n",
      "iteration 24 \t : \t loss 0.00996 - accuracy 0.97485\n",
      "iteration 25 \t : \t loss 0.00978 - accuracy 0.97485\n",
      "iteration 26 \t : \t loss 0.00960 - accuracy 0.97510\n",
      "iteration 27 \t : \t loss 0.00943 - accuracy 0.97534\n",
      "iteration 28 \t : \t loss 0.00927 - accuracy 0.97534\n",
      "iteration 29 \t : \t loss 0.00912 - accuracy 0.97632\n",
      "iteration 30 \t : \t loss 0.00897 - accuracy 0.97632\n",
      "iteration 31 \t : \t loss 0.00883 - accuracy 0.97705\n",
      "iteration 32 \t : \t loss 0.00870 - accuracy 0.97705\n",
      "iteration 33 \t : \t loss 0.00857 - accuracy 0.97729\n",
      "iteration 34 \t : \t loss 0.00844 - accuracy 0.97778\n",
      "iteration 35 \t : \t loss 0.00832 - accuracy 0.97803\n",
      "iteration 36 \t : \t loss 0.00820 - accuracy 0.97900\n",
      "iteration 37 \t : \t loss 0.00809 - accuracy 0.97974\n",
      "iteration 38 \t : \t loss 0.00798 - accuracy 0.97998\n",
      "iteration 39 \t : \t loss 0.00787 - accuracy 0.97998\n",
      "iteration 40 \t : \t loss 0.00777 - accuracy 0.97998\n",
      "iteration 41 \t : \t loss 0.00767 - accuracy 0.98022\n",
      "iteration 42 \t : \t loss 0.00757 - accuracy 0.98047\n",
      "iteration 43 \t : \t loss 0.00748 - accuracy 0.98096\n",
      "iteration 44 \t : \t loss 0.00738 - accuracy 0.98120\n",
      "iteration 45 \t : \t loss 0.00729 - accuracy 0.98120\n",
      "iteration 46 \t : \t loss 0.00720 - accuracy 0.98169\n",
      "iteration 47 \t : \t loss 0.00712 - accuracy 0.98193\n",
      "iteration 48 \t : \t loss 0.00703 - accuracy 0.98218\n",
      "iteration 49 \t : \t loss 0.00695 - accuracy 0.98242\n",
      "iteration 50 \t : \t loss 0.00687 - accuracy 0.98242\n",
      "iteration 51 \t : \t loss 0.00679 - accuracy 0.98291\n",
      "iteration 52 \t : \t loss 0.00671 - accuracy 0.98315\n",
      "iteration 53 \t : \t loss 0.00664 - accuracy 0.98364\n",
      "iteration 54 \t : \t loss 0.00656 - accuracy 0.98389\n",
      "iteration 55 \t : \t loss 0.00649 - accuracy 0.98438\n",
      "iteration 56 \t : \t loss 0.00642 - accuracy 0.98438\n",
      "iteration 57 \t : \t loss 0.00635 - accuracy 0.98486\n",
      "iteration 58 \t : \t loss 0.00628 - accuracy 0.98511\n",
      "iteration 59 \t : \t loss 0.00621 - accuracy 0.98535\n",
      "iteration 60 \t : \t loss 0.00614 - accuracy 0.98535\n",
      "iteration 61 \t : \t loss 0.00607 - accuracy 0.98560\n",
      "iteration 62 \t : \t loss 0.00601 - accuracy 0.98584\n",
      "iteration 63 \t : \t loss 0.00595 - accuracy 0.98584\n",
      "iteration 64 \t : \t loss 0.00588 - accuracy 0.98608\n",
      "iteration 65 \t : \t loss 0.00582 - accuracy 0.98608\n",
      "iteration 66 \t : \t loss 0.00576 - accuracy 0.98633\n",
      "iteration 67 \t : \t loss 0.00570 - accuracy 0.98657\n",
      "iteration 68 \t : \t loss 0.00564 - accuracy 0.98657\n",
      "iteration 69 \t : \t loss 0.00558 - accuracy 0.98657\n",
      "iteration 70 \t : \t loss 0.00553 - accuracy 0.98682\n",
      "iteration 71 \t : \t loss 0.00547 - accuracy 0.98706\n",
      "iteration 72 \t : \t loss 0.00541 - accuracy 0.98730\n",
      "iteration 73 \t : \t loss 0.00536 - accuracy 0.98730\n",
      "iteration 74 \t : \t loss 0.00531 - accuracy 0.98755\n",
      "iteration 75 \t : \t loss 0.00525 - accuracy 0.98755\n",
      "iteration 76 \t : \t loss 0.00520 - accuracy 0.98779\n",
      "iteration 77 \t : \t loss 0.00515 - accuracy 0.98853\n",
      "iteration 78 \t : \t loss 0.00510 - accuracy 0.98853\n",
      "iteration 79 \t : \t loss 0.00505 - accuracy 0.98877\n",
      "iteration 80 \t : \t loss 0.00500 - accuracy 0.98877\n",
      "iteration 81 \t : \t loss 0.00495 - accuracy 0.98926\n",
      "iteration 82 \t : \t loss 0.00490 - accuracy 0.98926\n",
      "iteration 83 \t : \t loss 0.00485 - accuracy 0.98950\n",
      "iteration 84 \t : \t loss 0.00480 - accuracy 0.98950\n",
      "iteration 85 \t : \t loss 0.00476 - accuracy 0.98975\n",
      "iteration 86 \t : \t loss 0.00471 - accuracy 0.98975\n",
      "iteration 87 \t : \t loss 0.00467 - accuracy 0.98975\n",
      "iteration 88 \t : \t loss 0.00462 - accuracy 0.99023\n",
      "iteration 89 \t : \t loss 0.00458 - accuracy 0.99023\n",
      "iteration 90 \t : \t loss 0.00453 - accuracy 0.99023\n",
      "iteration 91 \t : \t loss 0.00449 - accuracy 0.99048\n",
      "iteration 92 \t : \t loss 0.00445 - accuracy 0.99072\n",
      "iteration 93 \t : \t loss 0.00440 - accuracy 0.99072\n",
      "iteration 94 \t : \t loss 0.00436 - accuracy 0.99072\n",
      "iteration 95 \t : \t loss 0.00432 - accuracy 0.99097\n",
      "iteration 96 \t : \t loss 0.00428 - accuracy 0.99097\n",
      "iteration 97 \t : \t loss 0.00424 - accuracy 0.99121\n",
      "iteration 98 \t : \t loss 0.00420 - accuracy 0.99121\n",
      "iteration 99 \t : \t loss 0.00416 - accuracy 0.99121\n",
      "iteration 100 \t : \t loss 0.00412 - accuracy 0.99121\n",
      "iteration 101 \t : \t loss 0.00408 - accuracy 0.99121\n",
      "iteration 102 \t : \t loss 0.00405 - accuracy 0.99121\n",
      "iteration 103 \t : \t loss 0.00401 - accuracy 0.99146\n",
      "iteration 104 \t : \t loss 0.00397 - accuracy 0.99170\n",
      "iteration 105 \t : \t loss 0.00393 - accuracy 0.99194\n",
      "iteration 106 \t : \t loss 0.00390 - accuracy 0.99219\n",
      "iteration 107 \t : \t loss 0.00386 - accuracy 0.99219\n",
      "iteration 108 \t : \t loss 0.00383 - accuracy 0.99219\n",
      "iteration 109 \t : \t loss 0.00379 - accuracy 0.99219\n",
      "iteration 110 \t : \t loss 0.00376 - accuracy 0.99243\n",
      "iteration 111 \t : \t loss 0.00372 - accuracy 0.99243\n",
      "iteration 112 \t : \t loss 0.00369 - accuracy 0.99268\n",
      "iteration 113 \t : \t loss 0.00366 - accuracy 0.99268\n",
      "iteration 114 \t : \t loss 0.00362 - accuracy 0.99292\n",
      "iteration 115 \t : \t loss 0.00359 - accuracy 0.99316\n",
      "iteration 116 \t : \t loss 0.00356 - accuracy 0.99341\n",
      "iteration 117 \t : \t loss 0.00353 - accuracy 0.99341\n",
      "iteration 118 \t : \t loss 0.00349 - accuracy 0.99341\n",
      "iteration 119 \t : \t loss 0.00346 - accuracy 0.99341\n",
      "iteration 120 \t : \t loss 0.00343 - accuracy 0.99365\n",
      "iteration 121 \t : \t loss 0.00340 - accuracy 0.99390\n",
      "iteration 122 \t : \t loss 0.00337 - accuracy 0.99390\n",
      "iteration 123 \t : \t loss 0.00334 - accuracy 0.99390\n",
      "iteration 124 \t : \t loss 0.00331 - accuracy 0.99414\n",
      "iteration 125 \t : \t loss 0.00328 - accuracy 0.99414\n",
      "iteration 126 \t : \t loss 0.00325 - accuracy 0.99414\n",
      "iteration 127 \t : \t loss 0.00322 - accuracy 0.99414\n",
      "iteration 128 \t : \t loss 0.00320 - accuracy 0.99414\n",
      "iteration 129 \t : \t loss 0.00317 - accuracy 0.99438\n",
      "iteration 130 \t : \t loss 0.00314 - accuracy 0.99438\n",
      "iteration 131 \t : \t loss 0.00311 - accuracy 0.99438\n",
      "iteration 132 \t : \t loss 0.00309 - accuracy 0.99438\n",
      "iteration 133 \t : \t loss 0.00306 - accuracy 0.99438\n",
      "iteration 134 \t : \t loss 0.00303 - accuracy 0.99438\n",
      "iteration 135 \t : \t loss 0.00301 - accuracy 0.99438\n",
      "iteration 136 \t : \t loss 0.00298 - accuracy 0.99487\n",
      "iteration 137 \t : \t loss 0.00295 - accuracy 0.99487\n",
      "iteration 138 \t : \t loss 0.00293 - accuracy 0.99487\n",
      "iteration 139 \t : \t loss 0.00290 - accuracy 0.99487\n",
      "iteration 140 \t : \t loss 0.00288 - accuracy 0.99487\n",
      "iteration 141 \t : \t loss 0.00285 - accuracy 0.99512\n",
      "iteration 142 \t : \t loss 0.00283 - accuracy 0.99512\n",
      "iteration 143 \t : \t loss 0.00281 - accuracy 0.99512\n",
      "iteration 144 \t : \t loss 0.00278 - accuracy 0.99512\n",
      "iteration 145 \t : \t loss 0.00276 - accuracy 0.99512\n",
      "iteration 146 \t : \t loss 0.00273 - accuracy 0.99536\n",
      "iteration 147 \t : \t loss 0.00271 - accuracy 0.99536\n",
      "iteration 148 \t : \t loss 0.00269 - accuracy 0.99561\n",
      "iteration 149 \t : \t loss 0.00267 - accuracy 0.99585\n",
      "iteration 150 \t : \t loss 0.00264 - accuracy 0.99585\n",
      "iteration 151 \t : \t loss 0.00262 - accuracy 0.99585\n",
      "iteration 152 \t : \t loss 0.00260 - accuracy 0.99585\n",
      "iteration 153 \t : \t loss 0.00258 - accuracy 0.99585\n",
      "iteration 154 \t : \t loss 0.00256 - accuracy 0.99585\n",
      "iteration 155 \t : \t loss 0.00254 - accuracy 0.99585\n",
      "iteration 156 \t : \t loss 0.00251 - accuracy 0.99585\n",
      "iteration 157 \t : \t loss 0.00249 - accuracy 0.99585\n",
      "iteration 158 \t : \t loss 0.00247 - accuracy 0.99609\n",
      "iteration 159 \t : \t loss 0.00245 - accuracy 0.99609\n",
      "iteration 160 \t : \t loss 0.00243 - accuracy 0.99609\n",
      "iteration 161 \t : \t loss 0.00241 - accuracy 0.99634\n",
      "iteration 162 \t : \t loss 0.00239 - accuracy 0.99634\n",
      "iteration 163 \t : \t loss 0.00237 - accuracy 0.99683\n",
      "iteration 164 \t : \t loss 0.00235 - accuracy 0.99707\n",
      "iteration 165 \t : \t loss 0.00233 - accuracy 0.99707\n",
      "iteration 166 \t : \t loss 0.00232 - accuracy 0.99707\n",
      "iteration 167 \t : \t loss 0.00230 - accuracy 0.99707\n",
      "iteration 168 \t : \t loss 0.00228 - accuracy 0.99707\n",
      "iteration 169 \t : \t loss 0.00226 - accuracy 0.99707\n",
      "iteration 170 \t : \t loss 0.00224 - accuracy 0.99707\n",
      "iteration 171 \t : \t loss 0.00222 - accuracy 0.99707\n",
      "iteration 172 \t : \t loss 0.00221 - accuracy 0.99707\n",
      "iteration 173 \t : \t loss 0.00219 - accuracy 0.99731\n",
      "iteration 174 \t : \t loss 0.00217 - accuracy 0.99731\n",
      "iteration 175 \t : \t loss 0.00215 - accuracy 0.99731\n",
      "iteration 176 \t : \t loss 0.00214 - accuracy 0.99731\n",
      "iteration 177 \t : \t loss 0.00212 - accuracy 0.99731\n",
      "iteration 178 \t : \t loss 0.00210 - accuracy 0.99731\n",
      "iteration 179 \t : \t loss 0.00209 - accuracy 0.99731\n",
      "iteration 180 \t : \t loss 0.00207 - accuracy 0.99780\n",
      "iteration 181 \t : \t loss 0.00206 - accuracy 0.99780\n",
      "iteration 182 \t : \t loss 0.00204 - accuracy 0.99780\n",
      "iteration 183 \t : \t loss 0.00202 - accuracy 0.99805\n",
      "iteration 184 \t : \t loss 0.00201 - accuracy 0.99805\n",
      "iteration 185 \t : \t loss 0.00199 - accuracy 0.99805\n",
      "iteration 186 \t : \t loss 0.00198 - accuracy 0.99805\n",
      "iteration 187 \t : \t loss 0.00196 - accuracy 0.99829\n",
      "iteration 188 \t : \t loss 0.00195 - accuracy 0.99829\n",
      "iteration 189 \t : \t loss 0.00193 - accuracy 0.99829\n",
      "iteration 190 \t : \t loss 0.00192 - accuracy 0.99829\n",
      "iteration 191 \t : \t loss 0.00190 - accuracy 0.99829\n",
      "iteration 192 \t : \t loss 0.00189 - accuracy 0.99829\n",
      "iteration 193 \t : \t loss 0.00188 - accuracy 0.99829\n",
      "iteration 194 \t : \t loss 0.00186 - accuracy 0.99829\n",
      "iteration 195 \t : \t loss 0.00185 - accuracy 0.99829\n",
      "iteration 196 \t : \t loss 0.00183 - accuracy 0.99829\n",
      "iteration 197 \t : \t loss 0.00182 - accuracy 0.99829\n",
      "iteration 198 \t : \t loss 0.00181 - accuracy 0.99829\n",
      "iteration 199 \t : \t loss 0.00179 - accuracy 0.99829\n",
      "iteration 0 \t : \t loss 0.17386 - accuracy 0.35229\n",
      "iteration 1 \t : \t loss 0.17787 - accuracy 0.36475\n",
      "iteration 2 \t : \t loss 0.17603 - accuracy 0.37964\n",
      "iteration 3 \t : \t loss 0.17386 - accuracy 0.39844\n",
      "iteration 4 \t : \t loss 0.17105 - accuracy 0.42505\n",
      "iteration 5 \t : \t loss 0.16709 - accuracy 0.46802\n",
      "iteration 6 \t : \t loss 0.16123 - accuracy 0.52393\n",
      "iteration 7 \t : \t loss 0.15247 - accuracy 0.57275\n",
      "iteration 8 \t : \t loss 0.14022 - accuracy 0.62256\n",
      "iteration 9 \t : \t loss 0.12560 - accuracy 0.65503\n",
      "iteration 10 \t : \t loss 0.11123 - accuracy 0.69189\n",
      "iteration 11 \t : \t loss 0.09891 - accuracy 0.72559\n",
      "iteration 12 \t : \t loss 0.08870 - accuracy 0.75513\n",
      "iteration 13 \t : \t loss 0.08013 - accuracy 0.77734\n",
      "iteration 14 \t : \t loss 0.07298 - accuracy 0.79663\n",
      "iteration 15 \t : \t loss 0.06708 - accuracy 0.81201\n",
      "iteration 16 \t : \t loss 0.06222 - accuracy 0.82642\n",
      "iteration 17 \t : \t loss 0.05816 - accuracy 0.83521\n",
      "iteration 18 \t : \t loss 0.05470 - accuracy 0.84375\n",
      "iteration 19 \t : \t loss 0.05169 - accuracy 0.85303\n",
      "iteration 20 \t : \t loss 0.04902 - accuracy 0.85986\n",
      "iteration 21 \t : \t loss 0.04662 - accuracy 0.86865\n",
      "iteration 22 \t : \t loss 0.04443 - accuracy 0.87329\n",
      "iteration 23 \t : \t loss 0.04242 - accuracy 0.88110\n",
      "iteration 24 \t : \t loss 0.04056 - accuracy 0.88550\n",
      "iteration 25 \t : \t loss 0.03885 - accuracy 0.89185\n",
      "iteration 26 \t : \t loss 0.03726 - accuracy 0.89722\n",
      "iteration 27 \t : \t loss 0.03580 - accuracy 0.90210\n",
      "iteration 28 \t : \t loss 0.03445 - accuracy 0.90601\n",
      "iteration 29 \t : \t loss 0.03320 - accuracy 0.91064\n",
      "iteration 30 \t : \t loss 0.03206 - accuracy 0.91333\n",
      "iteration 31 \t : \t loss 0.03100 - accuracy 0.91772\n",
      "iteration 32 \t : \t loss 0.03002 - accuracy 0.91992\n",
      "iteration 33 \t : \t loss 0.02911 - accuracy 0.92163\n",
      "iteration 34 \t : \t loss 0.02827 - accuracy 0.92480\n",
      "iteration 35 \t : \t loss 0.02748 - accuracy 0.92627\n",
      "iteration 36 \t : \t loss 0.02675 - accuracy 0.92798\n",
      "iteration 37 \t : \t loss 0.02606 - accuracy 0.92896\n",
      "iteration 38 \t : \t loss 0.02541 - accuracy 0.93091\n",
      "iteration 39 \t : \t loss 0.02480 - accuracy 0.93237\n",
      "iteration 40 \t : \t loss 0.02422 - accuracy 0.93359\n",
      "iteration 41 \t : \t loss 0.02366 - accuracy 0.93555\n",
      "iteration 42 \t : \t loss 0.02314 - accuracy 0.93701\n",
      "iteration 43 \t : \t loss 0.02263 - accuracy 0.93823\n",
      "iteration 44 \t : \t loss 0.02215 - accuracy 0.93945\n",
      "iteration 45 \t : \t loss 0.02169 - accuracy 0.94116\n",
      "iteration 46 \t : \t loss 0.02124 - accuracy 0.94263\n",
      "iteration 47 \t : \t loss 0.02081 - accuracy 0.94360\n",
      "iteration 48 \t : \t loss 0.02039 - accuracy 0.94556\n",
      "iteration 49 \t : \t loss 0.01998 - accuracy 0.94629\n",
      "iteration 50 \t : \t loss 0.01959 - accuracy 0.94727\n",
      "iteration 51 \t : \t loss 0.01921 - accuracy 0.94751\n",
      "iteration 52 \t : \t loss 0.01883 - accuracy 0.94873\n",
      "iteration 53 \t : \t loss 0.01847 - accuracy 0.94995\n",
      "iteration 54 \t : \t loss 0.01812 - accuracy 0.95166\n",
      "iteration 55 \t : \t loss 0.01777 - accuracy 0.95361\n",
      "iteration 56 \t : \t loss 0.01743 - accuracy 0.95435\n",
      "iteration 57 \t : \t loss 0.01710 - accuracy 0.95483\n",
      "iteration 58 \t : \t loss 0.01678 - accuracy 0.95581\n",
      "iteration 59 \t : \t loss 0.01646 - accuracy 0.95728\n",
      "iteration 60 \t : \t loss 0.01615 - accuracy 0.95825\n",
      "iteration 61 \t : \t loss 0.01585 - accuracy 0.95972\n",
      "iteration 62 \t : \t loss 0.01555 - accuracy 0.96045\n",
      "iteration 63 \t : \t loss 0.01526 - accuracy 0.96118\n",
      "iteration 64 \t : \t loss 0.01497 - accuracy 0.96143\n",
      "iteration 65 \t : \t loss 0.01469 - accuracy 0.96289\n",
      "iteration 66 \t : \t loss 0.01441 - accuracy 0.96362\n",
      "iteration 67 \t : \t loss 0.01414 - accuracy 0.96509\n",
      "iteration 68 \t : \t loss 0.01387 - accuracy 0.96558\n",
      "iteration 69 \t : \t loss 0.01361 - accuracy 0.96582\n",
      "iteration 70 \t : \t loss 0.01335 - accuracy 0.96655\n",
      "iteration 71 \t : \t loss 0.01310 - accuracy 0.96680\n",
      "iteration 72 \t : \t loss 0.01285 - accuracy 0.96777\n",
      "iteration 73 \t : \t loss 0.01260 - accuracy 0.96899\n",
      "iteration 74 \t : \t loss 0.01236 - accuracy 0.96973\n",
      "iteration 75 \t : \t loss 0.01212 - accuracy 0.97095\n",
      "iteration 76 \t : \t loss 0.01189 - accuracy 0.97217\n",
      "iteration 77 \t : \t loss 0.01165 - accuracy 0.97217\n",
      "iteration 78 \t : \t loss 0.01143 - accuracy 0.97241\n",
      "iteration 79 \t : \t loss 0.01120 - accuracy 0.97388\n",
      "iteration 80 \t : \t loss 0.01098 - accuracy 0.97461\n",
      "iteration 81 \t : \t loss 0.01076 - accuracy 0.97559\n",
      "iteration 82 \t : \t loss 0.01055 - accuracy 0.97607\n",
      "iteration 83 \t : \t loss 0.01034 - accuracy 0.97729\n",
      "iteration 84 \t : \t loss 0.01013 - accuracy 0.97803\n",
      "iteration 85 \t : \t loss 0.00993 - accuracy 0.97876\n",
      "iteration 86 \t : \t loss 0.00972 - accuracy 0.98022\n",
      "iteration 87 \t : \t loss 0.00953 - accuracy 0.98145\n",
      "iteration 88 \t : \t loss 0.00933 - accuracy 0.98193\n",
      "iteration 89 \t : \t loss 0.00914 - accuracy 0.98291\n",
      "iteration 90 \t : \t loss 0.00895 - accuracy 0.98340\n",
      "iteration 91 \t : \t loss 0.00876 - accuracy 0.98438\n",
      "iteration 92 \t : \t loss 0.00858 - accuracy 0.98486\n",
      "iteration 93 \t : \t loss 0.00840 - accuracy 0.98511\n",
      "iteration 94 \t : \t loss 0.00822 - accuracy 0.98535\n",
      "iteration 95 \t : \t loss 0.00805 - accuracy 0.98584\n",
      "iteration 96 \t : \t loss 0.00788 - accuracy 0.98608\n",
      "iteration 97 \t : \t loss 0.00771 - accuracy 0.98706\n",
      "iteration 98 \t : \t loss 0.00755 - accuracy 0.98755\n",
      "iteration 99 \t : \t loss 0.00739 - accuracy 0.98804\n",
      "iteration 100 \t : \t loss 0.00723 - accuracy 0.98853\n",
      "iteration 101 \t : \t loss 0.00707 - accuracy 0.98901\n",
      "iteration 102 \t : \t loss 0.00692 - accuracy 0.98950\n",
      "iteration 103 \t : \t loss 0.00677 - accuracy 0.98999\n",
      "iteration 104 \t : \t loss 0.00663 - accuracy 0.99023\n",
      "iteration 105 \t : \t loss 0.00648 - accuracy 0.99048\n",
      "iteration 106 \t : \t loss 0.00634 - accuracy 0.99072\n",
      "iteration 107 \t : \t loss 0.00621 - accuracy 0.99146\n",
      "iteration 108 \t : \t loss 0.00607 - accuracy 0.99146\n",
      "iteration 109 \t : \t loss 0.00594 - accuracy 0.99268\n",
      "iteration 110 \t : \t loss 0.00581 - accuracy 0.99268\n",
      "iteration 111 \t : \t loss 0.00569 - accuracy 0.99292\n",
      "iteration 112 \t : \t loss 0.00557 - accuracy 0.99341\n",
      "iteration 113 \t : \t loss 0.00545 - accuracy 0.99390\n",
      "iteration 114 \t : \t loss 0.00533 - accuracy 0.99390\n",
      "iteration 115 \t : \t loss 0.00522 - accuracy 0.99438\n",
      "iteration 116 \t : \t loss 0.00510 - accuracy 0.99438\n",
      "iteration 117 \t : \t loss 0.00499 - accuracy 0.99536\n",
      "iteration 118 \t : \t loss 0.00489 - accuracy 0.99561\n",
      "iteration 119 \t : \t loss 0.00478 - accuracy 0.99609\n",
      "iteration 120 \t : \t loss 0.00468 - accuracy 0.99634\n",
      "iteration 121 \t : \t loss 0.00458 - accuracy 0.99634\n",
      "iteration 122 \t : \t loss 0.00449 - accuracy 0.99658\n",
      "iteration 123 \t : \t loss 0.00439 - accuracy 0.99658\n",
      "iteration 124 \t : \t loss 0.00430 - accuracy 0.99683\n",
      "iteration 125 \t : \t loss 0.00421 - accuracy 0.99683\n",
      "iteration 126 \t : \t loss 0.00412 - accuracy 0.99683\n",
      "iteration 127 \t : \t loss 0.00403 - accuracy 0.99731\n",
      "iteration 128 \t : \t loss 0.00395 - accuracy 0.99731\n",
      "iteration 129 \t : \t loss 0.00387 - accuracy 0.99731\n",
      "iteration 130 \t : \t loss 0.00379 - accuracy 0.99731\n",
      "iteration 131 \t : \t loss 0.00371 - accuracy 0.99731\n",
      "iteration 132 \t : \t loss 0.00364 - accuracy 0.99780\n",
      "iteration 133 \t : \t loss 0.00356 - accuracy 0.99780\n",
      "iteration 134 \t : \t loss 0.00349 - accuracy 0.99780\n",
      "iteration 135 \t : \t loss 0.00342 - accuracy 0.99805\n",
      "iteration 136 \t : \t loss 0.00335 - accuracy 0.99829\n",
      "iteration 137 \t : \t loss 0.00328 - accuracy 0.99829\n",
      "iteration 138 \t : \t loss 0.00322 - accuracy 0.99829\n",
      "iteration 139 \t : \t loss 0.00315 - accuracy 0.99829\n",
      "iteration 140 \t : \t loss 0.00309 - accuracy 0.99829\n",
      "iteration 141 \t : \t loss 0.00303 - accuracy 0.99829\n",
      "iteration 142 \t : \t loss 0.00297 - accuracy 0.99829\n",
      "iteration 143 \t : \t loss 0.00291 - accuracy 0.99829\n",
      "iteration 144 \t : \t loss 0.00286 - accuracy 0.99829\n",
      "iteration 145 \t : \t loss 0.00280 - accuracy 0.99829\n",
      "iteration 146 \t : \t loss 0.00275 - accuracy 0.99829\n",
      "iteration 147 \t : \t loss 0.00269 - accuracy 0.99854\n",
      "iteration 148 \t : \t loss 0.00264 - accuracy 0.99854\n",
      "iteration 149 \t : \t loss 0.00259 - accuracy 0.99854\n",
      "iteration 150 \t : \t loss 0.00254 - accuracy 0.99902\n",
      "iteration 151 \t : \t loss 0.00250 - accuracy 0.99902\n",
      "iteration 152 \t : \t loss 0.00245 - accuracy 0.99902\n",
      "iteration 153 \t : \t loss 0.00240 - accuracy 0.99902\n",
      "iteration 154 \t : \t loss 0.00236 - accuracy 0.99902\n",
      "iteration 155 \t : \t loss 0.00232 - accuracy 0.99902\n",
      "iteration 156 \t : \t loss 0.00227 - accuracy 0.99902\n",
      "iteration 157 \t : \t loss 0.00223 - accuracy 0.99902\n",
      "iteration 158 \t : \t loss 0.00219 - accuracy 0.99902\n",
      "iteration 159 \t : \t loss 0.00215 - accuracy 0.99902\n",
      "iteration 160 \t : \t loss 0.00211 - accuracy 0.99902\n",
      "iteration 161 \t : \t loss 0.00208 - accuracy 0.99902\n",
      "iteration 162 \t : \t loss 0.00204 - accuracy 0.99927\n",
      "iteration 163 \t : \t loss 0.00200 - accuracy 0.99951\n",
      "iteration 164 \t : \t loss 0.00197 - accuracy 0.99951\n",
      "iteration 165 \t : \t loss 0.00193 - accuracy 0.99951\n",
      "iteration 166 \t : \t loss 0.00190 - accuracy 0.99951\n",
      "iteration 167 \t : \t loss 0.00187 - accuracy 0.99951\n",
      "iteration 168 \t : \t loss 0.00184 - accuracy 0.99951\n",
      "iteration 169 \t : \t loss 0.00180 - accuracy 0.99951\n",
      "iteration 170 \t : \t loss 0.00177 - accuracy 0.99951\n",
      "iteration 171 \t : \t loss 0.00174 - accuracy 0.99951\n",
      "iteration 172 \t : \t loss 0.00171 - accuracy 0.99951\n",
      "iteration 173 \t : \t loss 0.00169 - accuracy 0.99951\n",
      "iteration 174 \t : \t loss 0.00166 - accuracy 0.99951\n",
      "iteration 175 \t : \t loss 0.00163 - accuracy 0.99951\n",
      "iteration 176 \t : \t loss 0.00160 - accuracy 0.99951\n",
      "iteration 177 \t : \t loss 0.00158 - accuracy 0.99951\n",
      "iteration 178 \t : \t loss 0.00155 - accuracy 0.99951\n",
      "iteration 179 \t : \t loss 0.00153 - accuracy 0.99951\n",
      "iteration 180 \t : \t loss 0.00150 - accuracy 0.99976\n",
      "iteration 181 \t : \t loss 0.00148 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00145 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00143 - accuracy 0.99976\n",
      "iteration 184 \t : \t loss 0.00141 - accuracy 0.99976\n",
      "iteration 185 \t : \t loss 0.00139 - accuracy 0.99976\n",
      "iteration 186 \t : \t loss 0.00136 - accuracy 0.99976\n",
      "iteration 187 \t : \t loss 0.00134 - accuracy 0.99976\n",
      "iteration 188 \t : \t loss 0.00132 - accuracy 0.99976\n",
      "iteration 189 \t : \t loss 0.00130 - accuracy 0.99976\n",
      "iteration 190 \t : \t loss 0.00128 - accuracy 0.99976\n",
      "iteration 191 \t : \t loss 0.00126 - accuracy 0.99976\n",
      "iteration 192 \t : \t loss 0.00124 - accuracy 0.99976\n",
      "iteration 193 \t : \t loss 0.00122 - accuracy 0.99976\n",
      "iteration 194 \t : \t loss 0.00121 - accuracy 0.99976\n",
      "iteration 195 \t : \t loss 0.00119 - accuracy 0.99976\n",
      "iteration 196 \t : \t loss 0.00117 - accuracy 0.99976\n",
      "iteration 197 \t : \t loss 0.00115 - accuracy 0.99976\n",
      "iteration 198 \t : \t loss 0.00114 - accuracy 0.99976\n",
      "iteration 199 \t : \t loss 0.00112 - accuracy 0.99976\n",
      "iteration 0 \t : \t loss 0.10677 - accuracy 0.70142\n",
      "iteration 1 \t : \t loss 0.05566 - accuracy 0.86792\n",
      "iteration 2 \t : \t loss 0.04209 - accuracy 0.89722\n",
      "iteration 3 \t : \t loss 0.03600 - accuracy 0.90747\n",
      "iteration 4 \t : \t loss 0.03242 - accuracy 0.91626\n",
      "iteration 5 \t : \t loss 0.02999 - accuracy 0.91968\n",
      "iteration 6 \t : \t loss 0.02819 - accuracy 0.92334\n",
      "iteration 7 \t : \t loss 0.02676 - accuracy 0.92505\n",
      "iteration 8 \t : \t loss 0.02558 - accuracy 0.92700\n",
      "iteration 9 \t : \t loss 0.02459 - accuracy 0.92749\n",
      "iteration 10 \t : \t loss 0.02372 - accuracy 0.92944\n",
      "iteration 11 \t : \t loss 0.02294 - accuracy 0.93262\n",
      "iteration 12 \t : \t loss 0.02225 - accuracy 0.93408\n",
      "iteration 13 \t : \t loss 0.02162 - accuracy 0.93604\n",
      "iteration 14 \t : \t loss 0.02104 - accuracy 0.93970\n",
      "iteration 15 \t : \t loss 0.02050 - accuracy 0.94214\n",
      "iteration 16 \t : \t loss 0.02000 - accuracy 0.94434\n",
      "iteration 17 \t : \t loss 0.01952 - accuracy 0.94531\n",
      "iteration 18 \t : \t loss 0.01908 - accuracy 0.94678\n",
      "iteration 19 \t : \t loss 0.01866 - accuracy 0.94897\n",
      "iteration 20 \t : \t loss 0.01826 - accuracy 0.94946\n",
      "iteration 21 \t : \t loss 0.01788 - accuracy 0.95068\n",
      "iteration 22 \t : \t loss 0.01751 - accuracy 0.95142\n",
      "iteration 23 \t : \t loss 0.01716 - accuracy 0.95288\n",
      "iteration 24 \t : \t loss 0.01683 - accuracy 0.95459\n",
      "iteration 25 \t : \t loss 0.01650 - accuracy 0.95508\n",
      "iteration 26 \t : \t loss 0.01619 - accuracy 0.95581\n",
      "iteration 27 \t : \t loss 0.01589 - accuracy 0.95630\n",
      "iteration 28 \t : \t loss 0.01560 - accuracy 0.95728\n",
      "iteration 29 \t : \t loss 0.01532 - accuracy 0.95776\n",
      "iteration 30 \t : \t loss 0.01504 - accuracy 0.95801\n",
      "iteration 31 \t : \t loss 0.01478 - accuracy 0.95898\n",
      "iteration 32 \t : \t loss 0.01452 - accuracy 0.95947\n",
      "iteration 33 \t : \t loss 0.01427 - accuracy 0.96045\n",
      "iteration 34 \t : \t loss 0.01403 - accuracy 0.96118\n",
      "iteration 35 \t : \t loss 0.01379 - accuracy 0.96265\n",
      "iteration 36 \t : \t loss 0.01356 - accuracy 0.96289\n",
      "iteration 37 \t : \t loss 0.01333 - accuracy 0.96411\n",
      "iteration 38 \t : \t loss 0.01311 - accuracy 0.96411\n",
      "iteration 39 \t : \t loss 0.01290 - accuracy 0.96436\n",
      "iteration 40 \t : \t loss 0.01269 - accuracy 0.96655\n",
      "iteration 41 \t : \t loss 0.01249 - accuracy 0.96704\n",
      "iteration 42 \t : \t loss 0.01229 - accuracy 0.96753\n",
      "iteration 43 \t : \t loss 0.01209 - accuracy 0.96753\n",
      "iteration 44 \t : \t loss 0.01190 - accuracy 0.96777\n",
      "iteration 45 \t : \t loss 0.01171 - accuracy 0.96826\n",
      "iteration 46 \t : \t loss 0.01153 - accuracy 0.96875\n",
      "iteration 47 \t : \t loss 0.01135 - accuracy 0.96899\n",
      "iteration 48 \t : \t loss 0.01117 - accuracy 0.97021\n",
      "iteration 49 \t : \t loss 0.01100 - accuracy 0.97021\n",
      "iteration 50 \t : \t loss 0.01083 - accuracy 0.97070\n",
      "iteration 51 \t : \t loss 0.01067 - accuracy 0.97144\n",
      "iteration 52 \t : \t loss 0.01050 - accuracy 0.97314\n",
      "iteration 53 \t : \t loss 0.01034 - accuracy 0.97363\n",
      "iteration 54 \t : \t loss 0.01019 - accuracy 0.97412\n",
      "iteration 55 \t : \t loss 0.01004 - accuracy 0.97510\n",
      "iteration 56 \t : \t loss 0.00989 - accuracy 0.97583\n",
      "iteration 57 \t : \t loss 0.00974 - accuracy 0.97607\n",
      "iteration 58 \t : \t loss 0.00959 - accuracy 0.97705\n",
      "iteration 59 \t : \t loss 0.00945 - accuracy 0.97778\n",
      "iteration 60 \t : \t loss 0.00931 - accuracy 0.97876\n",
      "iteration 61 \t : \t loss 0.00918 - accuracy 0.97876\n",
      "iteration 62 \t : \t loss 0.00904 - accuracy 0.97876\n",
      "iteration 63 \t : \t loss 0.00891 - accuracy 0.97949\n",
      "iteration 64 \t : \t loss 0.00878 - accuracy 0.97974\n",
      "iteration 65 \t : \t loss 0.00866 - accuracy 0.98022\n",
      "iteration 66 \t : \t loss 0.00853 - accuracy 0.98071\n",
      "iteration 67 \t : \t loss 0.00841 - accuracy 0.98145\n",
      "iteration 68 \t : \t loss 0.00829 - accuracy 0.98169\n",
      "iteration 69 \t : \t loss 0.00817 - accuracy 0.98267\n",
      "iteration 70 \t : \t loss 0.00805 - accuracy 0.98315\n",
      "iteration 71 \t : \t loss 0.00794 - accuracy 0.98413\n",
      "iteration 72 \t : \t loss 0.00783 - accuracy 0.98462\n",
      "iteration 73 \t : \t loss 0.00772 - accuracy 0.98462\n",
      "iteration 74 \t : \t loss 0.00761 - accuracy 0.98486\n",
      "iteration 75 \t : \t loss 0.00751 - accuracy 0.98511\n",
      "iteration 76 \t : \t loss 0.00740 - accuracy 0.98511\n",
      "iteration 77 \t : \t loss 0.00730 - accuracy 0.98511\n",
      "iteration 78 \t : \t loss 0.00720 - accuracy 0.98535\n",
      "iteration 79 \t : \t loss 0.00710 - accuracy 0.98560\n",
      "iteration 80 \t : \t loss 0.00700 - accuracy 0.98608\n",
      "iteration 81 \t : \t loss 0.00691 - accuracy 0.98633\n",
      "iteration 82 \t : \t loss 0.00681 - accuracy 0.98657\n",
      "iteration 83 \t : \t loss 0.00672 - accuracy 0.98730\n",
      "iteration 84 \t : \t loss 0.00663 - accuracy 0.98755\n",
      "iteration 85 \t : \t loss 0.00654 - accuracy 0.98755\n",
      "iteration 86 \t : \t loss 0.00645 - accuracy 0.98804\n",
      "iteration 87 \t : \t loss 0.00637 - accuracy 0.98828\n",
      "iteration 88 \t : \t loss 0.00628 - accuracy 0.98853\n",
      "iteration 89 \t : \t loss 0.00620 - accuracy 0.98853\n",
      "iteration 90 \t : \t loss 0.00611 - accuracy 0.98877\n",
      "iteration 91 \t : \t loss 0.00603 - accuracy 0.98901\n",
      "iteration 92 \t : \t loss 0.00595 - accuracy 0.98901\n",
      "iteration 93 \t : \t loss 0.00588 - accuracy 0.98975\n",
      "iteration 94 \t : \t loss 0.00580 - accuracy 0.99023\n",
      "iteration 95 \t : \t loss 0.00572 - accuracy 0.99048\n",
      "iteration 96 \t : \t loss 0.00565 - accuracy 0.99072\n",
      "iteration 97 \t : \t loss 0.00558 - accuracy 0.99121\n",
      "iteration 98 \t : \t loss 0.00550 - accuracy 0.99146\n",
      "iteration 99 \t : \t loss 0.00543 - accuracy 0.99194\n",
      "iteration 100 \t : \t loss 0.00536 - accuracy 0.99194\n",
      "iteration 101 \t : \t loss 0.00530 - accuracy 0.99194\n",
      "iteration 102 \t : \t loss 0.00523 - accuracy 0.99219\n",
      "iteration 103 \t : \t loss 0.00516 - accuracy 0.99219\n",
      "iteration 104 \t : \t loss 0.00510 - accuracy 0.99243\n",
      "iteration 105 \t : \t loss 0.00503 - accuracy 0.99268\n",
      "iteration 106 \t : \t loss 0.00497 - accuracy 0.99292\n",
      "iteration 107 \t : \t loss 0.00491 - accuracy 0.99341\n",
      "iteration 108 \t : \t loss 0.00484 - accuracy 0.99365\n",
      "iteration 109 \t : \t loss 0.00478 - accuracy 0.99390\n",
      "iteration 110 \t : \t loss 0.00472 - accuracy 0.99390\n",
      "iteration 111 \t : \t loss 0.00467 - accuracy 0.99414\n",
      "iteration 112 \t : \t loss 0.00461 - accuracy 0.99487\n",
      "iteration 113 \t : \t loss 0.00455 - accuracy 0.99487\n",
      "iteration 114 \t : \t loss 0.00450 - accuracy 0.99487\n",
      "iteration 115 \t : \t loss 0.00444 - accuracy 0.99487\n",
      "iteration 116 \t : \t loss 0.00439 - accuracy 0.99512\n",
      "iteration 117 \t : \t loss 0.00433 - accuracy 0.99536\n",
      "iteration 118 \t : \t loss 0.00428 - accuracy 0.99536\n",
      "iteration 119 \t : \t loss 0.00423 - accuracy 0.99536\n",
      "iteration 120 \t : \t loss 0.00418 - accuracy 0.99536\n",
      "iteration 121 \t : \t loss 0.00413 - accuracy 0.99536\n",
      "iteration 122 \t : \t loss 0.00408 - accuracy 0.99536\n",
      "iteration 123 \t : \t loss 0.00403 - accuracy 0.99536\n",
      "iteration 124 \t : \t loss 0.00398 - accuracy 0.99536\n",
      "iteration 125 \t : \t loss 0.00394 - accuracy 0.99561\n",
      "iteration 126 \t : \t loss 0.00389 - accuracy 0.99561\n",
      "iteration 127 \t : \t loss 0.00384 - accuracy 0.99561\n",
      "iteration 128 \t : \t loss 0.00380 - accuracy 0.99561\n",
      "iteration 129 \t : \t loss 0.00375 - accuracy 0.99561\n",
      "iteration 130 \t : \t loss 0.00371 - accuracy 0.99609\n",
      "iteration 131 \t : \t loss 0.00367 - accuracy 0.99609\n",
      "iteration 132 \t : \t loss 0.00363 - accuracy 0.99658\n",
      "iteration 133 \t : \t loss 0.00358 - accuracy 0.99658\n",
      "iteration 134 \t : \t loss 0.00354 - accuracy 0.99658\n",
      "iteration 135 \t : \t loss 0.00350 - accuracy 0.99658\n",
      "iteration 136 \t : \t loss 0.00346 - accuracy 0.99658\n",
      "iteration 137 \t : \t loss 0.00342 - accuracy 0.99658\n",
      "iteration 138 \t : \t loss 0.00338 - accuracy 0.99683\n",
      "iteration 139 \t : \t loss 0.00335 - accuracy 0.99683\n",
      "iteration 140 \t : \t loss 0.00331 - accuracy 0.99707\n",
      "iteration 141 \t : \t loss 0.00327 - accuracy 0.99731\n",
      "iteration 142 \t : \t loss 0.00324 - accuracy 0.99756\n",
      "iteration 143 \t : \t loss 0.00320 - accuracy 0.99756\n",
      "iteration 144 \t : \t loss 0.00317 - accuracy 0.99756\n",
      "iteration 145 \t : \t loss 0.00313 - accuracy 0.99756\n",
      "iteration 146 \t : \t loss 0.00310 - accuracy 0.99756\n",
      "iteration 147 \t : \t loss 0.00306 - accuracy 0.99756\n",
      "iteration 148 \t : \t loss 0.00303 - accuracy 0.99756\n",
      "iteration 149 \t : \t loss 0.00300 - accuracy 0.99780\n",
      "iteration 150 \t : \t loss 0.00296 - accuracy 0.99780\n",
      "iteration 151 \t : \t loss 0.00293 - accuracy 0.99780\n",
      "iteration 152 \t : \t loss 0.00290 - accuracy 0.99780\n",
      "iteration 153 \t : \t loss 0.00287 - accuracy 0.99780\n",
      "iteration 154 \t : \t loss 0.00284 - accuracy 0.99780\n",
      "iteration 155 \t : \t loss 0.00281 - accuracy 0.99780\n",
      "iteration 156 \t : \t loss 0.00278 - accuracy 0.99780\n",
      "iteration 157 \t : \t loss 0.00275 - accuracy 0.99805\n",
      "iteration 158 \t : \t loss 0.00272 - accuracy 0.99805\n",
      "iteration 159 \t : \t loss 0.00270 - accuracy 0.99805\n",
      "iteration 160 \t : \t loss 0.00267 - accuracy 0.99805\n",
      "iteration 161 \t : \t loss 0.00264 - accuracy 0.99805\n",
      "iteration 162 \t : \t loss 0.00261 - accuracy 0.99805\n",
      "iteration 163 \t : \t loss 0.00259 - accuracy 0.99805\n",
      "iteration 164 \t : \t loss 0.00256 - accuracy 0.99805\n",
      "iteration 165 \t : \t loss 0.00254 - accuracy 0.99805\n",
      "iteration 166 \t : \t loss 0.00251 - accuracy 0.99829\n",
      "iteration 167 \t : \t loss 0.00249 - accuracy 0.99829\n",
      "iteration 168 \t : \t loss 0.00246 - accuracy 0.99829\n",
      "iteration 169 \t : \t loss 0.00244 - accuracy 0.99829\n",
      "iteration 170 \t : \t loss 0.00241 - accuracy 0.99829\n",
      "iteration 171 \t : \t loss 0.00239 - accuracy 0.99829\n",
      "iteration 172 \t : \t loss 0.00237 - accuracy 0.99829\n",
      "iteration 173 \t : \t loss 0.00234 - accuracy 0.99829\n",
      "iteration 174 \t : \t loss 0.00232 - accuracy 0.99829\n",
      "iteration 175 \t : \t loss 0.00230 - accuracy 0.99829\n",
      "iteration 176 \t : \t loss 0.00228 - accuracy 0.99829\n",
      "iteration 177 \t : \t loss 0.00226 - accuracy 0.99829\n",
      "iteration 178 \t : \t loss 0.00223 - accuracy 0.99829\n",
      "iteration 179 \t : \t loss 0.00221 - accuracy 0.99829\n",
      "iteration 180 \t : \t loss 0.00219 - accuracy 0.99829\n",
      "iteration 181 \t : \t loss 0.00217 - accuracy 0.99829\n",
      "iteration 182 \t : \t loss 0.00215 - accuracy 0.99829\n",
      "iteration 183 \t : \t loss 0.00213 - accuracy 0.99829\n",
      "iteration 184 \t : \t loss 0.00211 - accuracy 0.99829\n",
      "iteration 185 \t : \t loss 0.00209 - accuracy 0.99829\n",
      "iteration 186 \t : \t loss 0.00207 - accuracy 0.99854\n",
      "iteration 187 \t : \t loss 0.00206 - accuracy 0.99854\n",
      "iteration 188 \t : \t loss 0.00204 - accuracy 0.99854\n",
      "iteration 189 \t : \t loss 0.00202 - accuracy 0.99902\n",
      "iteration 190 \t : \t loss 0.00200 - accuracy 0.99902\n",
      "iteration 191 \t : \t loss 0.00198 - accuracy 0.99902\n",
      "iteration 192 \t : \t loss 0.00197 - accuracy 0.99902\n",
      "iteration 193 \t : \t loss 0.00195 - accuracy 0.99902\n",
      "iteration 194 \t : \t loss 0.00193 - accuracy 0.99902\n",
      "iteration 195 \t : \t loss 0.00191 - accuracy 0.99902\n",
      "iteration 196 \t : \t loss 0.00190 - accuracy 0.99902\n",
      "iteration 197 \t : \t loss 0.00188 - accuracy 0.99902\n",
      "iteration 198 \t : \t loss 0.00187 - accuracy 0.99902\n",
      "iteration 199 \t : \t loss 0.00185 - accuracy 0.99902\n",
      "iteration 0 \t : \t loss 0.23215 - accuracy 0.11401\n",
      "iteration 1 \t : \t loss 0.22877 - accuracy 0.13428\n",
      "iteration 2 \t : \t loss 0.22489 - accuracy 0.17578\n",
      "iteration 3 \t : \t loss 0.21926 - accuracy 0.24390\n",
      "iteration 4 \t : \t loss 0.21016 - accuracy 0.34277\n",
      "iteration 5 \t : \t loss 0.19510 - accuracy 0.43457\n",
      "iteration 6 \t : \t loss 0.17334 - accuracy 0.50903\n",
      "iteration 7 \t : \t loss 0.14954 - accuracy 0.56348\n",
      "iteration 8 \t : \t loss 0.12923 - accuracy 0.61816\n",
      "iteration 9 \t : \t loss 0.11342 - accuracy 0.66064\n",
      "iteration 10 \t : \t loss 0.10121 - accuracy 0.69873\n",
      "iteration 11 \t : \t loss 0.09175 - accuracy 0.72461\n",
      "iteration 12 \t : \t loss 0.08420 - accuracy 0.74634\n",
      "iteration 13 \t : \t loss 0.07793 - accuracy 0.76636\n",
      "iteration 14 \t : \t loss 0.07255 - accuracy 0.78516\n",
      "iteration 15 \t : \t loss 0.06787 - accuracy 0.80298\n",
      "iteration 16 \t : \t loss 0.06375 - accuracy 0.81226\n",
      "iteration 17 \t : \t loss 0.06014 - accuracy 0.82349\n",
      "iteration 18 \t : \t loss 0.05696 - accuracy 0.83154\n",
      "iteration 19 \t : \t loss 0.05415 - accuracy 0.84058\n",
      "iteration 20 \t : \t loss 0.05165 - accuracy 0.85034\n",
      "iteration 21 \t : \t loss 0.04942 - accuracy 0.85815\n",
      "iteration 22 \t : \t loss 0.04741 - accuracy 0.86328\n",
      "iteration 23 \t : \t loss 0.04561 - accuracy 0.86938\n",
      "iteration 24 \t : \t loss 0.04397 - accuracy 0.87622\n",
      "iteration 25 \t : \t loss 0.04247 - accuracy 0.87988\n",
      "iteration 26 \t : \t loss 0.04111 - accuracy 0.88379\n",
      "iteration 27 \t : \t loss 0.03985 - accuracy 0.88647\n",
      "iteration 28 \t : \t loss 0.03870 - accuracy 0.88965\n",
      "iteration 29 \t : \t loss 0.03763 - accuracy 0.89209\n",
      "iteration 30 \t : \t loss 0.03663 - accuracy 0.89697\n",
      "iteration 31 \t : \t loss 0.03571 - accuracy 0.89990\n",
      "iteration 32 \t : \t loss 0.03484 - accuracy 0.90259\n",
      "iteration 33 \t : \t loss 0.03402 - accuracy 0.90356\n",
      "iteration 34 \t : \t loss 0.03325 - accuracy 0.90479\n",
      "iteration 35 \t : \t loss 0.03251 - accuracy 0.90723\n",
      "iteration 36 \t : \t loss 0.03182 - accuracy 0.90942\n",
      "iteration 37 \t : \t loss 0.03115 - accuracy 0.91089\n",
      "iteration 38 \t : \t loss 0.03052 - accuracy 0.91211\n",
      "iteration 39 \t : \t loss 0.02991 - accuracy 0.91479\n",
      "iteration 40 \t : \t loss 0.02932 - accuracy 0.91650\n",
      "iteration 41 \t : \t loss 0.02876 - accuracy 0.91772\n",
      "iteration 42 \t : \t loss 0.02821 - accuracy 0.91943\n",
      "iteration 43 \t : \t loss 0.02769 - accuracy 0.92188\n",
      "iteration 44 \t : \t loss 0.02717 - accuracy 0.92334\n",
      "iteration 45 \t : \t loss 0.02667 - accuracy 0.92529\n",
      "iteration 46 \t : \t loss 0.02619 - accuracy 0.92676\n",
      "iteration 47 \t : \t loss 0.02572 - accuracy 0.92847\n",
      "iteration 48 \t : \t loss 0.02525 - accuracy 0.93018\n",
      "iteration 49 \t : \t loss 0.02480 - accuracy 0.93188\n",
      "iteration 50 \t : \t loss 0.02436 - accuracy 0.93237\n",
      "iteration 51 \t : \t loss 0.02392 - accuracy 0.93359\n",
      "iteration 52 \t : \t loss 0.02350 - accuracy 0.93481\n",
      "iteration 53 \t : \t loss 0.02308 - accuracy 0.93555\n",
      "iteration 54 \t : \t loss 0.02267 - accuracy 0.93677\n",
      "iteration 55 \t : \t loss 0.02226 - accuracy 0.93774\n",
      "iteration 56 \t : \t loss 0.02186 - accuracy 0.93872\n",
      "iteration 57 \t : \t loss 0.02147 - accuracy 0.94067\n",
      "iteration 58 \t : \t loss 0.02108 - accuracy 0.94165\n",
      "iteration 59 \t : \t loss 0.02070 - accuracy 0.94263\n",
      "iteration 60 \t : \t loss 0.02032 - accuracy 0.94385\n",
      "iteration 61 \t : \t loss 0.01995 - accuracy 0.94409\n",
      "iteration 62 \t : \t loss 0.01958 - accuracy 0.94580\n",
      "iteration 63 \t : \t loss 0.01922 - accuracy 0.94775\n",
      "iteration 64 \t : \t loss 0.01886 - accuracy 0.94922\n",
      "iteration 65 \t : \t loss 0.01851 - accuracy 0.94995\n",
      "iteration 66 \t : \t loss 0.01816 - accuracy 0.95117\n",
      "iteration 67 \t : \t loss 0.01781 - accuracy 0.95288\n",
      "iteration 68 \t : \t loss 0.01747 - accuracy 0.95459\n",
      "iteration 69 \t : \t loss 0.01713 - accuracy 0.95679\n",
      "iteration 70 \t : \t loss 0.01680 - accuracy 0.95776\n",
      "iteration 71 \t : \t loss 0.01647 - accuracy 0.95947\n",
      "iteration 72 \t : \t loss 0.01614 - accuracy 0.95996\n",
      "iteration 73 \t : \t loss 0.01582 - accuracy 0.96094\n",
      "iteration 74 \t : \t loss 0.01550 - accuracy 0.96191\n",
      "iteration 75 \t : \t loss 0.01519 - accuracy 0.96289\n",
      "iteration 76 \t : \t loss 0.01488 - accuracy 0.96484\n",
      "iteration 77 \t : \t loss 0.01457 - accuracy 0.96606\n",
      "iteration 78 \t : \t loss 0.01427 - accuracy 0.96777\n",
      "iteration 79 \t : \t loss 0.01397 - accuracy 0.96826\n",
      "iteration 80 \t : \t loss 0.01368 - accuracy 0.96899\n",
      "iteration 81 \t : \t loss 0.01339 - accuracy 0.97021\n",
      "iteration 82 \t : \t loss 0.01311 - accuracy 0.97070\n",
      "iteration 83 \t : \t loss 0.01283 - accuracy 0.97168\n",
      "iteration 84 \t : \t loss 0.01255 - accuracy 0.97290\n",
      "iteration 85 \t : \t loss 0.01228 - accuracy 0.97314\n",
      "iteration 86 \t : \t loss 0.01201 - accuracy 0.97339\n",
      "iteration 87 \t : \t loss 0.01175 - accuracy 0.97437\n",
      "iteration 88 \t : \t loss 0.01149 - accuracy 0.97534\n",
      "iteration 89 \t : \t loss 0.01123 - accuracy 0.97607\n",
      "iteration 90 \t : \t loss 0.01098 - accuracy 0.97729\n",
      "iteration 91 \t : \t loss 0.01073 - accuracy 0.97803\n",
      "iteration 92 \t : \t loss 0.01049 - accuracy 0.97803\n",
      "iteration 93 \t : \t loss 0.01025 - accuracy 0.97827\n",
      "iteration 94 \t : \t loss 0.01002 - accuracy 0.97900\n",
      "iteration 95 \t : \t loss 0.00979 - accuracy 0.97925\n",
      "iteration 96 \t : \t loss 0.00956 - accuracy 0.97998\n",
      "iteration 97 \t : \t loss 0.00934 - accuracy 0.98047\n",
      "iteration 98 \t : \t loss 0.00912 - accuracy 0.98120\n",
      "iteration 99 \t : \t loss 0.00891 - accuracy 0.98193\n",
      "iteration 100 \t : \t loss 0.00870 - accuracy 0.98193\n",
      "iteration 101 \t : \t loss 0.00850 - accuracy 0.98267\n",
      "iteration 102 \t : \t loss 0.00830 - accuracy 0.98340\n",
      "iteration 103 \t : \t loss 0.00810 - accuracy 0.98389\n",
      "iteration 104 \t : \t loss 0.00791 - accuracy 0.98438\n",
      "iteration 105 \t : \t loss 0.00772 - accuracy 0.98535\n",
      "iteration 106 \t : \t loss 0.00754 - accuracy 0.98608\n",
      "iteration 107 \t : \t loss 0.00736 - accuracy 0.98682\n",
      "iteration 108 \t : \t loss 0.00719 - accuracy 0.98804\n",
      "iteration 109 \t : \t loss 0.00702 - accuracy 0.98877\n",
      "iteration 110 \t : \t loss 0.00685 - accuracy 0.98950\n",
      "iteration 111 \t : \t loss 0.00669 - accuracy 0.99023\n",
      "iteration 112 \t : \t loss 0.00653 - accuracy 0.99072\n",
      "iteration 113 \t : \t loss 0.00637 - accuracy 0.99170\n",
      "iteration 114 \t : \t loss 0.00622 - accuracy 0.99194\n",
      "iteration 115 \t : \t loss 0.00607 - accuracy 0.99219\n",
      "iteration 116 \t : \t loss 0.00593 - accuracy 0.99243\n",
      "iteration 117 \t : \t loss 0.00579 - accuracy 0.99316\n",
      "iteration 118 \t : \t loss 0.00565 - accuracy 0.99365\n",
      "iteration 119 \t : \t loss 0.00551 - accuracy 0.99390\n",
      "iteration 120 \t : \t loss 0.00538 - accuracy 0.99438\n",
      "iteration 121 \t : \t loss 0.00525 - accuracy 0.99438\n",
      "iteration 122 \t : \t loss 0.00513 - accuracy 0.99463\n",
      "iteration 123 \t : \t loss 0.00501 - accuracy 0.99487\n",
      "iteration 124 \t : \t loss 0.00489 - accuracy 0.99487\n",
      "iteration 125 \t : \t loss 0.00477 - accuracy 0.99512\n",
      "iteration 126 \t : \t loss 0.00466 - accuracy 0.99536\n",
      "iteration 127 \t : \t loss 0.00455 - accuracy 0.99536\n",
      "iteration 128 \t : \t loss 0.00444 - accuracy 0.99561\n",
      "iteration 129 \t : \t loss 0.00433 - accuracy 0.99561\n",
      "iteration 130 \t : \t loss 0.00423 - accuracy 0.99561\n",
      "iteration 131 \t : \t loss 0.00413 - accuracy 0.99585\n",
      "iteration 132 \t : \t loss 0.00403 - accuracy 0.99609\n",
      "iteration 133 \t : \t loss 0.00393 - accuracy 0.99658\n",
      "iteration 134 \t : \t loss 0.00384 - accuracy 0.99707\n",
      "iteration 135 \t : \t loss 0.00375 - accuracy 0.99707\n",
      "iteration 136 \t : \t loss 0.00366 - accuracy 0.99707\n",
      "iteration 137 \t : \t loss 0.00358 - accuracy 0.99731\n",
      "iteration 138 \t : \t loss 0.00349 - accuracy 0.99731\n",
      "iteration 139 \t : \t loss 0.00341 - accuracy 0.99731\n",
      "iteration 140 \t : \t loss 0.00333 - accuracy 0.99731\n",
      "iteration 141 \t : \t loss 0.00325 - accuracy 0.99731\n",
      "iteration 142 \t : \t loss 0.00318 - accuracy 0.99756\n",
      "iteration 143 \t : \t loss 0.00311 - accuracy 0.99805\n",
      "iteration 144 \t : \t loss 0.00303 - accuracy 0.99805\n",
      "iteration 145 \t : \t loss 0.00297 - accuracy 0.99829\n",
      "iteration 146 \t : \t loss 0.00290 - accuracy 0.99829\n",
      "iteration 147 \t : \t loss 0.00283 - accuracy 0.99854\n",
      "iteration 148 \t : \t loss 0.00277 - accuracy 0.99878\n",
      "iteration 149 \t : \t loss 0.00271 - accuracy 0.99878\n",
      "iteration 150 \t : \t loss 0.00265 - accuracy 0.99927\n",
      "iteration 151 \t : \t loss 0.00259 - accuracy 0.99927\n",
      "iteration 152 \t : \t loss 0.00254 - accuracy 0.99927\n",
      "iteration 153 \t : \t loss 0.00249 - accuracy 0.99927\n",
      "iteration 154 \t : \t loss 0.00243 - accuracy 0.99927\n",
      "iteration 155 \t : \t loss 0.00238 - accuracy 0.99927\n",
      "iteration 156 \t : \t loss 0.00233 - accuracy 0.99927\n",
      "iteration 157 \t : \t loss 0.00229 - accuracy 0.99927\n",
      "iteration 158 \t : \t loss 0.00224 - accuracy 0.99927\n",
      "iteration 159 \t : \t loss 0.00220 - accuracy 0.99927\n",
      "iteration 160 \t : \t loss 0.00215 - accuracy 0.99951\n",
      "iteration 161 \t : \t loss 0.00211 - accuracy 0.99951\n",
      "iteration 162 \t : \t loss 0.00207 - accuracy 0.99951\n",
      "iteration 163 \t : \t loss 0.00203 - accuracy 0.99951\n",
      "iteration 164 \t : \t loss 0.00199 - accuracy 0.99951\n",
      "iteration 165 \t : \t loss 0.00196 - accuracy 0.99951\n",
      "iteration 166 \t : \t loss 0.00192 - accuracy 0.99951\n",
      "iteration 167 \t : \t loss 0.00188 - accuracy 0.99951\n",
      "iteration 168 \t : \t loss 0.00185 - accuracy 0.99951\n",
      "iteration 169 \t : \t loss 0.00182 - accuracy 0.99951\n",
      "iteration 170 \t : \t loss 0.00178 - accuracy 0.99951\n",
      "iteration 171 \t : \t loss 0.00175 - accuracy 0.99951\n",
      "iteration 172 \t : \t loss 0.00172 - accuracy 0.99976\n",
      "iteration 173 \t : \t loss 0.00169 - accuracy 0.99976\n",
      "iteration 174 \t : \t loss 0.00166 - accuracy 0.99976\n",
      "iteration 175 \t : \t loss 0.00163 - accuracy 0.99976\n",
      "iteration 176 \t : \t loss 0.00160 - accuracy 0.99976\n",
      "iteration 177 \t : \t loss 0.00158 - accuracy 0.99976\n",
      "iteration 178 \t : \t loss 0.00155 - accuracy 0.99976\n",
      "iteration 179 \t : \t loss 0.00153 - accuracy 0.99976\n",
      "iteration 180 \t : \t loss 0.00150 - accuracy 0.99976\n",
      "iteration 181 \t : \t loss 0.00148 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00145 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00143 - accuracy 0.99976\n",
      "iteration 184 \t : \t loss 0.00141 - accuracy 0.99976\n",
      "iteration 185 \t : \t loss 0.00139 - accuracy 0.99976\n",
      "iteration 186 \t : \t loss 0.00136 - accuracy 0.99976\n",
      "iteration 187 \t : \t loss 0.00134 - accuracy 0.99976\n",
      "iteration 188 \t : \t loss 0.00132 - accuracy 0.99976\n",
      "iteration 189 \t : \t loss 0.00130 - accuracy 0.99976\n",
      "iteration 190 \t : \t loss 0.00128 - accuracy 0.99976\n",
      "iteration 191 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00123 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00119 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00118 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00114 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.12712 - accuracy 0.67163\n",
      "iteration 1 \t : \t loss 0.06256 - accuracy 0.85547\n",
      "iteration 2 \t : \t loss 0.04917 - accuracy 0.87964\n",
      "iteration 3 \t : \t loss 0.04285 - accuracy 0.89014\n",
      "iteration 4 \t : \t loss 0.03894 - accuracy 0.89526\n",
      "iteration 5 \t : \t loss 0.03618 - accuracy 0.90088\n",
      "iteration 6 \t : \t loss 0.03405 - accuracy 0.90601\n",
      "iteration 7 \t : \t loss 0.03234 - accuracy 0.90942\n",
      "iteration 8 \t : \t loss 0.03089 - accuracy 0.91187\n",
      "iteration 9 \t : \t loss 0.02964 - accuracy 0.91455\n",
      "iteration 10 \t : \t loss 0.02853 - accuracy 0.91821\n",
      "iteration 11 \t : \t loss 0.02753 - accuracy 0.92065\n",
      "iteration 12 \t : \t loss 0.02663 - accuracy 0.92383\n",
      "iteration 13 \t : \t loss 0.02580 - accuracy 0.92700\n",
      "iteration 14 \t : \t loss 0.02503 - accuracy 0.92896\n",
      "iteration 15 \t : \t loss 0.02432 - accuracy 0.92993\n",
      "iteration 16 \t : \t loss 0.02364 - accuracy 0.93213\n",
      "iteration 17 \t : \t loss 0.02301 - accuracy 0.93408\n",
      "iteration 18 \t : \t loss 0.02241 - accuracy 0.93628\n",
      "iteration 19 \t : \t loss 0.02185 - accuracy 0.93872\n",
      "iteration 20 \t : \t loss 0.02131 - accuracy 0.94092\n",
      "iteration 21 \t : \t loss 0.02079 - accuracy 0.94189\n",
      "iteration 22 \t : \t loss 0.02030 - accuracy 0.94336\n",
      "iteration 23 \t : \t loss 0.01983 - accuracy 0.94507\n",
      "iteration 24 \t : \t loss 0.01938 - accuracy 0.94727\n",
      "iteration 25 \t : \t loss 0.01894 - accuracy 0.94873\n",
      "iteration 26 \t : \t loss 0.01852 - accuracy 0.94995\n",
      "iteration 27 \t : \t loss 0.01812 - accuracy 0.95264\n",
      "iteration 28 \t : \t loss 0.01773 - accuracy 0.95410\n",
      "iteration 29 \t : \t loss 0.01735 - accuracy 0.95459\n",
      "iteration 30 \t : \t loss 0.01699 - accuracy 0.95581\n",
      "iteration 31 \t : \t loss 0.01664 - accuracy 0.95679\n",
      "iteration 32 \t : \t loss 0.01630 - accuracy 0.95776\n",
      "iteration 33 \t : \t loss 0.01597 - accuracy 0.95850\n",
      "iteration 34 \t : \t loss 0.01564 - accuracy 0.96094\n",
      "iteration 35 \t : \t loss 0.01533 - accuracy 0.96191\n",
      "iteration 36 \t : \t loss 0.01503 - accuracy 0.96265\n",
      "iteration 37 \t : \t loss 0.01474 - accuracy 0.96362\n",
      "iteration 38 \t : \t loss 0.01445 - accuracy 0.96436\n",
      "iteration 39 \t : \t loss 0.01417 - accuracy 0.96606\n",
      "iteration 40 \t : \t loss 0.01390 - accuracy 0.96631\n",
      "iteration 41 \t : \t loss 0.01363 - accuracy 0.96655\n",
      "iteration 42 \t : \t loss 0.01338 - accuracy 0.96753\n",
      "iteration 43 \t : \t loss 0.01312 - accuracy 0.96899\n",
      "iteration 44 \t : \t loss 0.01288 - accuracy 0.96973\n",
      "iteration 45 \t : \t loss 0.01264 - accuracy 0.97046\n",
      "iteration 46 \t : \t loss 0.01241 - accuracy 0.97144\n",
      "iteration 47 \t : \t loss 0.01218 - accuracy 0.97217\n",
      "iteration 48 \t : \t loss 0.01196 - accuracy 0.97241\n",
      "iteration 49 \t : \t loss 0.01174 - accuracy 0.97339\n",
      "iteration 50 \t : \t loss 0.01152 - accuracy 0.97412\n",
      "iteration 51 \t : \t loss 0.01132 - accuracy 0.97461\n",
      "iteration 52 \t : \t loss 0.01111 - accuracy 0.97534\n",
      "iteration 53 \t : \t loss 0.01091 - accuracy 0.97607\n",
      "iteration 54 \t : \t loss 0.01072 - accuracy 0.97681\n",
      "iteration 55 \t : \t loss 0.01053 - accuracy 0.97729\n",
      "iteration 56 \t : \t loss 0.01034 - accuracy 0.97729\n",
      "iteration 57 \t : \t loss 0.01016 - accuracy 0.97803\n",
      "iteration 58 \t : \t loss 0.00998 - accuracy 0.97876\n",
      "iteration 59 \t : \t loss 0.00981 - accuracy 0.98022\n",
      "iteration 60 \t : \t loss 0.00964 - accuracy 0.98071\n",
      "iteration 61 \t : \t loss 0.00947 - accuracy 0.98120\n",
      "iteration 62 \t : \t loss 0.00931 - accuracy 0.98120\n",
      "iteration 63 \t : \t loss 0.00915 - accuracy 0.98169\n",
      "iteration 64 \t : \t loss 0.00899 - accuracy 0.98242\n",
      "iteration 65 \t : \t loss 0.00884 - accuracy 0.98364\n",
      "iteration 66 \t : \t loss 0.00868 - accuracy 0.98462\n",
      "iteration 67 \t : \t loss 0.00854 - accuracy 0.98462\n",
      "iteration 68 \t : \t loss 0.00839 - accuracy 0.98584\n",
      "iteration 69 \t : \t loss 0.00825 - accuracy 0.98584\n",
      "iteration 70 \t : \t loss 0.00811 - accuracy 0.98608\n",
      "iteration 71 \t : \t loss 0.00797 - accuracy 0.98633\n",
      "iteration 72 \t : \t loss 0.00784 - accuracy 0.98682\n",
      "iteration 73 \t : \t loss 0.00771 - accuracy 0.98706\n",
      "iteration 74 \t : \t loss 0.00758 - accuracy 0.98730\n",
      "iteration 75 \t : \t loss 0.00746 - accuracy 0.98755\n",
      "iteration 76 \t : \t loss 0.00733 - accuracy 0.98828\n",
      "iteration 77 \t : \t loss 0.00721 - accuracy 0.98853\n",
      "iteration 78 \t : \t loss 0.00709 - accuracy 0.98853\n",
      "iteration 79 \t : \t loss 0.00698 - accuracy 0.98901\n",
      "iteration 80 \t : \t loss 0.00686 - accuracy 0.98926\n",
      "iteration 81 \t : \t loss 0.00675 - accuracy 0.98926\n",
      "iteration 82 \t : \t loss 0.00664 - accuracy 0.98950\n",
      "iteration 83 \t : \t loss 0.00654 - accuracy 0.98975\n",
      "iteration 84 \t : \t loss 0.00643 - accuracy 0.99048\n",
      "iteration 85 \t : \t loss 0.00633 - accuracy 0.99121\n",
      "iteration 86 \t : \t loss 0.00623 - accuracy 0.99146\n",
      "iteration 87 \t : \t loss 0.00613 - accuracy 0.99194\n",
      "iteration 88 \t : \t loss 0.00603 - accuracy 0.99243\n",
      "iteration 89 \t : \t loss 0.00593 - accuracy 0.99268\n",
      "iteration 90 \t : \t loss 0.00584 - accuracy 0.99292\n",
      "iteration 91 \t : \t loss 0.00575 - accuracy 0.99292\n",
      "iteration 92 \t : \t loss 0.00566 - accuracy 0.99316\n",
      "iteration 93 \t : \t loss 0.00557 - accuracy 0.99316\n",
      "iteration 94 \t : \t loss 0.00549 - accuracy 0.99316\n",
      "iteration 95 \t : \t loss 0.00540 - accuracy 0.99341\n",
      "iteration 96 \t : \t loss 0.00532 - accuracy 0.99365\n",
      "iteration 97 \t : \t loss 0.00524 - accuracy 0.99365\n",
      "iteration 98 \t : \t loss 0.00516 - accuracy 0.99390\n",
      "iteration 99 \t : \t loss 0.00508 - accuracy 0.99438\n",
      "iteration 100 \t : \t loss 0.00500 - accuracy 0.99463\n",
      "iteration 101 \t : \t loss 0.00493 - accuracy 0.99463\n",
      "iteration 102 \t : \t loss 0.00485 - accuracy 0.99487\n",
      "iteration 103 \t : \t loss 0.00478 - accuracy 0.99487\n",
      "iteration 104 \t : \t loss 0.00471 - accuracy 0.99512\n",
      "iteration 105 \t : \t loss 0.00464 - accuracy 0.99561\n",
      "iteration 106 \t : \t loss 0.00457 - accuracy 0.99561\n",
      "iteration 107 \t : \t loss 0.00451 - accuracy 0.99561\n",
      "iteration 108 \t : \t loss 0.00444 - accuracy 0.99561\n",
      "iteration 109 \t : \t loss 0.00438 - accuracy 0.99561\n",
      "iteration 110 \t : \t loss 0.00432 - accuracy 0.99561\n",
      "iteration 111 \t : \t loss 0.00425 - accuracy 0.99609\n",
      "iteration 112 \t : \t loss 0.00419 - accuracy 0.99609\n",
      "iteration 113 \t : \t loss 0.00413 - accuracy 0.99609\n",
      "iteration 114 \t : \t loss 0.00408 - accuracy 0.99609\n",
      "iteration 115 \t : \t loss 0.00402 - accuracy 0.99609\n",
      "iteration 116 \t : \t loss 0.00396 - accuracy 0.99634\n",
      "iteration 117 \t : \t loss 0.00391 - accuracy 0.99634\n",
      "iteration 118 \t : \t loss 0.00385 - accuracy 0.99658\n",
      "iteration 119 \t : \t loss 0.00380 - accuracy 0.99658\n",
      "iteration 120 \t : \t loss 0.00375 - accuracy 0.99658\n",
      "iteration 121 \t : \t loss 0.00370 - accuracy 0.99683\n",
      "iteration 122 \t : \t loss 0.00365 - accuracy 0.99707\n",
      "iteration 123 \t : \t loss 0.00360 - accuracy 0.99707\n",
      "iteration 124 \t : \t loss 0.00355 - accuracy 0.99731\n",
      "iteration 125 \t : \t loss 0.00350 - accuracy 0.99731\n",
      "iteration 126 \t : \t loss 0.00346 - accuracy 0.99731\n",
      "iteration 127 \t : \t loss 0.00341 - accuracy 0.99805\n",
      "iteration 128 \t : \t loss 0.00336 - accuracy 0.99805\n",
      "iteration 129 \t : \t loss 0.00332 - accuracy 0.99805\n",
      "iteration 130 \t : \t loss 0.00328 - accuracy 0.99805\n",
      "iteration 131 \t : \t loss 0.00324 - accuracy 0.99805\n",
      "iteration 132 \t : \t loss 0.00319 - accuracy 0.99805\n",
      "iteration 133 \t : \t loss 0.00315 - accuracy 0.99805\n",
      "iteration 134 \t : \t loss 0.00311 - accuracy 0.99805\n",
      "iteration 135 \t : \t loss 0.00307 - accuracy 0.99805\n",
      "iteration 136 \t : \t loss 0.00303 - accuracy 0.99805\n",
      "iteration 137 \t : \t loss 0.00300 - accuracy 0.99805\n",
      "iteration 138 \t : \t loss 0.00296 - accuracy 0.99805\n",
      "iteration 139 \t : \t loss 0.00292 - accuracy 0.99805\n",
      "iteration 140 \t : \t loss 0.00289 - accuracy 0.99829\n",
      "iteration 141 \t : \t loss 0.00285 - accuracy 0.99829\n",
      "iteration 142 \t : \t loss 0.00282 - accuracy 0.99829\n",
      "iteration 143 \t : \t loss 0.00278 - accuracy 0.99829\n",
      "iteration 144 \t : \t loss 0.00275 - accuracy 0.99829\n",
      "iteration 145 \t : \t loss 0.00271 - accuracy 0.99829\n",
      "iteration 146 \t : \t loss 0.00268 - accuracy 0.99854\n",
      "iteration 147 \t : \t loss 0.00265 - accuracy 0.99878\n",
      "iteration 148 \t : \t loss 0.00262 - accuracy 0.99878\n",
      "iteration 149 \t : \t loss 0.00259 - accuracy 0.99927\n",
      "iteration 150 \t : \t loss 0.00256 - accuracy 0.99927\n",
      "iteration 151 \t : \t loss 0.00253 - accuracy 0.99927\n",
      "iteration 152 \t : \t loss 0.00250 - accuracy 0.99927\n",
      "iteration 153 \t : \t loss 0.00247 - accuracy 0.99927\n",
      "iteration 154 \t : \t loss 0.00244 - accuracy 0.99927\n",
      "iteration 155 \t : \t loss 0.00242 - accuracy 0.99927\n",
      "iteration 156 \t : \t loss 0.00239 - accuracy 0.99951\n",
      "iteration 157 \t : \t loss 0.00236 - accuracy 0.99951\n",
      "iteration 158 \t : \t loss 0.00234 - accuracy 0.99951\n",
      "iteration 159 \t : \t loss 0.00231 - accuracy 0.99951\n",
      "iteration 160 \t : \t loss 0.00228 - accuracy 0.99951\n",
      "iteration 161 \t : \t loss 0.00226 - accuracy 0.99951\n",
      "iteration 162 \t : \t loss 0.00223 - accuracy 0.99951\n",
      "iteration 163 \t : \t loss 0.00221 - accuracy 0.99951\n",
      "iteration 164 \t : \t loss 0.00219 - accuracy 0.99951\n",
      "iteration 165 \t : \t loss 0.00216 - accuracy 0.99951\n",
      "iteration 166 \t : \t loss 0.00214 - accuracy 0.99951\n",
      "iteration 167 \t : \t loss 0.00212 - accuracy 0.99951\n",
      "iteration 168 \t : \t loss 0.00210 - accuracy 0.99951\n",
      "iteration 169 \t : \t loss 0.00208 - accuracy 0.99951\n",
      "iteration 170 \t : \t loss 0.00205 - accuracy 0.99951\n",
      "iteration 171 \t : \t loss 0.00203 - accuracy 0.99951\n",
      "iteration 172 \t : \t loss 0.00201 - accuracy 0.99951\n",
      "iteration 173 \t : \t loss 0.00199 - accuracy 0.99951\n",
      "iteration 174 \t : \t loss 0.00197 - accuracy 0.99951\n",
      "iteration 175 \t : \t loss 0.00195 - accuracy 0.99951\n",
      "iteration 176 \t : \t loss 0.00193 - accuracy 0.99951\n",
      "iteration 177 \t : \t loss 0.00191 - accuracy 0.99951\n",
      "iteration 178 \t : \t loss 0.00190 - accuracy 0.99976\n",
      "iteration 179 \t : \t loss 0.00188 - accuracy 0.99976\n",
      "iteration 180 \t : \t loss 0.00186 - accuracy 0.99976\n",
      "iteration 181 \t : \t loss 0.00184 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00182 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00181 - accuracy 0.99976\n",
      "iteration 184 \t : \t loss 0.00179 - accuracy 0.99976\n",
      "iteration 185 \t : \t loss 0.00177 - accuracy 0.99976\n",
      "iteration 186 \t : \t loss 0.00176 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00174 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00172 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00171 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00169 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00168 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00166 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00165 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00163 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00162 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00161 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00159 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00156 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.23285 - accuracy 0.09668\n",
      "iteration 1 \t : \t loss 0.22900 - accuracy 0.11548\n",
      "iteration 2 \t : \t loss 0.22530 - accuracy 0.14062\n",
      "iteration 3 \t : \t loss 0.22010 - accuracy 0.19507\n",
      "iteration 4 \t : \t loss 0.21194 - accuracy 0.28003\n",
      "iteration 5 \t : \t loss 0.19861 - accuracy 0.39844\n",
      "iteration 6 \t : \t loss 0.17879 - accuracy 0.49341\n",
      "iteration 7 \t : \t loss 0.15515 - accuracy 0.57349\n",
      "iteration 8 \t : \t loss 0.13280 - accuracy 0.63940\n",
      "iteration 9 \t : \t loss 0.11472 - accuracy 0.68311\n",
      "iteration 10 \t : \t loss 0.10111 - accuracy 0.72021\n",
      "iteration 11 \t : \t loss 0.09089 - accuracy 0.74487\n",
      "iteration 12 \t : \t loss 0.08287 - accuracy 0.76807\n",
      "iteration 13 \t : \t loss 0.07629 - accuracy 0.78662\n",
      "iteration 14 \t : \t loss 0.07076 - accuracy 0.80249\n",
      "iteration 15 \t : \t loss 0.06604 - accuracy 0.81812\n",
      "iteration 16 \t : \t loss 0.06202 - accuracy 0.82983\n",
      "iteration 17 \t : \t loss 0.05857 - accuracy 0.83936\n",
      "iteration 18 \t : \t loss 0.05560 - accuracy 0.84619\n",
      "iteration 19 \t : \t loss 0.05302 - accuracy 0.85254\n",
      "iteration 20 \t : \t loss 0.05075 - accuracy 0.85864\n",
      "iteration 21 \t : \t loss 0.04874 - accuracy 0.86377\n",
      "iteration 22 \t : \t loss 0.04693 - accuracy 0.86841\n",
      "iteration 23 \t : \t loss 0.04530 - accuracy 0.87329\n",
      "iteration 24 \t : \t loss 0.04380 - accuracy 0.87915\n",
      "iteration 25 \t : \t loss 0.04242 - accuracy 0.88452\n",
      "iteration 26 \t : \t loss 0.04115 - accuracy 0.88672\n",
      "iteration 27 \t : \t loss 0.03996 - accuracy 0.89014\n",
      "iteration 28 \t : \t loss 0.03885 - accuracy 0.89331\n",
      "iteration 29 \t : \t loss 0.03780 - accuracy 0.89697\n",
      "iteration 30 \t : \t loss 0.03681 - accuracy 0.90015\n",
      "iteration 31 \t : \t loss 0.03587 - accuracy 0.90259\n",
      "iteration 32 \t : \t loss 0.03498 - accuracy 0.90479\n",
      "iteration 33 \t : \t loss 0.03414 - accuracy 0.90601\n",
      "iteration 34 \t : \t loss 0.03333 - accuracy 0.90845\n",
      "iteration 35 \t : \t loss 0.03256 - accuracy 0.91040\n",
      "iteration 36 \t : \t loss 0.03182 - accuracy 0.91260\n",
      "iteration 37 \t : \t loss 0.03111 - accuracy 0.91406\n",
      "iteration 38 \t : \t loss 0.03043 - accuracy 0.91504\n",
      "iteration 39 \t : \t loss 0.02977 - accuracy 0.91626\n",
      "iteration 40 \t : \t loss 0.02914 - accuracy 0.91772\n",
      "iteration 41 \t : \t loss 0.02852 - accuracy 0.91992\n",
      "iteration 42 \t : \t loss 0.02793 - accuracy 0.92090\n",
      "iteration 43 \t : \t loss 0.02736 - accuracy 0.92334\n",
      "iteration 44 \t : \t loss 0.02680 - accuracy 0.92554\n",
      "iteration 45 \t : \t loss 0.02626 - accuracy 0.92798\n",
      "iteration 46 \t : \t loss 0.02573 - accuracy 0.93018\n",
      "iteration 47 \t : \t loss 0.02522 - accuracy 0.93213\n",
      "iteration 48 \t : \t loss 0.02472 - accuracy 0.93384\n",
      "iteration 49 \t : \t loss 0.02423 - accuracy 0.93555\n",
      "iteration 50 \t : \t loss 0.02376 - accuracy 0.93677\n",
      "iteration 51 \t : \t loss 0.02329 - accuracy 0.93774\n",
      "iteration 52 \t : \t loss 0.02283 - accuracy 0.94019\n",
      "iteration 53 \t : \t loss 0.02238 - accuracy 0.94189\n",
      "iteration 54 \t : \t loss 0.02195 - accuracy 0.94385\n",
      "iteration 55 \t : \t loss 0.02151 - accuracy 0.94482\n",
      "iteration 56 \t : \t loss 0.02109 - accuracy 0.94604\n",
      "iteration 57 \t : \t loss 0.02068 - accuracy 0.94702\n",
      "iteration 58 \t : \t loss 0.02027 - accuracy 0.94922\n",
      "iteration 59 \t : \t loss 0.01987 - accuracy 0.95044\n",
      "iteration 60 \t : \t loss 0.01947 - accuracy 0.95190\n",
      "iteration 61 \t : \t loss 0.01908 - accuracy 0.95312\n",
      "iteration 62 \t : \t loss 0.01870 - accuracy 0.95459\n",
      "iteration 63 \t : \t loss 0.01833 - accuracy 0.95508\n",
      "iteration 64 \t : \t loss 0.01796 - accuracy 0.95703\n",
      "iteration 65 \t : \t loss 0.01759 - accuracy 0.95850\n",
      "iteration 66 \t : \t loss 0.01724 - accuracy 0.95947\n",
      "iteration 67 \t : \t loss 0.01688 - accuracy 0.96021\n",
      "iteration 68 \t : \t loss 0.01654 - accuracy 0.96021\n",
      "iteration 69 \t : \t loss 0.01619 - accuracy 0.96191\n",
      "iteration 70 \t : \t loss 0.01586 - accuracy 0.96338\n",
      "iteration 71 \t : \t loss 0.01553 - accuracy 0.96411\n",
      "iteration 72 \t : \t loss 0.01520 - accuracy 0.96533\n",
      "iteration 73 \t : \t loss 0.01488 - accuracy 0.96533\n",
      "iteration 74 \t : \t loss 0.01456 - accuracy 0.96655\n",
      "iteration 75 \t : \t loss 0.01425 - accuracy 0.96704\n",
      "iteration 76 \t : \t loss 0.01395 - accuracy 0.96777\n",
      "iteration 77 \t : \t loss 0.01365 - accuracy 0.96948\n",
      "iteration 78 \t : \t loss 0.01335 - accuracy 0.97095\n",
      "iteration 79 \t : \t loss 0.01306 - accuracy 0.97144\n",
      "iteration 80 \t : \t loss 0.01278 - accuracy 0.97290\n",
      "iteration 81 \t : \t loss 0.01250 - accuracy 0.97339\n",
      "iteration 82 \t : \t loss 0.01222 - accuracy 0.97461\n",
      "iteration 83 \t : \t loss 0.01196 - accuracy 0.97559\n",
      "iteration 84 \t : \t loss 0.01169 - accuracy 0.97559\n",
      "iteration 85 \t : \t loss 0.01143 - accuracy 0.97656\n",
      "iteration 86 \t : \t loss 0.01118 - accuracy 0.97705\n",
      "iteration 87 \t : \t loss 0.01093 - accuracy 0.97778\n",
      "iteration 88 \t : \t loss 0.01068 - accuracy 0.97900\n",
      "iteration 89 \t : \t loss 0.01044 - accuracy 0.97949\n",
      "iteration 90 \t : \t loss 0.01021 - accuracy 0.98145\n",
      "iteration 91 \t : \t loss 0.00997 - accuracy 0.98242\n",
      "iteration 92 \t : \t loss 0.00975 - accuracy 0.98267\n",
      "iteration 93 \t : \t loss 0.00953 - accuracy 0.98315\n",
      "iteration 94 \t : \t loss 0.00931 - accuracy 0.98438\n",
      "iteration 95 \t : \t loss 0.00909 - accuracy 0.98535\n",
      "iteration 96 \t : \t loss 0.00889 - accuracy 0.98608\n",
      "iteration 97 \t : \t loss 0.00868 - accuracy 0.98633\n",
      "iteration 98 \t : \t loss 0.00848 - accuracy 0.98682\n",
      "iteration 99 \t : \t loss 0.00828 - accuracy 0.98706\n",
      "iteration 100 \t : \t loss 0.00809 - accuracy 0.98755\n",
      "iteration 101 \t : \t loss 0.00790 - accuracy 0.98828\n",
      "iteration 102 \t : \t loss 0.00772 - accuracy 0.98877\n",
      "iteration 103 \t : \t loss 0.00754 - accuracy 0.98877\n",
      "iteration 104 \t : \t loss 0.00736 - accuracy 0.98877\n",
      "iteration 105 \t : \t loss 0.00718 - accuracy 0.98901\n",
      "iteration 106 \t : \t loss 0.00701 - accuracy 0.98901\n",
      "iteration 107 \t : \t loss 0.00685 - accuracy 0.98975\n",
      "iteration 108 \t : \t loss 0.00668 - accuracy 0.98999\n",
      "iteration 109 \t : \t loss 0.00652 - accuracy 0.99023\n",
      "iteration 110 \t : \t loss 0.00637 - accuracy 0.99048\n",
      "iteration 111 \t : \t loss 0.00621 - accuracy 0.99097\n",
      "iteration 112 \t : \t loss 0.00607 - accuracy 0.99146\n",
      "iteration 113 \t : \t loss 0.00592 - accuracy 0.99146\n",
      "iteration 114 \t : \t loss 0.00578 - accuracy 0.99170\n",
      "iteration 115 \t : \t loss 0.00564 - accuracy 0.99219\n",
      "iteration 116 \t : \t loss 0.00550 - accuracy 0.99268\n",
      "iteration 117 \t : \t loss 0.00537 - accuracy 0.99268\n",
      "iteration 118 \t : \t loss 0.00524 - accuracy 0.99292\n",
      "iteration 119 \t : \t loss 0.00511 - accuracy 0.99316\n",
      "iteration 120 \t : \t loss 0.00499 - accuracy 0.99390\n",
      "iteration 121 \t : \t loss 0.00487 - accuracy 0.99414\n",
      "iteration 122 \t : \t loss 0.00475 - accuracy 0.99414\n",
      "iteration 123 \t : \t loss 0.00463 - accuracy 0.99414\n",
      "iteration 124 \t : \t loss 0.00452 - accuracy 0.99414\n",
      "iteration 125 \t : \t loss 0.00441 - accuracy 0.99438\n",
      "iteration 126 \t : \t loss 0.00431 - accuracy 0.99487\n",
      "iteration 127 \t : \t loss 0.00421 - accuracy 0.99487\n",
      "iteration 128 \t : \t loss 0.00411 - accuracy 0.99512\n",
      "iteration 129 \t : \t loss 0.00401 - accuracy 0.99561\n",
      "iteration 130 \t : \t loss 0.00391 - accuracy 0.99609\n",
      "iteration 131 \t : \t loss 0.00382 - accuracy 0.99609\n",
      "iteration 132 \t : \t loss 0.00373 - accuracy 0.99634\n",
      "iteration 133 \t : \t loss 0.00365 - accuracy 0.99683\n",
      "iteration 134 \t : \t loss 0.00356 - accuracy 0.99707\n",
      "iteration 135 \t : \t loss 0.00348 - accuracy 0.99707\n",
      "iteration 136 \t : \t loss 0.00340 - accuracy 0.99707\n",
      "iteration 137 \t : \t loss 0.00332 - accuracy 0.99731\n",
      "iteration 138 \t : \t loss 0.00325 - accuracy 0.99756\n",
      "iteration 139 \t : \t loss 0.00317 - accuracy 0.99756\n",
      "iteration 140 \t : \t loss 0.00310 - accuracy 0.99805\n",
      "iteration 141 \t : \t loss 0.00303 - accuracy 0.99805\n",
      "iteration 142 \t : \t loss 0.00297 - accuracy 0.99805\n",
      "iteration 143 \t : \t loss 0.00290 - accuracy 0.99829\n",
      "iteration 144 \t : \t loss 0.00284 - accuracy 0.99878\n",
      "iteration 145 \t : \t loss 0.00278 - accuracy 0.99878\n",
      "iteration 146 \t : \t loss 0.00272 - accuracy 0.99878\n",
      "iteration 147 \t : \t loss 0.00266 - accuracy 0.99902\n",
      "iteration 148 \t : \t loss 0.00260 - accuracy 0.99902\n",
      "iteration 149 \t : \t loss 0.00255 - accuracy 0.99902\n",
      "iteration 150 \t : \t loss 0.00250 - accuracy 0.99902\n",
      "iteration 151 \t : \t loss 0.00244 - accuracy 0.99902\n",
      "iteration 152 \t : \t loss 0.00239 - accuracy 0.99902\n",
      "iteration 153 \t : \t loss 0.00235 - accuracy 0.99902\n",
      "iteration 154 \t : \t loss 0.00230 - accuracy 0.99902\n",
      "iteration 155 \t : \t loss 0.00225 - accuracy 0.99902\n",
      "iteration 156 \t : \t loss 0.00221 - accuracy 0.99902\n",
      "iteration 157 \t : \t loss 0.00217 - accuracy 0.99902\n",
      "iteration 158 \t : \t loss 0.00212 - accuracy 0.99902\n",
      "iteration 159 \t : \t loss 0.00208 - accuracy 0.99927\n",
      "iteration 160 \t : \t loss 0.00204 - accuracy 0.99951\n",
      "iteration 161 \t : \t loss 0.00200 - accuracy 0.99976\n",
      "iteration 162 \t : \t loss 0.00197 - accuracy 0.99976\n",
      "iteration 163 \t : \t loss 0.00193 - accuracy 0.99976\n",
      "iteration 164 \t : \t loss 0.00189 - accuracy 0.99976\n",
      "iteration 165 \t : \t loss 0.00186 - accuracy 0.99976\n",
      "iteration 166 \t : \t loss 0.00183 - accuracy 0.99976\n",
      "iteration 167 \t : \t loss 0.00179 - accuracy 0.99976\n",
      "iteration 168 \t : \t loss 0.00176 - accuracy 0.99976\n",
      "iteration 169 \t : \t loss 0.00173 - accuracy 0.99976\n",
      "iteration 170 \t : \t loss 0.00170 - accuracy 0.99976\n",
      "iteration 171 \t : \t loss 0.00167 - accuracy 0.99976\n",
      "iteration 172 \t : \t loss 0.00164 - accuracy 0.99976\n",
      "iteration 173 \t : \t loss 0.00161 - accuracy 0.99976\n",
      "iteration 174 \t : \t loss 0.00159 - accuracy 0.99976\n",
      "iteration 175 \t : \t loss 0.00156 - accuracy 0.99976\n",
      "iteration 176 \t : \t loss 0.00153 - accuracy 0.99976\n",
      "iteration 177 \t : \t loss 0.00151 - accuracy 0.99976\n",
      "iteration 178 \t : \t loss 0.00148 - accuracy 0.99976\n",
      "iteration 179 \t : \t loss 0.00146 - accuracy 0.99976\n",
      "iteration 180 \t : \t loss 0.00144 - accuracy 0.99976\n",
      "iteration 181 \t : \t loss 0.00141 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00139 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00137 - accuracy 0.99976\n",
      "iteration 184 \t : \t loss 0.00135 - accuracy 0.99976\n",
      "iteration 185 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00123 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00118 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00112 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00110 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00109 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.12395 - accuracy 0.68140\n",
      "iteration 1 \t : \t loss 0.06302 - accuracy 0.85059\n",
      "iteration 2 \t : \t loss 0.05000 - accuracy 0.87134\n",
      "iteration 3 \t : \t loss 0.04367 - accuracy 0.88452\n",
      "iteration 4 \t : \t loss 0.03969 - accuracy 0.89380\n",
      "iteration 5 \t : \t loss 0.03684 - accuracy 0.89990\n",
      "iteration 6 \t : \t loss 0.03465 - accuracy 0.90649\n",
      "iteration 7 \t : \t loss 0.03287 - accuracy 0.90991\n",
      "iteration 8 \t : \t loss 0.03138 - accuracy 0.91187\n",
      "iteration 9 \t : \t loss 0.03008 - accuracy 0.91479\n",
      "iteration 10 \t : \t loss 0.02894 - accuracy 0.91797\n",
      "iteration 11 \t : \t loss 0.02792 - accuracy 0.92017\n",
      "iteration 12 \t : \t loss 0.02699 - accuracy 0.92285\n",
      "iteration 13 \t : \t loss 0.02615 - accuracy 0.92505\n",
      "iteration 14 \t : \t loss 0.02536 - accuracy 0.92700\n",
      "iteration 15 \t : \t loss 0.02463 - accuracy 0.92871\n",
      "iteration 16 \t : \t loss 0.02395 - accuracy 0.93188\n",
      "iteration 17 \t : \t loss 0.02330 - accuracy 0.93457\n",
      "iteration 18 \t : \t loss 0.02269 - accuracy 0.93652\n",
      "iteration 19 \t : \t loss 0.02212 - accuracy 0.93848\n",
      "iteration 20 \t : \t loss 0.02157 - accuracy 0.93970\n",
      "iteration 21 \t : \t loss 0.02105 - accuracy 0.94165\n",
      "iteration 22 \t : \t loss 0.02054 - accuracy 0.94385\n",
      "iteration 23 \t : \t loss 0.02006 - accuracy 0.94580\n",
      "iteration 24 \t : \t loss 0.01960 - accuracy 0.94849\n",
      "iteration 25 \t : \t loss 0.01916 - accuracy 0.94946\n",
      "iteration 26 \t : \t loss 0.01873 - accuracy 0.95068\n",
      "iteration 27 \t : \t loss 0.01832 - accuracy 0.95239\n",
      "iteration 28 \t : \t loss 0.01792 - accuracy 0.95386\n",
      "iteration 29 \t : \t loss 0.01754 - accuracy 0.95459\n",
      "iteration 30 \t : \t loss 0.01717 - accuracy 0.95581\n",
      "iteration 31 \t : \t loss 0.01681 - accuracy 0.95630\n",
      "iteration 32 \t : \t loss 0.01646 - accuracy 0.95728\n",
      "iteration 33 \t : \t loss 0.01612 - accuracy 0.95825\n",
      "iteration 34 \t : \t loss 0.01579 - accuracy 0.95898\n",
      "iteration 35 \t : \t loss 0.01547 - accuracy 0.95947\n",
      "iteration 36 \t : \t loss 0.01516 - accuracy 0.96167\n",
      "iteration 37 \t : \t loss 0.01486 - accuracy 0.96387\n",
      "iteration 38 \t : \t loss 0.01457 - accuracy 0.96509\n",
      "iteration 39 \t : \t loss 0.01429 - accuracy 0.96582\n",
      "iteration 40 \t : \t loss 0.01401 - accuracy 0.96729\n",
      "iteration 41 \t : \t loss 0.01374 - accuracy 0.96851\n",
      "iteration 42 \t : \t loss 0.01348 - accuracy 0.96948\n",
      "iteration 43 \t : \t loss 0.01322 - accuracy 0.97046\n",
      "iteration 44 \t : \t loss 0.01297 - accuracy 0.97095\n",
      "iteration 45 \t : \t loss 0.01273 - accuracy 0.97144\n",
      "iteration 46 \t : \t loss 0.01249 - accuracy 0.97144\n",
      "iteration 47 \t : \t loss 0.01226 - accuracy 0.97217\n",
      "iteration 48 \t : \t loss 0.01203 - accuracy 0.97363\n",
      "iteration 49 \t : \t loss 0.01181 - accuracy 0.97437\n",
      "iteration 50 \t : \t loss 0.01160 - accuracy 0.97461\n",
      "iteration 51 \t : \t loss 0.01139 - accuracy 0.97485\n",
      "iteration 52 \t : \t loss 0.01118 - accuracy 0.97607\n",
      "iteration 53 \t : \t loss 0.01098 - accuracy 0.97705\n",
      "iteration 54 \t : \t loss 0.01078 - accuracy 0.97778\n",
      "iteration 55 \t : \t loss 0.01059 - accuracy 0.97827\n",
      "iteration 56 \t : \t loss 0.01040 - accuracy 0.97900\n",
      "iteration 57 \t : \t loss 0.01022 - accuracy 0.97949\n",
      "iteration 58 \t : \t loss 0.01004 - accuracy 0.97974\n",
      "iteration 59 \t : \t loss 0.00986 - accuracy 0.98022\n",
      "iteration 60 \t : \t loss 0.00969 - accuracy 0.98047\n",
      "iteration 61 \t : \t loss 0.00952 - accuracy 0.98071\n",
      "iteration 62 \t : \t loss 0.00936 - accuracy 0.98120\n",
      "iteration 63 \t : \t loss 0.00920 - accuracy 0.98120\n",
      "iteration 64 \t : \t loss 0.00904 - accuracy 0.98145\n",
      "iteration 65 \t : \t loss 0.00888 - accuracy 0.98169\n",
      "iteration 66 \t : \t loss 0.00873 - accuracy 0.98218\n",
      "iteration 67 \t : \t loss 0.00858 - accuracy 0.98267\n",
      "iteration 68 \t : \t loss 0.00844 - accuracy 0.98364\n",
      "iteration 69 \t : \t loss 0.00830 - accuracy 0.98389\n",
      "iteration 70 \t : \t loss 0.00816 - accuracy 0.98462\n",
      "iteration 71 \t : \t loss 0.00802 - accuracy 0.98535\n",
      "iteration 72 \t : \t loss 0.00789 - accuracy 0.98560\n",
      "iteration 73 \t : \t loss 0.00776 - accuracy 0.98633\n",
      "iteration 74 \t : \t loss 0.00763 - accuracy 0.98706\n",
      "iteration 75 \t : \t loss 0.00751 - accuracy 0.98730\n",
      "iteration 76 \t : \t loss 0.00738 - accuracy 0.98755\n",
      "iteration 77 \t : \t loss 0.00726 - accuracy 0.98853\n",
      "iteration 78 \t : \t loss 0.00715 - accuracy 0.98853\n",
      "iteration 79 \t : \t loss 0.00703 - accuracy 0.98877\n",
      "iteration 80 \t : \t loss 0.00692 - accuracy 0.98901\n",
      "iteration 81 \t : \t loss 0.00681 - accuracy 0.98950\n",
      "iteration 82 \t : \t loss 0.00670 - accuracy 0.98975\n",
      "iteration 83 \t : \t loss 0.00659 - accuracy 0.98999\n",
      "iteration 84 \t : \t loss 0.00649 - accuracy 0.99072\n",
      "iteration 85 \t : \t loss 0.00639 - accuracy 0.99072\n",
      "iteration 86 \t : \t loss 0.00629 - accuracy 0.99097\n",
      "iteration 87 \t : \t loss 0.00619 - accuracy 0.99121\n",
      "iteration 88 \t : \t loss 0.00609 - accuracy 0.99194\n",
      "iteration 89 \t : \t loss 0.00600 - accuracy 0.99219\n",
      "iteration 90 \t : \t loss 0.00591 - accuracy 0.99268\n",
      "iteration 91 \t : \t loss 0.00582 - accuracy 0.99292\n",
      "iteration 92 \t : \t loss 0.00573 - accuracy 0.99316\n",
      "iteration 93 \t : \t loss 0.00564 - accuracy 0.99341\n",
      "iteration 94 \t : \t loss 0.00555 - accuracy 0.99365\n",
      "iteration 95 \t : \t loss 0.00547 - accuracy 0.99414\n",
      "iteration 96 \t : \t loss 0.00539 - accuracy 0.99463\n",
      "iteration 97 \t : \t loss 0.00531 - accuracy 0.99463\n",
      "iteration 98 \t : \t loss 0.00523 - accuracy 0.99463\n",
      "iteration 99 \t : \t loss 0.00515 - accuracy 0.99463\n",
      "iteration 100 \t : \t loss 0.00508 - accuracy 0.99463\n",
      "iteration 101 \t : \t loss 0.00500 - accuracy 0.99463\n",
      "iteration 102 \t : \t loss 0.00493 - accuracy 0.99463\n",
      "iteration 103 \t : \t loss 0.00486 - accuracy 0.99463\n",
      "iteration 104 \t : \t loss 0.00479 - accuracy 0.99463\n",
      "iteration 105 \t : \t loss 0.00472 - accuracy 0.99463\n",
      "iteration 106 \t : \t loss 0.00465 - accuracy 0.99487\n",
      "iteration 107 \t : \t loss 0.00458 - accuracy 0.99487\n",
      "iteration 108 \t : \t loss 0.00452 - accuracy 0.99487\n",
      "iteration 109 \t : \t loss 0.00445 - accuracy 0.99512\n",
      "iteration 110 \t : \t loss 0.00439 - accuracy 0.99512\n",
      "iteration 111 \t : \t loss 0.00433 - accuracy 0.99536\n",
      "iteration 112 \t : \t loss 0.00427 - accuracy 0.99536\n",
      "iteration 113 \t : \t loss 0.00421 - accuracy 0.99561\n",
      "iteration 114 \t : \t loss 0.00415 - accuracy 0.99585\n",
      "iteration 115 \t : \t loss 0.00409 - accuracy 0.99609\n",
      "iteration 116 \t : \t loss 0.00404 - accuracy 0.99609\n",
      "iteration 117 \t : \t loss 0.00398 - accuracy 0.99609\n",
      "iteration 118 \t : \t loss 0.00393 - accuracy 0.99609\n",
      "iteration 119 \t : \t loss 0.00387 - accuracy 0.99609\n",
      "iteration 120 \t : \t loss 0.00382 - accuracy 0.99609\n",
      "iteration 121 \t : \t loss 0.00377 - accuracy 0.99634\n",
      "iteration 122 \t : \t loss 0.00372 - accuracy 0.99658\n",
      "iteration 123 \t : \t loss 0.00367 - accuracy 0.99658\n",
      "iteration 124 \t : \t loss 0.00362 - accuracy 0.99731\n",
      "iteration 125 \t : \t loss 0.00357 - accuracy 0.99731\n",
      "iteration 126 \t : \t loss 0.00353 - accuracy 0.99731\n",
      "iteration 127 \t : \t loss 0.00348 - accuracy 0.99731\n",
      "iteration 128 \t : \t loss 0.00343 - accuracy 0.99731\n",
      "iteration 129 \t : \t loss 0.00339 - accuracy 0.99731\n",
      "iteration 130 \t : \t loss 0.00335 - accuracy 0.99731\n",
      "iteration 131 \t : \t loss 0.00330 - accuracy 0.99731\n",
      "iteration 132 \t : \t loss 0.00326 - accuracy 0.99731\n",
      "iteration 133 \t : \t loss 0.00322 - accuracy 0.99756\n",
      "iteration 134 \t : \t loss 0.00318 - accuracy 0.99756\n",
      "iteration 135 \t : \t loss 0.00314 - accuracy 0.99780\n",
      "iteration 136 \t : \t loss 0.00310 - accuracy 0.99780\n",
      "iteration 137 \t : \t loss 0.00306 - accuracy 0.99805\n",
      "iteration 138 \t : \t loss 0.00302 - accuracy 0.99805\n",
      "iteration 139 \t : \t loss 0.00299 - accuracy 0.99805\n",
      "iteration 140 \t : \t loss 0.00295 - accuracy 0.99805\n",
      "iteration 141 \t : \t loss 0.00291 - accuracy 0.99805\n",
      "iteration 142 \t : \t loss 0.00288 - accuracy 0.99805\n",
      "iteration 143 \t : \t loss 0.00284 - accuracy 0.99805\n",
      "iteration 144 \t : \t loss 0.00281 - accuracy 0.99829\n",
      "iteration 145 \t : \t loss 0.00278 - accuracy 0.99854\n",
      "iteration 146 \t : \t loss 0.00274 - accuracy 0.99878\n",
      "iteration 147 \t : \t loss 0.00271 - accuracy 0.99878\n",
      "iteration 148 \t : \t loss 0.00268 - accuracy 0.99878\n",
      "iteration 149 \t : \t loss 0.00265 - accuracy 0.99878\n",
      "iteration 150 \t : \t loss 0.00262 - accuracy 0.99878\n",
      "iteration 151 \t : \t loss 0.00259 - accuracy 0.99878\n",
      "iteration 152 \t : \t loss 0.00256 - accuracy 0.99878\n",
      "iteration 153 \t : \t loss 0.00253 - accuracy 0.99878\n",
      "iteration 154 \t : \t loss 0.00250 - accuracy 0.99878\n",
      "iteration 155 \t : \t loss 0.00247 - accuracy 0.99878\n",
      "iteration 156 \t : \t loss 0.00244 - accuracy 0.99878\n",
      "iteration 157 \t : \t loss 0.00241 - accuracy 0.99878\n",
      "iteration 158 \t : \t loss 0.00239 - accuracy 0.99878\n",
      "iteration 159 \t : \t loss 0.00236 - accuracy 0.99878\n",
      "iteration 160 \t : \t loss 0.00234 - accuracy 0.99878\n",
      "iteration 161 \t : \t loss 0.00231 - accuracy 0.99878\n",
      "iteration 162 \t : \t loss 0.00228 - accuracy 0.99878\n",
      "iteration 163 \t : \t loss 0.00226 - accuracy 0.99878\n",
      "iteration 164 \t : \t loss 0.00224 - accuracy 0.99878\n",
      "iteration 165 \t : \t loss 0.00221 - accuracy 0.99878\n",
      "iteration 166 \t : \t loss 0.00219 - accuracy 0.99902\n",
      "iteration 167 \t : \t loss 0.00217 - accuracy 0.99902\n",
      "iteration 168 \t : \t loss 0.00214 - accuracy 0.99927\n",
      "iteration 169 \t : \t loss 0.00212 - accuracy 0.99927\n",
      "iteration 170 \t : \t loss 0.00210 - accuracy 0.99927\n",
      "iteration 171 \t : \t loss 0.00208 - accuracy 0.99927\n",
      "iteration 172 \t : \t loss 0.00206 - accuracy 0.99927\n",
      "iteration 173 \t : \t loss 0.00203 - accuracy 0.99927\n",
      "iteration 174 \t : \t loss 0.00201 - accuracy 0.99927\n",
      "iteration 175 \t : \t loss 0.00199 - accuracy 0.99927\n",
      "iteration 176 \t : \t loss 0.00197 - accuracy 0.99927\n",
      "iteration 177 \t : \t loss 0.00195 - accuracy 0.99951\n",
      "iteration 178 \t : \t loss 0.00193 - accuracy 0.99951\n",
      "iteration 179 \t : \t loss 0.00191 - accuracy 0.99951\n",
      "iteration 180 \t : \t loss 0.00190 - accuracy 0.99951\n",
      "iteration 181 \t : \t loss 0.00188 - accuracy 0.99951\n",
      "iteration 182 \t : \t loss 0.00186 - accuracy 0.99951\n",
      "iteration 183 \t : \t loss 0.00184 - accuracy 0.99951\n",
      "iteration 184 \t : \t loss 0.00182 - accuracy 0.99951\n",
      "iteration 185 \t : \t loss 0.00181 - accuracy 0.99951\n",
      "iteration 186 \t : \t loss 0.00179 - accuracy 0.99976\n",
      "iteration 187 \t : \t loss 0.00177 - accuracy 0.99976\n",
      "iteration 188 \t : \t loss 0.00176 - accuracy 0.99976\n",
      "iteration 189 \t : \t loss 0.00174 - accuracy 0.99976\n",
      "iteration 190 \t : \t loss 0.00172 - accuracy 0.99976\n",
      "iteration 191 \t : \t loss 0.00171 - accuracy 0.99976\n",
      "iteration 192 \t : \t loss 0.00169 - accuracy 0.99976\n",
      "iteration 193 \t : \t loss 0.00168 - accuracy 0.99976\n",
      "iteration 194 \t : \t loss 0.00166 - accuracy 0.99976\n",
      "iteration 195 \t : \t loss 0.00165 - accuracy 0.99976\n",
      "iteration 196 \t : \t loss 0.00163 - accuracy 0.99976\n",
      "iteration 197 \t : \t loss 0.00162 - accuracy 0.99976\n",
      "iteration 198 \t : \t loss 0.00160 - accuracy 0.99976\n",
      "iteration 199 \t : \t loss 0.00159 - accuracy 0.99976\n",
      "iteration 0 \t : \t loss 0.23232 - accuracy 0.11182\n",
      "iteration 1 \t : \t loss 0.22913 - accuracy 0.12769\n",
      "iteration 2 \t : \t loss 0.22558 - accuracy 0.15967\n",
      "iteration 3 \t : \t loss 0.22055 - accuracy 0.22778\n",
      "iteration 4 \t : \t loss 0.21261 - accuracy 0.31714\n",
      "iteration 5 \t : \t loss 0.19949 - accuracy 0.41138\n",
      "iteration 6 \t : \t loss 0.17975 - accuracy 0.48511\n",
      "iteration 7 \t : \t loss 0.15631 - accuracy 0.55054\n",
      "iteration 8 \t : \t loss 0.13424 - accuracy 0.60938\n",
      "iteration 9 \t : \t loss 0.11611 - accuracy 0.66699\n",
      "iteration 10 \t : \t loss 0.10210 - accuracy 0.71362\n",
      "iteration 11 \t : \t loss 0.09142 - accuracy 0.74365\n",
      "iteration 12 \t : \t loss 0.08315 - accuracy 0.76978\n",
      "iteration 13 \t : \t loss 0.07653 - accuracy 0.78955\n",
      "iteration 14 \t : \t loss 0.07105 - accuracy 0.80615\n",
      "iteration 15 \t : \t loss 0.06637 - accuracy 0.81982\n",
      "iteration 16 \t : \t loss 0.06231 - accuracy 0.83130\n",
      "iteration 17 \t : \t loss 0.05877 - accuracy 0.84009\n",
      "iteration 18 \t : \t loss 0.05566 - accuracy 0.84937\n",
      "iteration 19 \t : \t loss 0.05294 - accuracy 0.85596\n",
      "iteration 20 \t : \t loss 0.05055 - accuracy 0.86328\n",
      "iteration 21 \t : \t loss 0.04844 - accuracy 0.87061\n",
      "iteration 22 \t : \t loss 0.04658 - accuracy 0.87354\n",
      "iteration 23 \t : \t loss 0.04492 - accuracy 0.87842\n",
      "iteration 24 \t : \t loss 0.04343 - accuracy 0.88135\n",
      "iteration 25 \t : \t loss 0.04208 - accuracy 0.88379\n",
      "iteration 26 \t : \t loss 0.04085 - accuracy 0.88403\n",
      "iteration 27 \t : \t loss 0.03972 - accuracy 0.88672\n",
      "iteration 28 \t : \t loss 0.03867 - accuracy 0.88940\n",
      "iteration 29 \t : \t loss 0.03769 - accuracy 0.89307\n",
      "iteration 30 \t : \t loss 0.03677 - accuracy 0.89453\n",
      "iteration 31 \t : \t loss 0.03591 - accuracy 0.89624\n",
      "iteration 32 \t : \t loss 0.03509 - accuracy 0.89648\n",
      "iteration 33 \t : \t loss 0.03431 - accuracy 0.89990\n",
      "iteration 34 \t : \t loss 0.03357 - accuracy 0.90259\n",
      "iteration 35 \t : \t loss 0.03286 - accuracy 0.90454\n",
      "iteration 36 \t : \t loss 0.03218 - accuracy 0.90649\n",
      "iteration 37 \t : \t loss 0.03152 - accuracy 0.90967\n",
      "iteration 38 \t : \t loss 0.03089 - accuracy 0.91089\n",
      "iteration 39 \t : \t loss 0.03028 - accuracy 0.91309\n",
      "iteration 40 \t : \t loss 0.02968 - accuracy 0.91504\n",
      "iteration 41 \t : \t loss 0.02911 - accuracy 0.91821\n",
      "iteration 42 \t : \t loss 0.02855 - accuracy 0.92114\n",
      "iteration 43 \t : \t loss 0.02800 - accuracy 0.92188\n",
      "iteration 44 \t : \t loss 0.02747 - accuracy 0.92383\n",
      "iteration 45 \t : \t loss 0.02695 - accuracy 0.92554\n",
      "iteration 46 \t : \t loss 0.02645 - accuracy 0.92676\n",
      "iteration 47 \t : \t loss 0.02595 - accuracy 0.92725\n",
      "iteration 48 \t : \t loss 0.02547 - accuracy 0.92920\n",
      "iteration 49 \t : \t loss 0.02500 - accuracy 0.93091\n",
      "iteration 50 \t : \t loss 0.02453 - accuracy 0.93262\n",
      "iteration 51 \t : \t loss 0.02407 - accuracy 0.93433\n",
      "iteration 52 \t : \t loss 0.02363 - accuracy 0.93530\n",
      "iteration 53 \t : \t loss 0.02319 - accuracy 0.93579\n",
      "iteration 54 \t : \t loss 0.02276 - accuracy 0.93701\n",
      "iteration 55 \t : \t loss 0.02233 - accuracy 0.93896\n",
      "iteration 56 \t : \t loss 0.02191 - accuracy 0.94043\n",
      "iteration 57 \t : \t loss 0.02150 - accuracy 0.94189\n",
      "iteration 58 \t : \t loss 0.02110 - accuracy 0.94458\n",
      "iteration 59 \t : \t loss 0.02070 - accuracy 0.94580\n",
      "iteration 60 \t : \t loss 0.02031 - accuracy 0.94702\n",
      "iteration 61 \t : \t loss 0.01992 - accuracy 0.94824\n",
      "iteration 62 \t : \t loss 0.01954 - accuracy 0.94971\n",
      "iteration 63 \t : \t loss 0.01916 - accuracy 0.95044\n",
      "iteration 64 \t : \t loss 0.01879 - accuracy 0.95190\n",
      "iteration 65 \t : \t loss 0.01843 - accuracy 0.95312\n",
      "iteration 66 \t : \t loss 0.01807 - accuracy 0.95435\n",
      "iteration 67 \t : \t loss 0.01772 - accuracy 0.95654\n",
      "iteration 68 \t : \t loss 0.01737 - accuracy 0.95923\n",
      "iteration 69 \t : \t loss 0.01702 - accuracy 0.96021\n",
      "iteration 70 \t : \t loss 0.01668 - accuracy 0.96045\n",
      "iteration 71 \t : \t loss 0.01635 - accuracy 0.96143\n",
      "iteration 72 \t : \t loss 0.01602 - accuracy 0.96216\n",
      "iteration 73 \t : \t loss 0.01569 - accuracy 0.96362\n",
      "iteration 74 \t : \t loss 0.01537 - accuracy 0.96436\n",
      "iteration 75 \t : \t loss 0.01506 - accuracy 0.96533\n",
      "iteration 76 \t : \t loss 0.01475 - accuracy 0.96777\n",
      "iteration 77 \t : \t loss 0.01444 - accuracy 0.96875\n",
      "iteration 78 \t : \t loss 0.01414 - accuracy 0.96973\n",
      "iteration 79 \t : \t loss 0.01384 - accuracy 0.97095\n",
      "iteration 80 \t : \t loss 0.01355 - accuracy 0.97217\n",
      "iteration 81 \t : \t loss 0.01326 - accuracy 0.97241\n",
      "iteration 82 \t : \t loss 0.01298 - accuracy 0.97314\n",
      "iteration 83 \t : \t loss 0.01270 - accuracy 0.97412\n",
      "iteration 84 \t : \t loss 0.01242 - accuracy 0.97461\n",
      "iteration 85 \t : \t loss 0.01215 - accuracy 0.97583\n",
      "iteration 86 \t : \t loss 0.01188 - accuracy 0.97729\n",
      "iteration 87 \t : \t loss 0.01162 - accuracy 0.97754\n",
      "iteration 88 \t : \t loss 0.01136 - accuracy 0.97827\n",
      "iteration 89 \t : \t loss 0.01110 - accuracy 0.97876\n",
      "iteration 90 \t : \t loss 0.01085 - accuracy 0.97949\n",
      "iteration 91 \t : \t loss 0.01060 - accuracy 0.97974\n",
      "iteration 92 \t : \t loss 0.01036 - accuracy 0.97998\n",
      "iteration 93 \t : \t loss 0.01012 - accuracy 0.98047\n",
      "iteration 94 \t : \t loss 0.00989 - accuracy 0.98169\n",
      "iteration 95 \t : \t loss 0.00965 - accuracy 0.98242\n",
      "iteration 96 \t : \t loss 0.00943 - accuracy 0.98340\n",
      "iteration 97 \t : \t loss 0.00920 - accuracy 0.98389\n",
      "iteration 98 \t : \t loss 0.00898 - accuracy 0.98462\n",
      "iteration 99 \t : \t loss 0.00877 - accuracy 0.98486\n",
      "iteration 100 \t : \t loss 0.00856 - accuracy 0.98535\n",
      "iteration 101 \t : \t loss 0.00835 - accuracy 0.98682\n",
      "iteration 102 \t : \t loss 0.00815 - accuracy 0.98706\n",
      "iteration 103 \t : \t loss 0.00795 - accuracy 0.98779\n",
      "iteration 104 \t : \t loss 0.00775 - accuracy 0.98828\n",
      "iteration 105 \t : \t loss 0.00756 - accuracy 0.98901\n",
      "iteration 106 \t : \t loss 0.00738 - accuracy 0.98999\n",
      "iteration 107 \t : \t loss 0.00720 - accuracy 0.99048\n",
      "iteration 108 \t : \t loss 0.00702 - accuracy 0.99048\n",
      "iteration 109 \t : \t loss 0.00684 - accuracy 0.99048\n",
      "iteration 110 \t : \t loss 0.00667 - accuracy 0.99072\n",
      "iteration 111 \t : \t loss 0.00651 - accuracy 0.99121\n",
      "iteration 112 \t : \t loss 0.00635 - accuracy 0.99146\n",
      "iteration 113 \t : \t loss 0.00619 - accuracy 0.99170\n",
      "iteration 114 \t : \t loss 0.00603 - accuracy 0.99219\n",
      "iteration 115 \t : \t loss 0.00588 - accuracy 0.99243\n",
      "iteration 116 \t : \t loss 0.00574 - accuracy 0.99316\n",
      "iteration 117 \t : \t loss 0.00559 - accuracy 0.99341\n",
      "iteration 118 \t : \t loss 0.00545 - accuracy 0.99438\n",
      "iteration 119 \t : \t loss 0.00532 - accuracy 0.99463\n",
      "iteration 120 \t : \t loss 0.00519 - accuracy 0.99463\n",
      "iteration 121 \t : \t loss 0.00506 - accuracy 0.99487\n",
      "iteration 122 \t : \t loss 0.00493 - accuracy 0.99512\n",
      "iteration 123 \t : \t loss 0.00481 - accuracy 0.99512\n",
      "iteration 124 \t : \t loss 0.00469 - accuracy 0.99512\n",
      "iteration 125 \t : \t loss 0.00458 - accuracy 0.99512\n",
      "iteration 126 \t : \t loss 0.00447 - accuracy 0.99561\n",
      "iteration 127 \t : \t loss 0.00436 - accuracy 0.99585\n",
      "iteration 128 \t : \t loss 0.00425 - accuracy 0.99609\n",
      "iteration 129 \t : \t loss 0.00415 - accuracy 0.99609\n",
      "iteration 130 \t : \t loss 0.00405 - accuracy 0.99658\n",
      "iteration 131 \t : \t loss 0.00395 - accuracy 0.99707\n",
      "iteration 132 \t : \t loss 0.00385 - accuracy 0.99707\n",
      "iteration 133 \t : \t loss 0.00376 - accuracy 0.99707\n",
      "iteration 134 \t : \t loss 0.00367 - accuracy 0.99707\n",
      "iteration 135 \t : \t loss 0.00358 - accuracy 0.99707\n",
      "iteration 136 \t : \t loss 0.00350 - accuracy 0.99707\n",
      "iteration 137 \t : \t loss 0.00341 - accuracy 0.99780\n",
      "iteration 138 \t : \t loss 0.00333 - accuracy 0.99780\n",
      "iteration 139 \t : \t loss 0.00326 - accuracy 0.99780\n",
      "iteration 140 \t : \t loss 0.00318 - accuracy 0.99780\n",
      "iteration 141 \t : \t loss 0.00311 - accuracy 0.99805\n",
      "iteration 142 \t : \t loss 0.00303 - accuracy 0.99805\n",
      "iteration 143 \t : \t loss 0.00297 - accuracy 0.99805\n",
      "iteration 144 \t : \t loss 0.00290 - accuracy 0.99805\n",
      "iteration 145 \t : \t loss 0.00283 - accuracy 0.99805\n",
      "iteration 146 \t : \t loss 0.00277 - accuracy 0.99805\n",
      "iteration 147 \t : \t loss 0.00271 - accuracy 0.99854\n",
      "iteration 148 \t : \t loss 0.00265 - accuracy 0.99854\n",
      "iteration 149 \t : \t loss 0.00259 - accuracy 0.99854\n",
      "iteration 150 \t : \t loss 0.00254 - accuracy 0.99878\n",
      "iteration 151 \t : \t loss 0.00248 - accuracy 0.99878\n",
      "iteration 152 \t : \t loss 0.00243 - accuracy 0.99878\n",
      "iteration 153 \t : \t loss 0.00238 - accuracy 0.99878\n",
      "iteration 154 \t : \t loss 0.00233 - accuracy 0.99878\n",
      "iteration 155 \t : \t loss 0.00228 - accuracy 0.99902\n",
      "iteration 156 \t : \t loss 0.00223 - accuracy 0.99927\n",
      "iteration 157 \t : \t loss 0.00219 - accuracy 0.99927\n",
      "iteration 158 \t : \t loss 0.00214 - accuracy 0.99927\n",
      "iteration 159 \t : \t loss 0.00210 - accuracy 0.99927\n",
      "iteration 160 \t : \t loss 0.00206 - accuracy 0.99951\n",
      "iteration 161 \t : \t loss 0.00202 - accuracy 0.99951\n",
      "iteration 162 \t : \t loss 0.00198 - accuracy 0.99951\n",
      "iteration 163 \t : \t loss 0.00194 - accuracy 0.99951\n",
      "iteration 164 \t : \t loss 0.00190 - accuracy 0.99951\n",
      "iteration 165 \t : \t loss 0.00187 - accuracy 0.99951\n",
      "iteration 166 \t : \t loss 0.00183 - accuracy 0.99951\n",
      "iteration 167 \t : \t loss 0.00180 - accuracy 0.99951\n",
      "iteration 168 \t : \t loss 0.00177 - accuracy 0.99976\n",
      "iteration 169 \t : \t loss 0.00174 - accuracy 0.99976\n",
      "iteration 170 \t : \t loss 0.00170 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00167 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00164 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00162 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00159 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00156 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00154 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00151 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00149 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00146 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00139 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00125 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00123 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00121 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00118 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00116 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00113 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00111 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00110 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00108 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.12251 - accuracy 0.68530\n",
      "iteration 1 \t : \t loss 0.05881 - accuracy 0.87134\n",
      "iteration 2 \t : \t loss 0.04571 - accuracy 0.89453\n",
      "iteration 3 \t : \t loss 0.03948 - accuracy 0.90479\n",
      "iteration 4 \t : \t loss 0.03565 - accuracy 0.91113\n",
      "iteration 5 \t : \t loss 0.03295 - accuracy 0.91504\n",
      "iteration 6 \t : \t loss 0.03090 - accuracy 0.91797\n",
      "iteration 7 \t : \t loss 0.02926 - accuracy 0.92163\n",
      "iteration 8 \t : \t loss 0.02789 - accuracy 0.92456\n",
      "iteration 9 \t : \t loss 0.02672 - accuracy 0.92627\n",
      "iteration 10 \t : \t loss 0.02569 - accuracy 0.93018\n",
      "iteration 11 \t : \t loss 0.02477 - accuracy 0.93311\n",
      "iteration 12 \t : \t loss 0.02394 - accuracy 0.93481\n",
      "iteration 13 \t : \t loss 0.02319 - accuracy 0.93604\n",
      "iteration 14 \t : \t loss 0.02249 - accuracy 0.93726\n",
      "iteration 15 \t : \t loss 0.02185 - accuracy 0.93872\n",
      "iteration 16 \t : \t loss 0.02124 - accuracy 0.94043\n",
      "iteration 17 \t : \t loss 0.02068 - accuracy 0.94165\n",
      "iteration 18 \t : \t loss 0.02014 - accuracy 0.94507\n",
      "iteration 19 \t : \t loss 0.01964 - accuracy 0.94727\n",
      "iteration 20 \t : \t loss 0.01916 - accuracy 0.94946\n",
      "iteration 21 \t : \t loss 0.01870 - accuracy 0.95068\n",
      "iteration 22 \t : \t loss 0.01826 - accuracy 0.95215\n",
      "iteration 23 \t : \t loss 0.01784 - accuracy 0.95435\n",
      "iteration 24 \t : \t loss 0.01744 - accuracy 0.95508\n",
      "iteration 25 \t : \t loss 0.01705 - accuracy 0.95654\n",
      "iteration 26 \t : \t loss 0.01668 - accuracy 0.95728\n",
      "iteration 27 \t : \t loss 0.01632 - accuracy 0.95850\n",
      "iteration 28 \t : \t loss 0.01597 - accuracy 0.95923\n",
      "iteration 29 \t : \t loss 0.01564 - accuracy 0.96094\n",
      "iteration 30 \t : \t loss 0.01531 - accuracy 0.96143\n",
      "iteration 31 \t : \t loss 0.01500 - accuracy 0.96167\n",
      "iteration 32 \t : \t loss 0.01469 - accuracy 0.96338\n",
      "iteration 33 \t : \t loss 0.01440 - accuracy 0.96411\n",
      "iteration 34 \t : \t loss 0.01411 - accuracy 0.96509\n",
      "iteration 35 \t : \t loss 0.01383 - accuracy 0.96631\n",
      "iteration 36 \t : \t loss 0.01356 - accuracy 0.96729\n",
      "iteration 37 \t : \t loss 0.01329 - accuracy 0.96826\n",
      "iteration 38 \t : \t loss 0.01304 - accuracy 0.96899\n",
      "iteration 39 \t : \t loss 0.01278 - accuracy 0.96997\n",
      "iteration 40 \t : \t loss 0.01254 - accuracy 0.97070\n",
      "iteration 41 \t : \t loss 0.01230 - accuracy 0.97095\n",
      "iteration 42 \t : \t loss 0.01207 - accuracy 0.97168\n",
      "iteration 43 \t : \t loss 0.01184 - accuracy 0.97241\n",
      "iteration 44 \t : \t loss 0.01162 - accuracy 0.97314\n",
      "iteration 45 \t : \t loss 0.01141 - accuracy 0.97339\n",
      "iteration 46 \t : \t loss 0.01120 - accuracy 0.97363\n",
      "iteration 47 \t : \t loss 0.01099 - accuracy 0.97388\n",
      "iteration 48 \t : \t loss 0.01079 - accuracy 0.97510\n",
      "iteration 49 \t : \t loss 0.01059 - accuracy 0.97559\n",
      "iteration 50 \t : \t loss 0.01040 - accuracy 0.97607\n",
      "iteration 51 \t : \t loss 0.01021 - accuracy 0.97681\n",
      "iteration 52 \t : \t loss 0.01003 - accuracy 0.97705\n",
      "iteration 53 \t : \t loss 0.00985 - accuracy 0.97778\n",
      "iteration 54 \t : \t loss 0.00968 - accuracy 0.97827\n",
      "iteration 55 \t : \t loss 0.00950 - accuracy 0.97827\n",
      "iteration 56 \t : \t loss 0.00934 - accuracy 0.97852\n",
      "iteration 57 \t : \t loss 0.00917 - accuracy 0.97900\n",
      "iteration 58 \t : \t loss 0.00901 - accuracy 0.97949\n",
      "iteration 59 \t : \t loss 0.00886 - accuracy 0.97998\n",
      "iteration 60 \t : \t loss 0.00870 - accuracy 0.98047\n",
      "iteration 61 \t : \t loss 0.00855 - accuracy 0.98120\n",
      "iteration 62 \t : \t loss 0.00841 - accuracy 0.98193\n",
      "iteration 63 \t : \t loss 0.00826 - accuracy 0.98267\n",
      "iteration 64 \t : \t loss 0.00812 - accuracy 0.98291\n",
      "iteration 65 \t : \t loss 0.00798 - accuracy 0.98340\n",
      "iteration 66 \t : \t loss 0.00785 - accuracy 0.98364\n",
      "iteration 67 \t : \t loss 0.00772 - accuracy 0.98389\n",
      "iteration 68 \t : \t loss 0.00759 - accuracy 0.98413\n",
      "iteration 69 \t : \t loss 0.00746 - accuracy 0.98438\n",
      "iteration 70 \t : \t loss 0.00734 - accuracy 0.98535\n",
      "iteration 71 \t : \t loss 0.00721 - accuracy 0.98608\n",
      "iteration 72 \t : \t loss 0.00710 - accuracy 0.98657\n",
      "iteration 73 \t : \t loss 0.00698 - accuracy 0.98657\n",
      "iteration 74 \t : \t loss 0.00687 - accuracy 0.98730\n",
      "iteration 75 \t : \t loss 0.00675 - accuracy 0.98730\n",
      "iteration 76 \t : \t loss 0.00664 - accuracy 0.98730\n",
      "iteration 77 \t : \t loss 0.00654 - accuracy 0.98755\n",
      "iteration 78 \t : \t loss 0.00643 - accuracy 0.98828\n",
      "iteration 79 \t : \t loss 0.00633 - accuracy 0.98828\n",
      "iteration 80 \t : \t loss 0.00623 - accuracy 0.98853\n",
      "iteration 81 \t : \t loss 0.00613 - accuracy 0.98926\n",
      "iteration 82 \t : \t loss 0.00603 - accuracy 0.98950\n",
      "iteration 83 \t : \t loss 0.00594 - accuracy 0.98950\n",
      "iteration 84 \t : \t loss 0.00585 - accuracy 0.98999\n",
      "iteration 85 \t : \t loss 0.00575 - accuracy 0.99048\n",
      "iteration 86 \t : \t loss 0.00567 - accuracy 0.99072\n",
      "iteration 87 \t : \t loss 0.00558 - accuracy 0.99097\n",
      "iteration 88 \t : \t loss 0.00549 - accuracy 0.99121\n",
      "iteration 89 \t : \t loss 0.00541 - accuracy 0.99219\n",
      "iteration 90 \t : \t loss 0.00533 - accuracy 0.99243\n",
      "iteration 91 \t : \t loss 0.00525 - accuracy 0.99268\n",
      "iteration 92 \t : \t loss 0.00517 - accuracy 0.99292\n",
      "iteration 93 \t : \t loss 0.00509 - accuracy 0.99316\n",
      "iteration 94 \t : \t loss 0.00501 - accuracy 0.99316\n",
      "iteration 95 \t : \t loss 0.00494 - accuracy 0.99414\n",
      "iteration 96 \t : \t loss 0.00487 - accuracy 0.99463\n",
      "iteration 97 \t : \t loss 0.00479 - accuracy 0.99512\n",
      "iteration 98 \t : \t loss 0.00472 - accuracy 0.99512\n",
      "iteration 99 \t : \t loss 0.00465 - accuracy 0.99512\n",
      "iteration 100 \t : \t loss 0.00459 - accuracy 0.99561\n",
      "iteration 101 \t : \t loss 0.00452 - accuracy 0.99609\n",
      "iteration 102 \t : \t loss 0.00446 - accuracy 0.99609\n",
      "iteration 103 \t : \t loss 0.00439 - accuracy 0.99609\n",
      "iteration 104 \t : \t loss 0.00433 - accuracy 0.99609\n",
      "iteration 105 \t : \t loss 0.00427 - accuracy 0.99609\n",
      "iteration 106 \t : \t loss 0.00421 - accuracy 0.99634\n",
      "iteration 107 \t : \t loss 0.00415 - accuracy 0.99658\n",
      "iteration 108 \t : \t loss 0.00409 - accuracy 0.99658\n",
      "iteration 109 \t : \t loss 0.00404 - accuracy 0.99658\n",
      "iteration 110 \t : \t loss 0.00398 - accuracy 0.99658\n",
      "iteration 111 \t : \t loss 0.00392 - accuracy 0.99658\n",
      "iteration 112 \t : \t loss 0.00387 - accuracy 0.99658\n",
      "iteration 113 \t : \t loss 0.00382 - accuracy 0.99658\n",
      "iteration 114 \t : \t loss 0.00377 - accuracy 0.99683\n",
      "iteration 115 \t : \t loss 0.00372 - accuracy 0.99683\n",
      "iteration 116 \t : \t loss 0.00367 - accuracy 0.99707\n",
      "iteration 117 \t : \t loss 0.00362 - accuracy 0.99707\n",
      "iteration 118 \t : \t loss 0.00357 - accuracy 0.99707\n",
      "iteration 119 \t : \t loss 0.00352 - accuracy 0.99707\n",
      "iteration 120 \t : \t loss 0.00348 - accuracy 0.99707\n",
      "iteration 121 \t : \t loss 0.00343 - accuracy 0.99707\n",
      "iteration 122 \t : \t loss 0.00339 - accuracy 0.99707\n",
      "iteration 123 \t : \t loss 0.00334 - accuracy 0.99707\n",
      "iteration 124 \t : \t loss 0.00330 - accuracy 0.99707\n",
      "iteration 125 \t : \t loss 0.00326 - accuracy 0.99731\n",
      "iteration 126 \t : \t loss 0.00322 - accuracy 0.99731\n",
      "iteration 127 \t : \t loss 0.00318 - accuracy 0.99731\n",
      "iteration 128 \t : \t loss 0.00314 - accuracy 0.99731\n",
      "iteration 129 \t : \t loss 0.00310 - accuracy 0.99731\n",
      "iteration 130 \t : \t loss 0.00306 - accuracy 0.99731\n",
      "iteration 131 \t : \t loss 0.00302 - accuracy 0.99756\n",
      "iteration 132 \t : \t loss 0.00299 - accuracy 0.99756\n",
      "iteration 133 \t : \t loss 0.00295 - accuracy 0.99756\n",
      "iteration 134 \t : \t loss 0.00291 - accuracy 0.99756\n",
      "iteration 135 \t : \t loss 0.00288 - accuracy 0.99756\n",
      "iteration 136 \t : \t loss 0.00284 - accuracy 0.99756\n",
      "iteration 137 \t : \t loss 0.00281 - accuracy 0.99756\n",
      "iteration 138 \t : \t loss 0.00278 - accuracy 0.99756\n",
      "iteration 139 \t : \t loss 0.00274 - accuracy 0.99780\n",
      "iteration 140 \t : \t loss 0.00271 - accuracy 0.99805\n",
      "iteration 141 \t : \t loss 0.00268 - accuracy 0.99805\n",
      "iteration 142 \t : \t loss 0.00265 - accuracy 0.99805\n",
      "iteration 143 \t : \t loss 0.00262 - accuracy 0.99805\n",
      "iteration 144 \t : \t loss 0.00259 - accuracy 0.99805\n",
      "iteration 145 \t : \t loss 0.00256 - accuracy 0.99805\n",
      "iteration 146 \t : \t loss 0.00253 - accuracy 0.99829\n",
      "iteration 147 \t : \t loss 0.00250 - accuracy 0.99829\n",
      "iteration 148 \t : \t loss 0.00247 - accuracy 0.99829\n",
      "iteration 149 \t : \t loss 0.00245 - accuracy 0.99854\n",
      "iteration 150 \t : \t loss 0.00242 - accuracy 0.99854\n",
      "iteration 151 \t : \t loss 0.00239 - accuracy 0.99854\n",
      "iteration 152 \t : \t loss 0.00237 - accuracy 0.99854\n",
      "iteration 153 \t : \t loss 0.00234 - accuracy 0.99878\n",
      "iteration 154 \t : \t loss 0.00232 - accuracy 0.99878\n",
      "iteration 155 \t : \t loss 0.00229 - accuracy 0.99878\n",
      "iteration 156 \t : \t loss 0.00227 - accuracy 0.99878\n",
      "iteration 157 \t : \t loss 0.00224 - accuracy 0.99878\n",
      "iteration 158 \t : \t loss 0.00222 - accuracy 0.99878\n",
      "iteration 159 \t : \t loss 0.00219 - accuracy 0.99878\n",
      "iteration 160 \t : \t loss 0.00217 - accuracy 0.99902\n",
      "iteration 161 \t : \t loss 0.00215 - accuracy 0.99902\n",
      "iteration 162 \t : \t loss 0.00213 - accuracy 0.99902\n",
      "iteration 163 \t : \t loss 0.00211 - accuracy 0.99902\n",
      "iteration 164 \t : \t loss 0.00208 - accuracy 0.99902\n",
      "iteration 165 \t : \t loss 0.00206 - accuracy 0.99902\n",
      "iteration 166 \t : \t loss 0.00204 - accuracy 0.99902\n",
      "iteration 167 \t : \t loss 0.00202 - accuracy 0.99902\n",
      "iteration 168 \t : \t loss 0.00200 - accuracy 0.99902\n",
      "iteration 169 \t : \t loss 0.00198 - accuracy 0.99902\n",
      "iteration 170 \t : \t loss 0.00196 - accuracy 0.99902\n",
      "iteration 171 \t : \t loss 0.00194 - accuracy 0.99902\n",
      "iteration 172 \t : \t loss 0.00192 - accuracy 0.99902\n",
      "iteration 173 \t : \t loss 0.00190 - accuracy 0.99902\n",
      "iteration 174 \t : \t loss 0.00189 - accuracy 0.99902\n",
      "iteration 175 \t : \t loss 0.00187 - accuracy 0.99902\n",
      "iteration 176 \t : \t loss 0.00185 - accuracy 0.99902\n",
      "iteration 177 \t : \t loss 0.00183 - accuracy 0.99902\n",
      "iteration 178 \t : \t loss 0.00181 - accuracy 0.99902\n",
      "iteration 179 \t : \t loss 0.00180 - accuracy 0.99927\n",
      "iteration 180 \t : \t loss 0.00178 - accuracy 0.99927\n",
      "iteration 181 \t : \t loss 0.00176 - accuracy 0.99927\n",
      "iteration 182 \t : \t loss 0.00175 - accuracy 0.99927\n",
      "iteration 183 \t : \t loss 0.00173 - accuracy 0.99927\n",
      "iteration 184 \t : \t loss 0.00172 - accuracy 0.99927\n",
      "iteration 185 \t : \t loss 0.00170 - accuracy 0.99927\n",
      "iteration 186 \t : \t loss 0.00168 - accuracy 0.99951\n",
      "iteration 187 \t : \t loss 0.00167 - accuracy 0.99951\n",
      "iteration 188 \t : \t loss 0.00165 - accuracy 0.99951\n",
      "iteration 189 \t : \t loss 0.00164 - accuracy 0.99951\n",
      "iteration 190 \t : \t loss 0.00163 - accuracy 0.99951\n",
      "iteration 191 \t : \t loss 0.00161 - accuracy 0.99951\n",
      "iteration 192 \t : \t loss 0.00160 - accuracy 0.99951\n",
      "iteration 193 \t : \t loss 0.00158 - accuracy 0.99951\n",
      "iteration 194 \t : \t loss 0.00157 - accuracy 0.99951\n",
      "iteration 195 \t : \t loss 0.00156 - accuracy 0.99951\n",
      "iteration 196 \t : \t loss 0.00154 - accuracy 0.99951\n",
      "iteration 197 \t : \t loss 0.00153 - accuracy 0.99951\n",
      "iteration 198 \t : \t loss 0.00152 - accuracy 0.99951\n",
      "iteration 199 \t : \t loss 0.00150 - accuracy 0.99951\n",
      "iteration 0 \t : \t loss 0.23174 - accuracy 0.11450\n",
      "iteration 1 \t : \t loss 0.22855 - accuracy 0.13403\n",
      "iteration 2 \t : \t loss 0.22502 - accuracy 0.16821\n",
      "iteration 3 \t : \t loss 0.21997 - accuracy 0.22510\n",
      "iteration 4 \t : \t loss 0.21187 - accuracy 0.31812\n",
      "iteration 5 \t : \t loss 0.19816 - accuracy 0.43213\n",
      "iteration 6 \t : \t loss 0.17674 - accuracy 0.51978\n",
      "iteration 7 \t : \t loss 0.15096 - accuracy 0.59204\n",
      "iteration 8 \t : \t loss 0.12770 - accuracy 0.65381\n",
      "iteration 9 \t : \t loss 0.10943 - accuracy 0.69751\n",
      "iteration 10 \t : \t loss 0.09573 - accuracy 0.73218\n",
      "iteration 11 \t : \t loss 0.08562 - accuracy 0.75537\n",
      "iteration 12 \t : \t loss 0.07800 - accuracy 0.77832\n",
      "iteration 13 \t : \t loss 0.07204 - accuracy 0.79688\n",
      "iteration 14 \t : \t loss 0.06721 - accuracy 0.80835\n",
      "iteration 15 \t : \t loss 0.06318 - accuracy 0.81958\n",
      "iteration 16 \t : \t loss 0.05973 - accuracy 0.82959\n",
      "iteration 17 \t : \t loss 0.05672 - accuracy 0.83887\n",
      "iteration 18 \t : \t loss 0.05405 - accuracy 0.84692\n",
      "iteration 19 \t : \t loss 0.05166 - accuracy 0.85547\n",
      "iteration 20 \t : \t loss 0.04952 - accuracy 0.86157\n",
      "iteration 21 \t : \t loss 0.04758 - accuracy 0.86890\n",
      "iteration 22 \t : \t loss 0.04583 - accuracy 0.87427\n",
      "iteration 23 \t : \t loss 0.04423 - accuracy 0.87622\n",
      "iteration 24 \t : \t loss 0.04279 - accuracy 0.88062\n",
      "iteration 25 \t : \t loss 0.04146 - accuracy 0.88379\n",
      "iteration 26 \t : \t loss 0.04025 - accuracy 0.88745\n",
      "iteration 27 \t : \t loss 0.03914 - accuracy 0.88818\n",
      "iteration 28 \t : \t loss 0.03811 - accuracy 0.89160\n",
      "iteration 29 \t : \t loss 0.03715 - accuracy 0.89429\n",
      "iteration 30 \t : \t loss 0.03625 - accuracy 0.89551\n",
      "iteration 31 \t : \t loss 0.03542 - accuracy 0.89868\n",
      "iteration 32 \t : \t loss 0.03462 - accuracy 0.90332\n",
      "iteration 33 \t : \t loss 0.03388 - accuracy 0.90503\n",
      "iteration 34 \t : \t loss 0.03317 - accuracy 0.90771\n",
      "iteration 35 \t : \t loss 0.03249 - accuracy 0.90967\n",
      "iteration 36 \t : \t loss 0.03184 - accuracy 0.91162\n",
      "iteration 37 \t : \t loss 0.03122 - accuracy 0.91235\n",
      "iteration 38 \t : \t loss 0.03063 - accuracy 0.91333\n",
      "iteration 39 \t : \t loss 0.03005 - accuracy 0.91479\n",
      "iteration 40 \t : \t loss 0.02949 - accuracy 0.91577\n",
      "iteration 41 \t : \t loss 0.02895 - accuracy 0.91675\n",
      "iteration 42 \t : \t loss 0.02843 - accuracy 0.91772\n",
      "iteration 43 \t : \t loss 0.02792 - accuracy 0.91919\n",
      "iteration 44 \t : \t loss 0.02743 - accuracy 0.92090\n",
      "iteration 45 \t : \t loss 0.02695 - accuracy 0.92261\n",
      "iteration 46 \t : \t loss 0.02648 - accuracy 0.92456\n",
      "iteration 47 \t : \t loss 0.02602 - accuracy 0.92725\n",
      "iteration 48 \t : \t loss 0.02557 - accuracy 0.92871\n",
      "iteration 49 \t : \t loss 0.02513 - accuracy 0.93042\n",
      "iteration 50 \t : \t loss 0.02469 - accuracy 0.93237\n",
      "iteration 51 \t : \t loss 0.02427 - accuracy 0.93384\n",
      "iteration 52 \t : \t loss 0.02385 - accuracy 0.93530\n",
      "iteration 53 \t : \t loss 0.02344 - accuracy 0.93677\n",
      "iteration 54 \t : \t loss 0.02304 - accuracy 0.93799\n",
      "iteration 55 \t : \t loss 0.02264 - accuracy 0.93945\n",
      "iteration 56 \t : \t loss 0.02225 - accuracy 0.94189\n",
      "iteration 57 \t : \t loss 0.02187 - accuracy 0.94336\n",
      "iteration 58 \t : \t loss 0.02149 - accuracy 0.94409\n",
      "iteration 59 \t : \t loss 0.02111 - accuracy 0.94507\n",
      "iteration 60 \t : \t loss 0.02074 - accuracy 0.94653\n",
      "iteration 61 \t : \t loss 0.02038 - accuracy 0.94824\n",
      "iteration 62 \t : \t loss 0.02001 - accuracy 0.94897\n",
      "iteration 63 \t : \t loss 0.01966 - accuracy 0.94922\n",
      "iteration 64 \t : \t loss 0.01930 - accuracy 0.94946\n",
      "iteration 65 \t : \t loss 0.01895 - accuracy 0.95142\n",
      "iteration 66 \t : \t loss 0.01861 - accuracy 0.95264\n",
      "iteration 67 \t : \t loss 0.01827 - accuracy 0.95337\n",
      "iteration 68 \t : \t loss 0.01793 - accuracy 0.95435\n",
      "iteration 69 \t : \t loss 0.01760 - accuracy 0.95508\n",
      "iteration 70 \t : \t loss 0.01726 - accuracy 0.95728\n",
      "iteration 71 \t : \t loss 0.01694 - accuracy 0.95801\n",
      "iteration 72 \t : \t loss 0.01661 - accuracy 0.95923\n",
      "iteration 73 \t : \t loss 0.01629 - accuracy 0.95996\n",
      "iteration 74 \t : \t loss 0.01597 - accuracy 0.96094\n",
      "iteration 75 \t : \t loss 0.01566 - accuracy 0.96289\n",
      "iteration 76 \t : \t loss 0.01535 - accuracy 0.96362\n",
      "iteration 77 \t : \t loss 0.01504 - accuracy 0.96460\n",
      "iteration 78 \t : \t loss 0.01474 - accuracy 0.96558\n",
      "iteration 79 \t : \t loss 0.01444 - accuracy 0.96655\n",
      "iteration 80 \t : \t loss 0.01414 - accuracy 0.96777\n",
      "iteration 81 \t : \t loss 0.01385 - accuracy 0.96851\n",
      "iteration 82 \t : \t loss 0.01356 - accuracy 0.96899\n",
      "iteration 83 \t : \t loss 0.01328 - accuracy 0.96973\n",
      "iteration 84 \t : \t loss 0.01299 - accuracy 0.97119\n",
      "iteration 85 \t : \t loss 0.01272 - accuracy 0.97168\n",
      "iteration 86 \t : \t loss 0.01244 - accuracy 0.97241\n",
      "iteration 87 \t : \t loss 0.01217 - accuracy 0.97388\n",
      "iteration 88 \t : \t loss 0.01190 - accuracy 0.97559\n",
      "iteration 89 \t : \t loss 0.01164 - accuracy 0.97607\n",
      "iteration 90 \t : \t loss 0.01138 - accuracy 0.97681\n",
      "iteration 91 \t : \t loss 0.01112 - accuracy 0.97705\n",
      "iteration 92 \t : \t loss 0.01087 - accuracy 0.97729\n",
      "iteration 93 \t : \t loss 0.01062 - accuracy 0.97778\n",
      "iteration 94 \t : \t loss 0.01038 - accuracy 0.97876\n",
      "iteration 95 \t : \t loss 0.01014 - accuracy 0.97998\n",
      "iteration 96 \t : \t loss 0.00990 - accuracy 0.98047\n",
      "iteration 97 \t : \t loss 0.00967 - accuracy 0.98120\n",
      "iteration 98 \t : \t loss 0.00944 - accuracy 0.98218\n",
      "iteration 99 \t : \t loss 0.00922 - accuracy 0.98218\n",
      "iteration 100 \t : \t loss 0.00900 - accuracy 0.98267\n",
      "iteration 101 \t : \t loss 0.00878 - accuracy 0.98340\n",
      "iteration 102 \t : \t loss 0.00857 - accuracy 0.98462\n",
      "iteration 103 \t : \t loss 0.00836 - accuracy 0.98560\n",
      "iteration 104 \t : \t loss 0.00816 - accuracy 0.98608\n",
      "iteration 105 \t : \t loss 0.00796 - accuracy 0.98682\n",
      "iteration 106 \t : \t loss 0.00777 - accuracy 0.98730\n",
      "iteration 107 \t : \t loss 0.00757 - accuracy 0.98755\n",
      "iteration 108 \t : \t loss 0.00739 - accuracy 0.98779\n",
      "iteration 109 \t : \t loss 0.00721 - accuracy 0.98804\n",
      "iteration 110 \t : \t loss 0.00703 - accuracy 0.98877\n",
      "iteration 111 \t : \t loss 0.00685 - accuracy 0.98877\n",
      "iteration 112 \t : \t loss 0.00668 - accuracy 0.98926\n",
      "iteration 113 \t : \t loss 0.00652 - accuracy 0.98999\n",
      "iteration 114 \t : \t loss 0.00635 - accuracy 0.99023\n",
      "iteration 115 \t : \t loss 0.00620 - accuracy 0.99048\n",
      "iteration 116 \t : \t loss 0.00604 - accuracy 0.99146\n",
      "iteration 117 \t : \t loss 0.00589 - accuracy 0.99194\n",
      "iteration 118 \t : \t loss 0.00574 - accuracy 0.99194\n",
      "iteration 119 \t : \t loss 0.00560 - accuracy 0.99194\n",
      "iteration 120 \t : \t loss 0.00546 - accuracy 0.99219\n",
      "iteration 121 \t : \t loss 0.00533 - accuracy 0.99219\n",
      "iteration 122 \t : \t loss 0.00519 - accuracy 0.99316\n",
      "iteration 123 \t : \t loss 0.00507 - accuracy 0.99341\n",
      "iteration 124 \t : \t loss 0.00494 - accuracy 0.99365\n",
      "iteration 125 \t : \t loss 0.00482 - accuracy 0.99438\n",
      "iteration 126 \t : \t loss 0.00470 - accuracy 0.99487\n",
      "iteration 127 \t : \t loss 0.00459 - accuracy 0.99561\n",
      "iteration 128 \t : \t loss 0.00447 - accuracy 0.99585\n",
      "iteration 129 \t : \t loss 0.00436 - accuracy 0.99634\n",
      "iteration 130 \t : \t loss 0.00426 - accuracy 0.99658\n",
      "iteration 131 \t : \t loss 0.00416 - accuracy 0.99707\n",
      "iteration 132 \t : \t loss 0.00406 - accuracy 0.99707\n",
      "iteration 133 \t : \t loss 0.00396 - accuracy 0.99707\n",
      "iteration 134 \t : \t loss 0.00386 - accuracy 0.99731\n",
      "iteration 135 \t : \t loss 0.00377 - accuracy 0.99756\n",
      "iteration 136 \t : \t loss 0.00368 - accuracy 0.99780\n",
      "iteration 137 \t : \t loss 0.00360 - accuracy 0.99780\n",
      "iteration 138 \t : \t loss 0.00351 - accuracy 0.99780\n",
      "iteration 139 \t : \t loss 0.00343 - accuracy 0.99805\n",
      "iteration 140 \t : \t loss 0.00335 - accuracy 0.99829\n",
      "iteration 141 \t : \t loss 0.00328 - accuracy 0.99829\n",
      "iteration 142 \t : \t loss 0.00320 - accuracy 0.99829\n",
      "iteration 143 \t : \t loss 0.00313 - accuracy 0.99829\n",
      "iteration 144 \t : \t loss 0.00306 - accuracy 0.99829\n",
      "iteration 145 \t : \t loss 0.00299 - accuracy 0.99878\n",
      "iteration 146 \t : \t loss 0.00293 - accuracy 0.99878\n",
      "iteration 147 \t : \t loss 0.00286 - accuracy 0.99878\n",
      "iteration 148 \t : \t loss 0.00280 - accuracy 0.99878\n",
      "iteration 149 \t : \t loss 0.00274 - accuracy 0.99878\n",
      "iteration 150 \t : \t loss 0.00268 - accuracy 0.99878\n",
      "iteration 151 \t : \t loss 0.00262 - accuracy 0.99878\n",
      "iteration 152 \t : \t loss 0.00257 - accuracy 0.99878\n",
      "iteration 153 \t : \t loss 0.00252 - accuracy 0.99878\n",
      "iteration 154 \t : \t loss 0.00246 - accuracy 0.99902\n",
      "iteration 155 \t : \t loss 0.00241 - accuracy 0.99927\n",
      "iteration 156 \t : \t loss 0.00237 - accuracy 0.99927\n",
      "iteration 157 \t : \t loss 0.00232 - accuracy 0.99951\n",
      "iteration 158 \t : \t loss 0.00227 - accuracy 0.99976\n",
      "iteration 159 \t : \t loss 0.00223 - accuracy 0.99976\n",
      "iteration 160 \t : \t loss 0.00218 - accuracy 0.99976\n",
      "iteration 161 \t : \t loss 0.00214 - accuracy 0.99976\n",
      "iteration 162 \t : \t loss 0.00210 - accuracy 0.99976\n",
      "iteration 163 \t : \t loss 0.00206 - accuracy 1.00000\n",
      "iteration 164 \t : \t loss 0.00202 - accuracy 1.00000\n",
      "iteration 165 \t : \t loss 0.00198 - accuracy 1.00000\n",
      "iteration 166 \t : \t loss 0.00195 - accuracy 1.00000\n",
      "iteration 167 \t : \t loss 0.00191 - accuracy 1.00000\n",
      "iteration 168 \t : \t loss 0.00188 - accuracy 1.00000\n",
      "iteration 169 \t : \t loss 0.00184 - accuracy 1.00000\n",
      "iteration 170 \t : \t loss 0.00181 - accuracy 1.00000\n",
      "iteration 171 \t : \t loss 0.00178 - accuracy 1.00000\n",
      "iteration 172 \t : \t loss 0.00175 - accuracy 1.00000\n",
      "iteration 173 \t : \t loss 0.00172 - accuracy 1.00000\n",
      "iteration 174 \t : \t loss 0.00169 - accuracy 1.00000\n",
      "iteration 175 \t : \t loss 0.00166 - accuracy 1.00000\n",
      "iteration 176 \t : \t loss 0.00163 - accuracy 1.00000\n",
      "iteration 177 \t : \t loss 0.00161 - accuracy 1.00000\n",
      "iteration 178 \t : \t loss 0.00158 - accuracy 1.00000\n",
      "iteration 179 \t : \t loss 0.00155 - accuracy 1.00000\n",
      "iteration 180 \t : \t loss 0.00153 - accuracy 1.00000\n",
      "iteration 181 \t : \t loss 0.00151 - accuracy 1.00000\n",
      "iteration 182 \t : \t loss 0.00148 - accuracy 1.00000\n",
      "iteration 183 \t : \t loss 0.00146 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00144 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00141 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00139 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00137 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00135 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00133 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00131 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00129 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00127 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00126 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00124 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00122 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00120 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00119 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00117 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00115 - accuracy 1.00000\n",
      "iteration 0 \t : \t loss 0.12138 - accuracy 0.68701\n",
      "iteration 1 \t : \t loss 0.06131 - accuracy 0.86450\n",
      "iteration 2 \t : \t loss 0.04842 - accuracy 0.88770\n",
      "iteration 3 \t : \t loss 0.04227 - accuracy 0.89429\n",
      "iteration 4 \t : \t loss 0.03846 - accuracy 0.90430\n",
      "iteration 5 \t : \t loss 0.03577 - accuracy 0.90869\n",
      "iteration 6 \t : \t loss 0.03370 - accuracy 0.91333\n",
      "iteration 7 \t : \t loss 0.03203 - accuracy 0.91650\n",
      "iteration 8 \t : \t loss 0.03064 - accuracy 0.91992\n",
      "iteration 9 \t : \t loss 0.02943 - accuracy 0.92261\n",
      "iteration 10 \t : \t loss 0.02837 - accuracy 0.92505\n",
      "iteration 11 \t : \t loss 0.02742 - accuracy 0.92676\n",
      "iteration 12 \t : \t loss 0.02655 - accuracy 0.92920\n",
      "iteration 13 \t : \t loss 0.02576 - accuracy 0.93091\n",
      "iteration 14 \t : \t loss 0.02503 - accuracy 0.93384\n",
      "iteration 15 \t : \t loss 0.02435 - accuracy 0.93677\n",
      "iteration 16 \t : \t loss 0.02372 - accuracy 0.93848\n",
      "iteration 17 \t : \t loss 0.02312 - accuracy 0.94067\n",
      "iteration 18 \t : \t loss 0.02255 - accuracy 0.94165\n",
      "iteration 19 \t : \t loss 0.02201 - accuracy 0.94312\n",
      "iteration 20 \t : \t loss 0.02150 - accuracy 0.94385\n",
      "iteration 21 \t : \t loss 0.02102 - accuracy 0.94507\n",
      "iteration 22 \t : \t loss 0.02055 - accuracy 0.94678\n",
      "iteration 23 \t : \t loss 0.02011 - accuracy 0.94751\n",
      "iteration 24 \t : \t loss 0.01968 - accuracy 0.94897\n",
      "iteration 25 \t : \t loss 0.01927 - accuracy 0.95093\n",
      "iteration 26 \t : \t loss 0.01887 - accuracy 0.95239\n",
      "iteration 27 \t : \t loss 0.01849 - accuracy 0.95312\n",
      "iteration 28 \t : \t loss 0.01812 - accuracy 0.95386\n",
      "iteration 29 \t : \t loss 0.01776 - accuracy 0.95483\n",
      "iteration 30 \t : \t loss 0.01742 - accuracy 0.95557\n",
      "iteration 31 \t : \t loss 0.01708 - accuracy 0.95630\n",
      "iteration 32 \t : \t loss 0.01676 - accuracy 0.95703\n",
      "iteration 33 \t : \t loss 0.01644 - accuracy 0.95825\n",
      "iteration 34 \t : \t loss 0.01614 - accuracy 0.95972\n",
      "iteration 35 \t : \t loss 0.01584 - accuracy 0.95996\n",
      "iteration 36 \t : \t loss 0.01555 - accuracy 0.96143\n",
      "iteration 37 \t : \t loss 0.01527 - accuracy 0.96240\n",
      "iteration 38 \t : \t loss 0.01500 - accuracy 0.96362\n",
      "iteration 39 \t : \t loss 0.01473 - accuracy 0.96436\n",
      "iteration 40 \t : \t loss 0.01447 - accuracy 0.96436\n",
      "iteration 41 \t : \t loss 0.01422 - accuracy 0.96484\n",
      "iteration 42 \t : \t loss 0.01397 - accuracy 0.96582\n",
      "iteration 43 \t : \t loss 0.01373 - accuracy 0.96680\n",
      "iteration 44 \t : \t loss 0.01349 - accuracy 0.96704\n",
      "iteration 45 \t : \t loss 0.01326 - accuracy 0.96704\n",
      "iteration 46 \t : \t loss 0.01303 - accuracy 0.96851\n",
      "iteration 47 \t : \t loss 0.01281 - accuracy 0.96875\n",
      "iteration 48 \t : \t loss 0.01259 - accuracy 0.96973\n",
      "iteration 49 \t : \t loss 0.01238 - accuracy 0.97046\n",
      "iteration 50 \t : \t loss 0.01217 - accuracy 0.97095\n",
      "iteration 51 \t : \t loss 0.01197 - accuracy 0.97144\n",
      "iteration 52 \t : \t loss 0.01177 - accuracy 0.97192\n",
      "iteration 53 \t : \t loss 0.01158 - accuracy 0.97241\n",
      "iteration 54 \t : \t loss 0.01138 - accuracy 0.97290\n",
      "iteration 55 \t : \t loss 0.01120 - accuracy 0.97290\n",
      "iteration 56 \t : \t loss 0.01101 - accuracy 0.97314\n",
      "iteration 57 \t : \t loss 0.01083 - accuracy 0.97388\n",
      "iteration 58 \t : \t loss 0.01065 - accuracy 0.97510\n",
      "iteration 59 \t : \t loss 0.01048 - accuracy 0.97559\n",
      "iteration 60 \t : \t loss 0.01031 - accuracy 0.97607\n",
      "iteration 61 \t : \t loss 0.01014 - accuracy 0.97607\n",
      "iteration 62 \t : \t loss 0.00998 - accuracy 0.97681\n",
      "iteration 63 \t : \t loss 0.00981 - accuracy 0.97681\n",
      "iteration 64 \t : \t loss 0.00966 - accuracy 0.97754\n",
      "iteration 65 \t : \t loss 0.00950 - accuracy 0.97852\n",
      "iteration 66 \t : \t loss 0.00935 - accuracy 0.97876\n",
      "iteration 67 \t : \t loss 0.00920 - accuracy 0.97876\n",
      "iteration 68 \t : \t loss 0.00905 - accuracy 0.97876\n",
      "iteration 69 \t : \t loss 0.00890 - accuracy 0.97974\n",
      "iteration 70 \t : \t loss 0.00876 - accuracy 0.98022\n",
      "iteration 71 \t : \t loss 0.00862 - accuracy 0.98145\n",
      "iteration 72 \t : \t loss 0.00848 - accuracy 0.98169\n",
      "iteration 73 \t : \t loss 0.00835 - accuracy 0.98242\n",
      "iteration 74 \t : \t loss 0.00822 - accuracy 0.98340\n",
      "iteration 75 \t : \t loss 0.00809 - accuracy 0.98438\n",
      "iteration 76 \t : \t loss 0.00796 - accuracy 0.98438\n",
      "iteration 77 \t : \t loss 0.00783 - accuracy 0.98486\n",
      "iteration 78 \t : \t loss 0.00771 - accuracy 0.98486\n",
      "iteration 79 \t : \t loss 0.00759 - accuracy 0.98511\n",
      "iteration 80 \t : \t loss 0.00747 - accuracy 0.98535\n",
      "iteration 81 \t : \t loss 0.00735 - accuracy 0.98584\n",
      "iteration 82 \t : \t loss 0.00724 - accuracy 0.98633\n",
      "iteration 83 \t : \t loss 0.00712 - accuracy 0.98657\n",
      "iteration 84 \t : \t loss 0.00701 - accuracy 0.98779\n",
      "iteration 85 \t : \t loss 0.00690 - accuracy 0.98828\n",
      "iteration 86 \t : \t loss 0.00679 - accuracy 0.98828\n",
      "iteration 87 \t : \t loss 0.00669 - accuracy 0.98853\n",
      "iteration 88 \t : \t loss 0.00659 - accuracy 0.98877\n",
      "iteration 89 \t : \t loss 0.00648 - accuracy 0.98926\n",
      "iteration 90 \t : \t loss 0.00638 - accuracy 0.98999\n",
      "iteration 91 \t : \t loss 0.00629 - accuracy 0.98999\n",
      "iteration 92 \t : \t loss 0.00619 - accuracy 0.99023\n",
      "iteration 93 \t : \t loss 0.00609 - accuracy 0.99023\n",
      "iteration 94 \t : \t loss 0.00600 - accuracy 0.99072\n",
      "iteration 95 \t : \t loss 0.00591 - accuracy 0.99146\n",
      "iteration 96 \t : \t loss 0.00582 - accuracy 0.99146\n",
      "iteration 97 \t : \t loss 0.00573 - accuracy 0.99170\n",
      "iteration 98 \t : \t loss 0.00564 - accuracy 0.99219\n",
      "iteration 99 \t : \t loss 0.00556 - accuracy 0.99243\n",
      "iteration 100 \t : \t loss 0.00548 - accuracy 0.99243\n",
      "iteration 101 \t : \t loss 0.00539 - accuracy 0.99243\n",
      "iteration 102 \t : \t loss 0.00531 - accuracy 0.99243\n",
      "iteration 103 \t : \t loss 0.00523 - accuracy 0.99243\n",
      "iteration 104 \t : \t loss 0.00516 - accuracy 0.99243\n",
      "iteration 105 \t : \t loss 0.00508 - accuracy 0.99268\n",
      "iteration 106 \t : \t loss 0.00500 - accuracy 0.99268\n",
      "iteration 107 \t : \t loss 0.00493 - accuracy 0.99292\n",
      "iteration 108 \t : \t loss 0.00486 - accuracy 0.99292\n",
      "iteration 109 \t : \t loss 0.00479 - accuracy 0.99316\n",
      "iteration 110 \t : \t loss 0.00472 - accuracy 0.99341\n",
      "iteration 111 \t : \t loss 0.00465 - accuracy 0.99390\n",
      "iteration 112 \t : \t loss 0.00458 - accuracy 0.99414\n",
      "iteration 113 \t : \t loss 0.00451 - accuracy 0.99414\n",
      "iteration 114 \t : \t loss 0.00445 - accuracy 0.99414\n",
      "iteration 115 \t : \t loss 0.00439 - accuracy 0.99463\n",
      "iteration 116 \t : \t loss 0.00432 - accuracy 0.99487\n",
      "iteration 117 \t : \t loss 0.00426 - accuracy 0.99487\n",
      "iteration 118 \t : \t loss 0.00420 - accuracy 0.99487\n",
      "iteration 119 \t : \t loss 0.00414 - accuracy 0.99487\n",
      "iteration 120 \t : \t loss 0.00408 - accuracy 0.99487\n",
      "iteration 121 \t : \t loss 0.00403 - accuracy 0.99487\n",
      "iteration 122 \t : \t loss 0.00397 - accuracy 0.99536\n",
      "iteration 123 \t : \t loss 0.00391 - accuracy 0.99536\n",
      "iteration 124 \t : \t loss 0.00386 - accuracy 0.99561\n",
      "iteration 125 \t : \t loss 0.00381 - accuracy 0.99561\n",
      "iteration 126 \t : \t loss 0.00375 - accuracy 0.99561\n",
      "iteration 127 \t : \t loss 0.00370 - accuracy 0.99561\n",
      "iteration 128 \t : \t loss 0.00365 - accuracy 0.99561\n",
      "iteration 129 \t : \t loss 0.00360 - accuracy 0.99561\n",
      "iteration 130 \t : \t loss 0.00355 - accuracy 0.99585\n",
      "iteration 131 \t : \t loss 0.00351 - accuracy 0.99609\n",
      "iteration 132 \t : \t loss 0.00346 - accuracy 0.99634\n",
      "iteration 133 \t : \t loss 0.00341 - accuracy 0.99634\n",
      "iteration 134 \t : \t loss 0.00337 - accuracy 0.99683\n",
      "iteration 135 \t : \t loss 0.00332 - accuracy 0.99683\n",
      "iteration 136 \t : \t loss 0.00328 - accuracy 0.99683\n",
      "iteration 137 \t : \t loss 0.00324 - accuracy 0.99707\n",
      "iteration 138 \t : \t loss 0.00319 - accuracy 0.99731\n",
      "iteration 139 \t : \t loss 0.00315 - accuracy 0.99731\n",
      "iteration 140 \t : \t loss 0.00311 - accuracy 0.99756\n",
      "iteration 141 \t : \t loss 0.00307 - accuracy 0.99756\n",
      "iteration 142 \t : \t loss 0.00303 - accuracy 0.99756\n",
      "iteration 143 \t : \t loss 0.00299 - accuracy 0.99756\n",
      "iteration 144 \t : \t loss 0.00296 - accuracy 0.99780\n",
      "iteration 145 \t : \t loss 0.00292 - accuracy 0.99780\n",
      "iteration 146 \t : \t loss 0.00288 - accuracy 0.99805\n",
      "iteration 147 \t : \t loss 0.00285 - accuracy 0.99805\n",
      "iteration 148 \t : \t loss 0.00281 - accuracy 0.99805\n",
      "iteration 149 \t : \t loss 0.00278 - accuracy 0.99829\n",
      "iteration 150 \t : \t loss 0.00274 - accuracy 0.99829\n",
      "iteration 151 \t : \t loss 0.00271 - accuracy 0.99829\n",
      "iteration 152 \t : \t loss 0.00268 - accuracy 0.99829\n",
      "iteration 153 \t : \t loss 0.00264 - accuracy 0.99829\n",
      "iteration 154 \t : \t loss 0.00261 - accuracy 0.99829\n",
      "iteration 155 \t : \t loss 0.00258 - accuracy 0.99829\n",
      "iteration 156 \t : \t loss 0.00255 - accuracy 0.99829\n",
      "iteration 157 \t : \t loss 0.00252 - accuracy 0.99829\n",
      "iteration 158 \t : \t loss 0.00249 - accuracy 0.99829\n",
      "iteration 159 \t : \t loss 0.00246 - accuracy 0.99829\n",
      "iteration 160 \t : \t loss 0.00243 - accuracy 0.99829\n",
      "iteration 161 \t : \t loss 0.00241 - accuracy 0.99829\n",
      "iteration 162 \t : \t loss 0.00238 - accuracy 0.99854\n",
      "iteration 163 \t : \t loss 0.00235 - accuracy 0.99854\n",
      "iteration 164 \t : \t loss 0.00232 - accuracy 0.99854\n",
      "iteration 165 \t : \t loss 0.00230 - accuracy 0.99854\n",
      "iteration 166 \t : \t loss 0.00227 - accuracy 0.99854\n",
      "iteration 167 \t : \t loss 0.00225 - accuracy 0.99854\n",
      "iteration 168 \t : \t loss 0.00222 - accuracy 0.99854\n",
      "iteration 169 \t : \t loss 0.00220 - accuracy 0.99854\n",
      "iteration 170 \t : \t loss 0.00217 - accuracy 0.99854\n",
      "iteration 171 \t : \t loss 0.00215 - accuracy 0.99878\n",
      "iteration 172 \t : \t loss 0.00213 - accuracy 0.99902\n",
      "iteration 173 \t : \t loss 0.00211 - accuracy 0.99902\n",
      "iteration 174 \t : \t loss 0.00208 - accuracy 0.99951\n",
      "iteration 175 \t : \t loss 0.00206 - accuracy 0.99951\n",
      "iteration 176 \t : \t loss 0.00204 - accuracy 0.99951\n",
      "iteration 177 \t : \t loss 0.00202 - accuracy 0.99951\n",
      "iteration 178 \t : \t loss 0.00200 - accuracy 0.99976\n",
      "iteration 179 \t : \t loss 0.00198 - accuracy 0.99976\n",
      "iteration 180 \t : \t loss 0.00196 - accuracy 0.99976\n",
      "iteration 181 \t : \t loss 0.00194 - accuracy 0.99976\n",
      "iteration 182 \t : \t loss 0.00192 - accuracy 0.99976\n",
      "iteration 183 \t : \t loss 0.00190 - accuracy 1.00000\n",
      "iteration 184 \t : \t loss 0.00188 - accuracy 1.00000\n",
      "iteration 185 \t : \t loss 0.00186 - accuracy 1.00000\n",
      "iteration 186 \t : \t loss 0.00184 - accuracy 1.00000\n",
      "iteration 187 \t : \t loss 0.00182 - accuracy 1.00000\n",
      "iteration 188 \t : \t loss 0.00180 - accuracy 1.00000\n",
      "iteration 189 \t : \t loss 0.00179 - accuracy 1.00000\n",
      "iteration 190 \t : \t loss 0.00177 - accuracy 1.00000\n",
      "iteration 191 \t : \t loss 0.00175 - accuracy 1.00000\n",
      "iteration 192 \t : \t loss 0.00174 - accuracy 1.00000\n",
      "iteration 193 \t : \t loss 0.00172 - accuracy 1.00000\n",
      "iteration 194 \t : \t loss 0.00170 - accuracy 1.00000\n",
      "iteration 195 \t : \t loss 0.00169 - accuracy 1.00000\n",
      "iteration 196 \t : \t loss 0.00167 - accuracy 1.00000\n",
      "iteration 197 \t : \t loss 0.00166 - accuracy 1.00000\n",
      "iteration 198 \t : \t loss 0.00164 - accuracy 1.00000\n",
      "iteration 199 \t : \t loss 0.00163 - accuracy 1.00000\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_samples_l = [1000, 3000, 7000, 10000, 30000, 60000]\n",
    "q = 200\n",
    "n_layers = 2\n",
    "\n",
    "error_rates_1, error_rates_2 = [], []\n",
    "\n",
    "for n_samples in n_samples_l:\n",
    "    # Initialization\n",
    "    DNN_trained = init_DBN(p, q, n_layers)\n",
    "    DNN_pre_trained = copy.deepcopy(DNN_trained)\n",
    "    \n",
    "    # Training\n",
    "    DNN_trained = principal_mnist(n_samples, DNN_trained, pre_train=False, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "    DNN_pre_trained = principal_mnist(n_samples, DNN_pre_trained, pre_train=True, nb_iter=nb_iter, nb_iter_RBM=nb_iter_RBM, lr=lr, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    error_rates_1.append(test_DNN(DNN_trained, test_image, test_label))\n",
    "    error_rates_2.append(test_DNN(DNN_pre_trained, test_image, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YlRZobwx8dQa",
    "outputId": "f299a81b-c2de-4b27-a263-d590eb3ccc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14670000000000005, 0.10660000000000003, 0.0917, 0.08899999999999997, 0.0927, 0.09099999999999997]\n",
      "[0.10319999999999996, 0.06940000000000002, 0.06120000000000003, 0.05710000000000004, 0.05630000000000002, 0.05259999999999998]\n"
     ]
    }
   ],
   "source": [
    "print(error_rates_1)\n",
    "print(error_rates_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "id": "UtuVms_M6tva",
    "outputId": "6db5f71d-10ec-47b0-faf6-533c73584fdd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_samples_l, error_rates_1, label='No pre-training', color='blue')\n",
    "plt.plot(n_samples_l, error_rates_2, label='With pre-training', color='red')\n",
    "plt.title('Error rate according to number of training images', size=20)\n",
    "plt.xlabel('Number of training images', size=15)\n",
    "plt.ylabel('Error rate', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "fig.savefig('n_samples.jpg', dpi = 300, bbox_inches='tight', orientation = 'landscape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbpYJ2r5QbUJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de DLII_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
